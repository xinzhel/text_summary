id,title,abstract,segmented_sents
2212.01207,Joint Open Knowledge Base Canonicalization and Linking,"  Open Information Extraction (OIE) methods extract a large number of OIE
triples (noun phrase, relation phrase, noun phrase) from text, which compose
large Open Knowledge Bases (OKBs). However, noun phrases (NPs) and relation
phrases (RPs) in OKBs are not canonicalized and often appear in different
paraphrased textual variants, which leads to redundant and ambiguous facts. To
address this problem, there are two related tasks: OKB canonicalization (i.e.,
convert NPs and RPs to canonicalized form) and OKB linking (i.e., link NPs and
RPs with their corresponding entities and relations in a curated Knowledge Base
(e.g., DBPedia). These two tasks are tightly coupled, and one task can benefit
significantly from the other. However, they have been studied in isolation so
far. In this paper, we explore the task of joint OKB canonicalization and
linking for the first time, and propose a novel framework JOCL based on factor
graph model to make them reinforce each other. JOCL is flexible enough to
combine different signals from both tasks, and able to extend to fit any new
signals. A thorough experimental study over two large scale OIE triple data
sets shows that our framework outperforms all the baseline methods for the task
of OKB canonicalization (OKB linking) in terms of average F1 (accuracy).
","<1> Open Information Extraction (OIE) methods extract a large number of OIE triples (noun phrase, relation phrase, noun phrase) from text, which compose large Open Knowledge Bases (OKBs). </1>
 <2> However, noun phrases (NPs) and relation phrases (RPs) in OKBs are not canonicalized and often appear in different paraphrased textual variants, which leads to redundant and ambiguous facts. </2>
 <3> To address this problem, there are two related tasks: OKB canonicalization (i.e., convert NPs and RPs to canonicalized form) and OKB linking (i.e., link NPs and RPs with their corresponding entities and relations in a curated Knowledge Base </3>
 <4> (e.g., DBPedia). </4>
 <5> These two tasks are tightly coupled, and one task can benefit significantly from the other. </5>
 <6> However, they have been studied in isolation so far. </6>
 <7> In this paper, we explore the task of joint OKB canonicalization and linking for the first time, and propose a novel framework JOCL based on factor graph model to make them reinforce each other. </7>
 <8> JOCL is flexible enough to combine different signals from both tasks, and able to extend to fit any new signals. </8>
 <9> A thorough experimental study over two large scale OIE triple data sets shows that our framework outperforms all the baseline methods for the task of OKB canonicalization (OKB linking) in terms of average F1 (accuracy). </9>
"
2012.10251,A Benchmark Arabic Dataset for Commonsense Explanation,"  Language comprehension and commonsense knowledge validation by machines are
challenging tasks that are still under researched and evaluated for Arabic
text. In this paper, we present a benchmark Arabic dataset for commonsense
explanation. The dataset consists of Arabic sentences that does not make sense
along with three choices to select among them the one that explains why the
sentence is false. Furthermore, this paper presents baseline results to assist
and encourage the future evaluation of research in this field. The dataset is
distributed under the Creative Commons CC-BY-SA 4.0 license and can be found on
GitHub
","<1> Language comprehension and commonsense knowledge validation by machines are challenging tasks that are still under researched and evaluated for Arabic text. </1>
 <2> In this paper, we present a benchmark Arabic dataset for commonsense explanation. </2>
 <3> The dataset consists of Arabic sentences that does not make sense along with three choices to select among them the one that explains why the sentence is false. </3>
 <4> Furthermore, this paper presents baseline results to assist and encourage the future evaluation of research in this field. </4>
 <5> The dataset is distributed under the Creative Commons CC-BY-SA 4.0 license and can be found on GitHub </5>
"
2212.07672,"Summary-Oriented Vision Modeling for Multimodal Abstractive
  Summarization","  Multimodal abstractive summarization (MAS) aims to produce a concise summary
given the multimodal data (text and vision). Existing studies mainly focus on
how to effectively use the visual features from the perspective of an article,
having achieved impressive success on the high-resource English dataset.
However, less attention has been paid to the visual features from the
perspective of the summary, which may limit the model performance, especially
in the low- and zero-resource scenarios. In this paper, we propose to improve
the summary quality through summary-oriented visual features. To this end, we
devise two auxiliary tasks including vision to summary task and masked image
modeling task. Together with the main summarization task, we optimize the MAS
model via the training objectives of all these tasks. By these means, the MAS
model can be enhanced by capturing the summary-oriented visual features,
thereby yielding more accurate summaries. Experiments on 44 languages, covering
mid-high-, low-, and zero-resource scenarios, verify the effectiveness and
superiority of the proposed approach, which achieves state-of-the-art
performance under all scenarios. Additionally, we will contribute a large-scale
multilingual multimodal abstractive summarization (MM-Sum) dataset.
","<1> Multimodal abstractive summarization (MAS) aims to produce a concise summary given the multimodal data (text and vision). </1>
 <2> Existing studies mainly focus on how to effectively use the visual features from the perspective of an article, having achieved impressive success on the high-resource English dataset. </2>
 <3> However, less attention has been paid to the visual features from the perspective of the summary, which may limit the model performance, especially in the low- and zero-resource scenarios. </3>
 <4> In this paper, we propose to improve the summary quality through summary-oriented visual features. </4>
 <5> To this end, we devise two auxiliary tasks including vision to summary task and masked image modeling task. </5>
 <6> Together with the main summarization task, we optimize the MAS model via the training objectives of all these tasks. </6>
 <7> By these means, the MAS model can be enhanced by capturing the summary-oriented visual features, thereby yielding more accurate summaries. </7>
 <8> Experiments on 44 languages, covering mid-high-, low-, and zero-resource scenarios, verify the effectiveness and superiority of the proposed approach, which achieves state-of-the-art performance under all scenarios. </8>
 <9> Additionally, we will contribute a large-scale multilingual multimodal abstractive summarization (MM-Sum) dataset. </9>
"
1812.06604,Siamese Networks for Semantic Pattern Similarity,"  Semantic Pattern Similarity is an interesting, though not often encountered
NLP task where two sentences are compared not by their specific meaning, but by
their more abstract semantic pattern (e.g., preposition or frame). We utilize
Siamese Networks to model this task, and show its usefulness in determining SQL
patterns for unseen questions in a database-backed question answering scenario.
Our approach achieves high accuracy and contains a built-in proxy for
confidence, which can be used to keep precision arbitrarily high.
","<1> Semantic Pattern Similarity is an interesting, though not often encountered NLP task where two sentences are compared not by their specific meaning, but by their more abstract semantic pattern (e.g., preposition or frame). </1>
 <2> We utilize Siamese Networks to model this task, and show its usefulness in determining SQL patterns for unseen questions in a database-backed question answering scenario. </2>
 <3>  </3>
 <4> Our approach achieves high accuracy and contains a built-in proxy for confidence, which can be used to keep precision arbitrarily high. </4>
"
2112.05973,Prosody Labelled Dataset for Hindi using Semi-Automated Approach,"  This study aims to develop a semi-automatically labelled prosody database for
Hindi, for enhancing the intonation component in ASR and TTS systems, which is
also helpful for building Speech to Speech Machine Translation systems.
Although no single standard for prosody labelling exists in Hindi, researchers
in the past have employed perceptual and statistical methods in literature to
draw inferences about the behaviour of prosody patterns in Hindi. Based on such
existing research and largely agreed upon theories of intonation in Hindi, this
study attempts to first develop a manually annotated prosodic corpus of Hindi
speech data, which is then used for training prediction models for generating
automatic prosodic labels. A total of 5,000 sentences (23,500 words) for
declarative and interrogative types have been labelled. The accuracy of the
trained models for pitch accent, intermediate phrase boundaries and accentual
phrase boundaries is 73.40%, 93.20%, and 43% respectively.
","<1> This study aims to develop a semi-automatically labelled prosody database for Hindi, for enhancing the intonation component in ASR and TTS systems, which is also helpful for building Speech to Speech Machine Translation systems. </1>
 <2>  </2>
 <3> Although no single standard for prosody labelling exists in Hindi, researchers in the past have employed perceptual and statistical methods in literature to draw inferences about the behaviour of prosody patterns in Hindi. </3>
 <4> Based on such existing research and largely agreed upon theories of intonation in Hindi, this study attempts to first develop a manually annotated prosodic corpus of Hindi speech data, which is then used for training prediction models for generating automatic prosodic labels. </4>
 <5> A total of 5,000 sentences (23,500 words) for declarative and interrogative types have been labelled. </5>
 <6> The accuracy of the trained models for pitch accent, intermediate phrase boundaries and accentual phrase boundaries is 73.40%, 93.20%, and 43% respectively. </6>
"
1912.03234,"What Do You Mean I'm Funny? Personalizing the Joke Skill of a
  Voice-Controlled Virtual Assistant","  A considerable part of the success experienced by Voice-controlled virtual
assistants (VVA) is due to the emotional and personalized experience they
deliver, with humor being a key component in providing an engaging interaction.
In this paper we describe methods used to improve the joke skill of a VVA
through personalization. The first method, based on traditional NLP techniques,
is robust and scalable. The others combine self-attentional network and
multi-task learning to obtain better results, at the cost of added complexity.
A significant challenge facing these systems is the lack of explicit user
feedback needed to provide labels for the models. Instead, we explore the use
of two implicit feedback-based labelling strategies. All models were evaluated
on real production data. Online results show that models trained on any of the
considered labels outperform a heuristic method, presenting a positive
real-world impact on user satisfaction. Offline results suggest that the
deep-learning approaches can improve the joke experience with respect to the
other considered methods.
","<1> A considerable part of the success experienced by Voice-controlled virtual assistants (VVA) is due to the emotional and personalized experience they deliver, with humor being a key component in providing an engaging interaction. </1>
 <2>  </2>
 <3> In this paper we describe methods used to improve the joke skill of a VVA through personalization. </3>
 <4> The first method, based on traditional NLP techniques, is robust and scalable. </4>
 <5> The others combine self-attentional network and multi-task learning to obtain better results, at the cost of added complexity. </5>
 <6>  </6>
 <7> A significant challenge facing these systems is the lack of explicit user feedback needed to provide labels for the models. </7>
 <8> Instead, we explore the use of two implicit feedback-based labelling strategies. </8>
 <9> All models were evaluated on real production data. </9>
 <10> Online results show that models trained on any of the considered labels outperform a heuristic method, presenting a positive real-world impact on user satisfaction. </10>
 <11> Offline results suggest that the deep-learning approaches can improve the joke experience with respect to the other considered methods. </11>
"
1512.07281,Topical differences between Chinese language Twitter and Sina Weibo,"  Sina Weibo, China's most popular microblogging platform, is currently used by
over $500M$ users and is considered to be a proxy of Chinese social life. In
this study, we contrast the discussions occurring on Sina Weibo and on Chinese
language Twitter in order to observe two different strands of Chinese culture:
people within China who use Sina Weibo with its government imposed restrictions
and those outside that are free to speak completely anonymously. We first
propose a simple ad-hoc algorithm to identify topics of Tweets and Weibo.
Different from previous works on micro-message topic detection, our algorithm
considers topics of the same contents but with different \#tags. Our algorithm
can also detect topics for Tweets and Weibos without any \#tags. Using a large
corpus of Weibo and Chinese language tweets, covering the period from January
$1$ to December $31$, $2012$, we obtain a list of topics using clustered \#tags
that we can then use to compare the two platforms. Surprisingly, we find that
there are no common entries among the Top $100$ most popular topics.
Furthermore, only $9.2\%$ of tweets correspond to the Top $1000$ topics on Sina
Weibo platform, and conversely only $4.4\%$ of weibos were found to discuss the
most popular Twitter topics. Our results reveal significant differences in
social attention on the two platforms, with most popular topics on Sina Weibo
relating to entertainment while most tweets corresponded to cultural or
political contents that is practically non existent in Sina Weibo.
","<1> Sina Weibo, China's most popular microblogging platform, is currently used by over $500M$ users and is considered to be a proxy of Chinese social life. </1>
 <2> In this study, we contrast the discussions occurring on Sina Weibo and on Chinese language Twitter in order to observe two different strands of Chinese culture: people within China who use Sina Weibo with its government imposed restrictions and those outside that are free to speak completely anonymously. </2>
 <3> We first propose a simple ad-hoc algorithm to identify topics of Tweets and Weibo. </3>
 <4>  </4>
 <5> Different from previous works on micro-message topic detection, our algorithm considers topics of the same contents but with different \#tags. </5>
 <6> Our algorithm can also detect topics for Tweets and Weibos without any \#tags. </6>
 <7> Using a large corpus of Weibo and Chinese language tweets, covering the period from January $1$ to December $31$, $2012$, we obtain a list of topics using clustered \#tags that we can then use to compare the two platforms. </7>
 <8> Surprisingly, we find that there are no common entries among the Top $100$ most popular topics. </8>
 <9>  </9>
 <10> Furthermore, only $9.2\%$ of tweets correspond to the Top $1000$ topics on Sina Weibo platform, and conversely only $4.4\%$ of weibos were found to discuss the most popular Twitter topics. </10>
 <11> Our results reveal significant differences in social attention on the two platforms, with most popular topics on Sina Weibo relating to entertainment while most tweets corresponded to cultural or political contents that is practically non existent in Sina Weibo. </11>
"
2012.07528,Disentangling Homophemes in Lip Reading using Perplexity Analysis,"  The performance of automated lip reading using visemes as a classification
schema has achieved less success compared with the use of ASCII characters and
words largely due to the problem of different words sharing identical visemes.
The Generative Pre-Training transformer is an effective autoregressive language
model used for many tasks in Natural Language Processing, including sentence
prediction and text classification.
  This paper proposes a new application for this model and applies it in the
context of lip reading, where it serves as a language model to convert visual
speech in the form of visemes, to language in the form of words and sentences.
The network uses the search for optimal perplexity to perform the
viseme-to-word mapping and is thus a solution to the one-to-many mapping
problem that exists whereby various words that sound different when spoken look
identical. This paper proposes a method to tackle the one-to-many mapping
problem when performing automated lip reading using solely visual cues in two
separate scenarios: the first scenario is where the word boundary, that is, the
beginning and the ending of a word, is unknown; and the second scenario is
where the boundary is known.
  Sentences from the benchmark BBC dataset ""Lip Reading Sentences in the
Wild""(LRS2), are classified with a character error rate of 10.7% and a word
error rate of 18.0%. The main contribution of this paper is to propose a method
of predicting words through the use of perplexity analysis when only visual
cues are present, using an autoregressive language model.
","<1>  </1>
 <2> The performance of automated lip reading using visemes as a classification schema has achieved less success compared with the use of ASCII characters and words largely due to the problem of different words sharing identical visemes. </2>
 <3>  </3>
 <4> The Generative Pre-Training transformer is an effective autoregressive language model used for many tasks in Natural Language Processing, including sentence prediction and text classification. </4>
 <5> This paper proposes a new application for this model and applies it in the context of lip reading, where it serves as a language model to convert visual speech in the form of visemes, to language in the form of words and sentences. </5>
 <6>  </6>
 <7> The network uses the search for optimal perplexity to perform the viseme-to-word mapping and is thus a solution to the one-to-many mapping problem that exists whereby various words that sound different when spoken look identical. </7>
 <8> This paper proposes a method to tackle the one-to-many mapping problem when performing automated lip reading using solely visual cues in two separate scenarios: the first scenario is where the word boundary, that is, the beginning and the ending of a word, is unknown; and the second scenario is where the boundary is known. </8>
 <9> Sentences from the benchmark BBC dataset ""Lip Reading Sentences in the Wild""(LRS2), are classified with a character error rate of 10.7% and a word error rate of 18.0%. </9>
 <10> The main contribution of this paper is to propose a method of predicting words through the use of perplexity analysis when only visual cues are present, using an autoregressive language model. </10>
"
1912.02636,"Exploration of Neural Machine Translation in Autoformalization of
  Mathematics in Mizar","  In this paper we share several experiments trying to automatically translate
informal mathematics into formal mathematics. In our context informal
mathematics refers to human-written mathematical sentences in the LaTeX format;
and formal mathematics refers to statements in the Mizar language. We conducted
our experiments against three established neural network-based machine
translation models that are known to deliver competitive results on translating
between natural languages. To train these models we also prepared four
informal-to-formal datasets. We compare and analyze our results according to
whether the model is supervised or unsupervised. In order to augment the data
available for auto-formalization and improve the results, we develop a custom
type-elaboration mechanism and integrate it in the supervised translation.
","<1>  </1>
 <2> In this paper we share several experiments trying to automatically translate informal mathematics into formal mathematics. </2>
 <3> In our context informal mathematics refers to human-written mathematical sentences in the LaTeX format; and formal mathematics refers to statements in the Mizar language. </3>
 <4> We conducted our experiments against three established neural network-based machine translation models that are known to deliver competitive results on translating between natural languages. </4>
 <5> To train these models we also prepared four informal-to-formal datasets. </5>
 <6> We compare and analyze our results according to whether the model is supervised or unsupervised. </6>
 <7> In order to augment the data available for auto-formalization and improve the results, we develop a custom type-elaboration mechanism and integrate it in the supervised translation. </7>
"
2112.13790,"Contextual Sentence Analysis for the Sentiment Prediction on Financial
  Data","  Newsletters and social networks can reflect the opinion about the market and
specific stocks from the perspective of analysts and the general public on
products and/or services provided by a company. Therefore, sentiment analysis
of these texts can provide useful information to help investors trade in the
market. In this paper, a hierarchical stack of Transformers model is proposed
to identify the sentiment associated with companies and stocks, by predicting a
score (of data type real) in a range between -1 and +1. Specifically, we
fine-tuned a RoBERTa model to process headlines and microblogs and combined it
with additional Transformer layers to process the sentence analysis with
sentiment dictionaries to improve the sentiment analysis. We evaluated it on
financial data released by SemEval-2017 task 5 and our proposition outperformed
the best systems of SemEval-2017 task 5 and strong baselines. Indeed, the
combination of contextual sentence analysis with the financial and general
sentiment dictionaries provided useful information to our model and allowed it
to generate more reliable sentiment scores.
","<1> Newsletters and social networks can reflect the opinion about the market and specific stocks from the perspective of analysts and the general public on products and/or services provided by a company. </1>
 <2> Therefore, sentiment analysis of these texts can provide useful information to help investors trade in the market. </2>
 <3> In this paper, a hierarchical stack of Transformers model is proposed to identify the sentiment associated with companies and stocks, by predicting a score (of data type real) in a range between -1 and +1. </3>
 <4> Specifically, we fine-tuned a RoBERTa model to process headlines and microblogs and combined it with additional Transformer layers to process the sentence analysis with sentiment dictionaries to improve the sentiment analysis. </4>
 <5> We evaluated it on financial data released by SemEval-2017 task 5 and our proposition outperformed the best systems of SemEval-2017 task 5 and strong baselines. </5>
 <6> Indeed, the combination of contextual sentence analysis with the financial and general sentiment dictionaries provided useful information to our model and allowed it to generate more reliable sentiment scores. </6>
"
1812.08039,Semantic Frame Parsing for Information Extraction : the CALOR corpus,"  This paper presents a publicly available corpus of French encyclopedic
history texts annotated according to the Berkeley FrameNet formalism. The main
difference in our approach compared to previous works on semantic parsing with
FrameNet is that we are not interested here in full text parsing but rather on
partial parsing. The goal is to select from the FrameNet resources the minimal
set of frames that are going to be useful for the applicative framework
targeted, in our case Information Extraction from encyclopedic documents. Such
an approach leverages the manual annotation of larger corpora than those
obtained through full text parsing and therefore opens the door to alternative
methods for Frame parsing than those used so far on the FrameNet 1.5 benchmark
corpus. The approaches compared in this study rely on an integrated sequence
labeling model which jointly optimizes frame identification and semantic role
segmentation and identification. The models compared are CRFs and multitasks
bi-LSTMs.
","<1> This paper presents a publicly available corpus of French encyclopedic history texts annotated according to the Berkeley FrameNet formalism. </1>
 <2> The main difference in our approach compared to previous works on semantic parsing with </2>
 <3> FrameNet is that we are not interested here in full text parsing but rather on partial parsing. </3>
 <4> The goal is to select from the FrameNet resources the minimal set of frames that are going to be useful for the applicative framework targeted, in our case Information Extraction from encyclopedic documents. </4>
 <5> Such an approach leverages the manual annotation of larger corpora than those obtained through full text parsing and therefore opens the door to alternative methods for Frame parsing than those used so far on the FrameNet 1.5 benchmark corpus. </5>
 <6> The approaches compared in this study rely on an integrated sequence labeling model which jointly optimizes frame identification and semantic role segmentation and identification. </6>
 <7> The models compared are CRFs and multitasks bi-LSTMs. </7>
"
1812.00271,Learning Speaker Representations with Mutual Information,"  Learning good representations is of crucial importance in deep learning.
Mutual Information (MI) or similar measures of statistical dependence are
promising tools for learning these representations in an unsupervised way. Even
though the mutual information between two random variables is hard to measure
directly in high dimensional spaces, some recent studies have shown that an
implicit optimization of MI can be achieved with an encoder-discriminator
architecture similar to that of Generative Adversarial Networks (GANs). In this
work, we learn representations that capture speaker identities by maximizing
the mutual information between the encoded representations of chunks of speech
randomly sampled from the same sentence. The proposed encoder relies on the
SincNet architecture and transforms raw speech waveform into a compact feature
vector. The discriminator is fed by either positive samples (of the joint
distribution of encoded chunks) or negative samples (from the product of the
marginals) and is trained to separate them. We report experiments showing that
this approach effectively learns useful speaker representations, leading to
promising results on speaker identification and verification tasks. Our
experiments consider both unsupervised and semi-supervised settings and compare
the performance achieved with different objective functions.
","<1> Learning good representations is of crucial importance in deep learning. </1>
 <2>  </2>
 <3> Mutual Information (MI) or similar measures of statistical dependence are promising tools for learning these representations in an unsupervised way. </3>
 <4> Even though the mutual information between two random variables is hard to measure directly in high dimensional spaces, some recent studies have shown that an implicit optimization of MI can be achieved with an encoder-discriminator architecture similar to that of Generative Adversarial Networks (GANs). </4>
 <5> In this work, we learn representations that capture speaker identities by maximizing the mutual information between the encoded representations of chunks of speech randomly sampled from the same sentence. </5>
 <6> The proposed encoder relies on the SincNet architecture and transforms raw speech waveform into a compact feature vector. </6>
 <7> The discriminator is fed by either positive samples (of the joint distribution of encoded chunks) or negative samples (from the product of the marginals) and is trained to separate them. </7>
 <8> We report experiments showing that this approach effectively learns useful speaker representations, leading to promising results on speaker identification and verification tasks. </8>
 <9> Our experiments consider both unsupervised and semi-supervised settings and compare the performance achieved with different objective functions. </9>
"
1512.02595,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,"  We show that an end-to-end deep learning approach can be used to recognize
either English or Mandarin Chinese speech--two vastly different languages.
Because it replaces entire pipelines of hand-engineered components with neural
networks, end-to-end learning allows us to handle a diverse variety of speech
including noisy environments, accents and different languages. Key to our
approach is our application of HPC techniques, resulting in a 7x speedup over
our previous system. Because of this efficiency, experiments that previously
took weeks now run in days. This enables us to iterate more quickly to identify
superior architectures and algorithms. As a result, in several cases, our
system is competitive with the transcription of human workers when benchmarked
on standard datasets. Finally, using a technique called Batch Dispatch with
GPUs in the data center, we show that our system can be inexpensively deployed
in an online setting, delivering low latency when serving users at scale.
","<1> We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. </1>
 <2> Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. </2>
 <3> Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. </3>
 <4> Because of this efficiency, experiments that previously took weeks now run in days. </4>
 <5> This enables us to iterate more quickly to identify superior architectures and algorithms. </5>
 <6> As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. </6>
 <7> Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale. </7>
"
2212.04348,Implicit causality in GPT-2: a case study,"  This case study investigates the extent to which a language model (GPT-2) is
able to capture native speakers' intuitions about implicit causality in a
sentence completion task. We first reproduce earlier results (showing lower
surprisal values for pronouns that are congruent with either the subject or
object, depending on which one corresponds to the implicit causality bias of
the verb), and then examine the effects of gender and verb frequency on model
performance. Our second study examines the reasoning ability of GPT-2: is the
model able to produce more sensible motivations for why the subject VERBed the
object if the verbs have stronger causality biases? We also developed a
methodology to avoid human raters being biased by obscenities and disfluencies
generated by the model.
","<1>  </1>
 <2> This case study investigates the extent to which a language model (GPT-2) is able to capture native speakers' intuitions about implicit causality in a sentence completion task. </2>
 <3> We first reproduce earlier results (showing lower surprisal values for pronouns that are congruent with either the subject or object, depending on which one corresponds to the implicit causality bias of the verb), and then examine the effects of gender and verb frequency on model performance. </3>
 <4> Our second study examines the reasoning ability of GPT-2: is the model able to produce more sensible motivations for why the subject VERBed the object if the verbs have stronger causality biases? </4>
 <5> We also developed a methodology to avoid human raters being biased by obscenities and disfluencies generated by the model. </5>
"
1812.02354,"Multi-Task Learning with Multi-View Attention for Answer Selection and
  Knowledge Base Question Answering","  Answer selection and knowledge base question answering (KBQA) are two
important tasks of question answering (QA) systems. Existing methods solve
these two tasks separately, which requires large number of repetitive work and
neglects the rich correlation information between tasks. In this paper, we
tackle answer selection and KBQA tasks simultaneously via multi-task learning
(MTL), motivated by the following motivations. First, both answer selection and
KBQA can be regarded as a ranking problem, with one at text-level while the
other at knowledge-level. Second, these two tasks can benefit each other:
answer selection can incorporate the external knowledge from knowledge base
(KB), while KBQA can be improved by learning contextual information from answer
selection. To fulfill the goal of jointly learning these two tasks, we propose
a novel multi-task learning scheme that utilizes multi-view attention learned
from various perspectives to enable these tasks to interact with each other as
well as learn more comprehensive sentence representations. The experiments
conducted on several real-world datasets demonstrate the effectiveness of the
proposed method, and the performance of answer selection and KBQA is improved.
Also, the multi-view attention scheme is proved to be effective in assembling
attentive information from different representational perspectives.
","<1> Answer selection and knowledge base question answering (KBQA) are two important tasks of question answering (QA) systems. </1>
 <2> Existing methods solve these two tasks separately, which requires large number of repetitive work and neglects the rich correlation information between tasks. </2>
 <3> In this paper, we tackle answer selection and KBQA tasks simultaneously via multi-task learning </3>
 <4> (MTL), motivated by the following motivations. </4>
 <5> First, both answer selection and KBQA can be regarded as a ranking problem, with one at text-level while the other at knowledge-level. </5>
 <6> Second, these two tasks can benefit each other: answer selection can incorporate the external knowledge from knowledge base </6>
 <7> (KB), while KBQA can be improved by learning contextual information from answer selection. </7>
 <8> To fulfill the goal of jointly learning these two tasks, we propose a novel multi-task learning scheme that utilizes multi-view attention learned from various perspectives to enable these tasks to interact with each other as well as learn more comprehensive sentence representations. </8>
 <9> The experiments conducted on several real-world datasets demonstrate the effectiveness of the proposed method, and the performance of answer selection and KBQA is improved. </9>
 <10>  </10>
 <11> Also, the multi-view attention scheme is proved to be effective in assembling attentive information from different representational perspectives. </11>
"
1212.2036,"Query-focused Multi-document Summarization: Combining a Novel Topic
  Model with Graph-based Semi-supervised Learning","  Graph-based semi-supervised learning has proven to be an effective approach
for query-focused multi-document summarization. The problem of previous
semi-supervised learning is that sentences are ranked without considering the
higher level information beyond sentence level. Researches on general
summarization illustrated that the addition of topic level can effectively
improve the summary quality. Inspired by previous researches, we propose a
two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised
learning approach. At the same time, we propose a novel topic model which makes
full use of the dependence between sentences and words. Experimental results on
DUC and TAC data sets demonstrate the effectiveness of our proposed approach.
","<1> Graph-based semi-supervised learning has proven to be an effective approach for query-focused multi-document summarization. </1>
 <2> The problem of previous semi-supervised learning is that sentences are ranked without considering the higher level information beyond sentence level. </2>
 <3> Researches on general summarization illustrated that the addition of topic level can effectively improve the summary quality. </3>
 <4> Inspired by previous researches, we propose a two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised learning approach. </4>
 <5> At the same time, we propose a novel topic model which makes full use of the dependence between sentences and words. </5>
 <6> Experimental results on DUC and TAC data sets demonstrate the effectiveness of our proposed approach. </6>
"
2212.07425,"Robust and Explainable Identification of Logical Fallacies in Natural
  Language Arguments","  The spread of misinformation, propaganda, and flawed argumentation has been
amplified in the Internet era. Given the volume of data and the subtlety of
identifying violations of argumentation norms, supporting information analytics
tasks, like content moderation, with trustworthy methods that can identify
logical fallacies is essential. In this paper, we formalize prior theoretical
work on logical fallacies into a comprehensive three-stage evaluation framework
of detection, coarse-grained, and fine-grained classification. We adapt
existing evaluation datasets for each stage of the evaluation. We employ three
families of robust and explainable methods based on prototype reasoning,
instance-based reasoning, and knowledge injection. The methods combine language
models with background knowledge and explainable mechanisms. Moreover, we
address data sparsity with strategies for data augmentation and curriculum
learning. Our three-stage framework natively consolidates prior datasets and
methods from existing tasks, like propaganda detection, serving as an
overarching evaluation testbed. We extensively evaluate these methods on our
datasets, focusing on their robustness and explainability. Our results provide
insight into the strengths and weaknesses of the methods on different
components and fallacy classes, indicating that fallacy identification is a
challenging task that may require specialized forms of reasoning to capture
various classes. We share our open-source code and data on GitHub to support
further work on logical fallacy identification.
","<1>  </1>
 <2> The spread of misinformation, propaganda, and flawed argumentation has been amplified in the Internet era. </2>
 <3> Given the volume of data and the subtlety of identifying violations of argumentation norms, supporting information analytics tasks, like content moderation, with trustworthy methods that can identify logical fallacies is essential. </3>
 <4> In this paper, we formalize prior theoretical work on logical fallacies into a comprehensive three-stage evaluation framework of detection, coarse-grained, and fine-grained classification. </4>
 <5> We adapt existing evaluation datasets for each stage of the evaluation. </5>
 <6> We employ three families of robust and explainable methods based on prototype reasoning, instance-based reasoning, and knowledge injection. </6>
 <7> The methods combine language models with background knowledge and explainable mechanisms. </7>
 <8> Moreover, we address data sparsity with strategies for data augmentation and curriculum learning. </8>
 <9> Our three-stage framework natively consolidates prior datasets and methods from existing tasks, like propaganda detection, serving as an overarching evaluation testbed. </9>
 <10> We extensively evaluate these methods on our datasets, focusing on their robustness and explainability. </10>
 <11> Our results provide insight into the strengths and weaknesses of the methods on different components and fallacy classes, indicating that fallacy identification is a challenging task that may require specialized forms of reasoning to capture various classes. </11>
 <12> We share our open-source code and data on GitHub to support further work on logical fallacy identification. </12>
"
2212.06933,"Paraphrase Identification with Deep Learning: A Review of Datasets and
  Methods","  The rapid advancement of AI technology has made text generation tools like
GPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can
pose serious threat to the credibility of various forms of media if these
technologies are used for plagiarism, including scientific literature and news
sources. Despite the development of automated methods for paraphrase
identification, detecting this type of plagiarism remains a challenge due to
the disparate nature of the datasets on which these methods are trained. In
this study, we review traditional and current approaches to paraphrase
identification and propose a refined typology of paraphrases. We also
investigate how this typology is represented in popular datasets and how
under-representation of certain types of paraphrases impacts detection
capabilities. Finally, we outline new directions for future research and
datasets in the pursuit of more effective paraphrase detection using AI.
","<1> The rapid advancement of AI technology has made text generation tools like GPT-3 and ChatGPT increasingly accessible, scalable, and effective. </1>
 <2> This can pose serious threat to the credibility of various forms of media if these technologies are used for plagiarism, including scientific literature and news sources. </2>
 <3> Despite the development of automated methods for paraphrase identification, detecting this type of plagiarism remains a challenge due to the disparate nature of the datasets on which these methods are trained. </3>
 <4> In this study, we review traditional and current approaches to paraphrase identification and propose a refined typology of paraphrases. </4>
 <5> We also investigate how this typology is represented in popular datasets and how under-representation of certain types of paraphrases impacts detection capabilities. </5>
 <6> Finally, we outline new directions for future research and datasets in the pursuit of more effective paraphrase detection using AI. </6>
"
2212.05546,"Associations Between Natural Language Processing (NLP) Enriched Social
  Determinants of Health and Suicide Death among US Veterans","  Importance: Social determinants of health (SDOH) are known to be associated
with increased risk of suicidal behaviors, but few studies utilized SDOH from
unstructured electronic health record (EHR) notes.
  Objective: To investigate associations between suicide and recent SDOH,
identified using structured and unstructured data.
  Design: Nested case-control study.
  Setting: EHR data from the US Veterans Health Administration (VHA).
  Participants: 6,122,785 Veterans who received care in the US VHA between
October 1, 2010, and September 30, 2015.
  Exposures: Occurrence of SDOH over a maximum span of two years compared with
no occurrence of SDOH.
  Main Outcomes and Measures: Cases of suicide deaths were matched with 4
controls on birth year, cohort entry date, sex, and duration of follow-up. We
developed an NLP system to extract SDOH from unstructured notes. Structured
data, NLP on unstructured data, and combining them yielded six, eight and nine
SDOH respectively. Adjusted odds ratios (aORs) and 95% confidence intervals
(CIs) were estimated using conditional logistic regression.
  Results: In our cohort, 8,821 Veterans committed suicide during 23,725,382
person-years of follow-up (incidence rate 37.18/100,000 person-years). Our
cohort was mostly male (92.23%) and white (76.99%). Across the five common SDOH
as covariates, NLP-extracted SDOH, on average, covered 80.03% of all SDOH
occurrences. All SDOH, measured by structured data and NLP, were significantly
associated with increased risk of suicide. The SDOH with the largest effects
was legal problems (aOR=2.66, 95% CI=.46-2.89), followed by violence (aOR=2.12,
95% CI=1.98-2.27). NLP-extracted and structured SDOH were also associated with
suicide.
  Conclusions and Relevance: NLP-extracted SDOH were always significantly
associated with increased risk of suicide among Veterans, suggesting the
potential of NLP in public health studies.
","<1> Importance: Social determinants of health (SDOH) are known to be associated with increased risk of suicidal behaviors, but few studies utilized SDOH from unstructured electronic health record (EHR) notes. </1>
 <2> Objective: To investigate associations between suicide and recent SDOH, identified using structured and unstructured data.   Design: Nested case-control study. </2>
 <3> Setting: EHR data from the US Veterans Health Administration (VHA). </3>
 <4> Participants: 6,122,785 Veterans who received care in the US VHA between October 1, 2010, and September 30, 2015. </4>
 <5> Exposures: Occurrence of SDOH over a maximum span of two years compared with no occurrence of SDOH. </5>
 <6> Main Outcomes and Measures: Cases of suicide deaths were matched with 4 controls on birth year, cohort entry date, sex, and duration of follow-up. </6>
 <7> We developed an NLP system to extract SDOH from unstructured notes. </7>
 <8> Structured data, NLP on unstructured data, and combining them yielded six, eight and nine SDOH respectively. </8>
 <9> Adjusted odds ratios (aORs) and 95% confidence intervals (CIs) were estimated using conditional logistic regression. </9>
 <10> Results: In our cohort, 8,821 Veterans committed suicide during 23,725,382 person-years of follow-up (incidence rate 37.18/100,000 person-years). </10>
 <11> Our cohort was mostly male (92.23%) and white (76.99%). </11>
 <12> Across the five common SDOH as covariates, NLP-extracted SDOH, on average, covered 80.03% of all SDOH occurrences. </12>
 <13> All SDOH, measured by structured data and NLP, were significantly associated with increased risk of suicide. </13>
 <14> The SDOH with the largest effects was legal problems (aOR=2.66, 95% CI=.46-2.89), followed by violence (aOR=2.12, 95% CI=1.98-2.27). </14>
 <15> NLP-extracted and structured SDOH were also associated with suicide. </15>
 <16> Conclusions and Relevance: NLP-extracted SDOH were always significantly associated with increased risk of suicide among Veterans, suggesting the potential of NLP in public health studies. </16>
"
1912.06927,"#MeTooMA: Multi-Aspect Annotations of Tweets Related to the MeToo
  Movement","  In this paper, we present a dataset containing 9,973 tweets related to the
MeToo movement that were manually annotated for five different linguistic
aspects: relevance, stance, hate speech, sarcasm, and dialogue acts. We present
a detailed account of the data collection and annotation processes. The
annotations have a very high inter-annotator agreement (0.79 to 0.93 k-alpha)
due to the domain expertise of the annotators and clear annotation
instructions. We analyze the data in terms of geographical distribution, label
correlations, and keywords. Lastly, we present some potential use cases of this
dataset. We expect this dataset would be of great interest to psycholinguists,
socio-linguists, and computational linguists to study the discursive space of
digitally mobilized social movements on sensitive issues like sexual
harassment.
","<1>  </1>
 <2> In this paper, we present a dataset containing 9,973 tweets related to the MeToo movement that were manually annotated for five different linguistic aspects: relevance, stance, hate speech, sarcasm, and dialogue acts. </2>
 <3> We present a detailed account of the data collection and annotation processes. </3>
 <4> The annotations have a very high inter-annotator agreement (0.79 to 0.93 k-alpha) due to the domain expertise of the annotators and clear annotation instructions. </4>
 <5> We analyze the data in terms of geographical distribution, label correlations, and keywords. </5>
 <6> Lastly, we present some potential use cases of this dataset. </6>
 <7> We expect this dataset would be of great interest to psycholinguists, socio-linguists, and computational linguists to study the discursive space of digitally mobilized social movements on sensitive issues like sexual harassment. </7>
"
1912.06813,"Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using
  Transformer with Text-to-Speech Pretraining","  We introduce a novel sequence-to-sequence (seq2seq) voice conversion (VC)
model based on the Transformer architecture with text-to-speech (TTS)
pretraining. Seq2seq VC models are attractive owing to their ability to convert
prosody. While seq2seq models based on recurrent neural networks (RNNs) and
convolutional neural networks (CNNs) have been successfully applied to VC, the
use of the Transformer network, which has shown promising results in various
speech processing tasks, has not yet been investigated. Nonetheless, their
data-hungry property and the mispronunciation of converted speech make seq2seq
models far from practical. To this end, we propose a simple yet effective
pretraining technique to transfer knowledge from learned TTS models, which
benefit from large-scale, easily accessible TTS corpora. VC models initialized
with such pretrained model parameters are able to generate effective hidden
representations for high-fidelity, highly intelligible converted speech.
Experimental results show that such a pretraining scheme can facilitate
data-efficient training and outperform an RNN-based seq2seq VC model in terms
of intelligibility, naturalness, and similarity.
","<1> We introduce a novel sequence-to-sequence (seq2seq) voice conversion (VC) model based on the Transformer architecture with text-to-speech (TTS) pretraining. </1>
 <2> Seq2seq VC models are attractive owing to their ability to convert prosody. </2>
 <3> While seq2seq models based on recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been successfully applied to VC, the use of the Transformer network, which has shown promising results in various speech processing tasks, has not yet been investigated. </3>
 <4> Nonetheless, their data-hungry property and the mispronunciation of converted speech make seq2seq models far from practical. </4>
 <5> To this end, we propose a simple yet effective pretraining technique to transfer knowledge from learned TTS models, which benefit from large-scale, easily accessible TTS corpora. </5>
 <6> VC models initialized with such pretrained model parameters are able to generate effective hidden representations for high-fidelity, highly intelligible converted speech. </6>
 <7>  </7>
 <8> Experimental results show that such a pretraining scheme can facilitate data-efficient training and outperform an RNN-based seq2seq VC model in terms of intelligibility, naturalness, and similarity. </8>
"
2112.04344,"Does Structure Matter? Leveraging Data-to-Text Generation for Answering
  Complex Information Needs","  In this work, our aim is to provide a structured answer in natural language
to a complex information need. Particularly, we envision using generative
models from the perspective of data-to-text generation. We propose the use of a
content selection and planning pipeline which aims at structuring the answer by
generating intermediate plans. The experimental evaluation is performed using
the TREC Complex Answer Retrieval (CAR) dataset. We evaluate both the generated
answer and its corresponding structure and show the effectiveness of
planning-based models in comparison to a text-to-text model.
","<1>  </1>
 <2> In this work, our aim is to provide a structured answer in natural language to a complex information need. </2>
 <3> Particularly, we envision using generative models from the perspective of data-to-text generation. </3>
 <4> We propose the use of a content selection and planning pipeline which aims at structuring the answer by generating intermediate plans. </4>
 <5> The experimental evaluation is performed using the TREC Complex Answer Retrieval (CAR) dataset. </5>
 <6> We evaluate both the generated answer and its corresponding structure and show the effectiveness of planning-based models in comparison to a text-to-text model. </6>
"
2112.06603,"Detecting Emotion Carriers by Combining Acoustic and Lexical
  Representations","  Personal narratives (PN) - spoken or written - are recollections of facts,
people, events, and thoughts from one's own experience. Emotion recognition and
sentiment analysis tasks are usually defined at the utterance or document
level. However, in this work, we focus on Emotion Carriers (EC) defined as the
segments (speech or text) that best explain the emotional state of the narrator
(""loss of father"", ""made me choose""). Once extracted, such EC can provide a
richer representation of the user state to improve natural language
understanding and dialogue modeling. In previous work, it has been shown that
EC can be identified using lexical features. However, spoken narratives should
provide a richer description of the context and the users' emotional state. In
this paper, we leverage word-based acoustic and textual embeddings as well as
early and late fusion techniques for the detection of ECs in spoken narratives.
For the acoustic word-level representations, we use Residual Neural Networks
(ResNet) pretrained on separate speech emotion corpora and fine-tuned to detect
EC. Experiments with different fusion and system combination strategies show
that late fusion leads to significant improvements for this task.
","<1> Personal narratives (PN) - spoken or written - are recollections of facts, people, events, and thoughts from one's own experience. </1>
 <2> Emotion recognition and sentiment analysis tasks are usually defined at the utterance or document level. </2>
 <3> However, in this work, we focus on Emotion Carriers (EC) defined as the segments (speech or text) that best explain the emotional state of the narrator (""loss of father"", ""made me choose""). </3>
 <4> Once extracted, such EC can provide a richer representation of the user state to improve natural language understanding and dialogue modeling. </4>
 <5> In previous work, it has been shown that EC can be identified using lexical features. </5>
 <6> However, spoken narratives should provide a richer description of the context and the users' emotional state. </6>
 <7> In this paper, we leverage word-based acoustic and textual embeddings as well as early and late fusion techniques for the detection of ECs in spoken narratives. </7>
 <8>  </8>
 <9> For the acoustic word-level representations, we use Residual Neural Networks </9>
 <10> (ResNet) pretrained on separate speech emotion corpora and fine-tuned to detect EC. </10>
 <11> Experiments with different fusion and system combination strategies show that late fusion leads to significant improvements for this task. </11>
"
1912.09008,Discriminative Sentence Modeling for Story Ending Prediction,"  Story Ending Prediction is a task that needs to select an appropriate ending
for the given story, which requires the machine to understand the story and
sometimes needs commonsense knowledge. To tackle this task, we propose a new
neural network called Diff-Net for better modeling the differences of each
ending in this task. The proposed model could discriminate two endings in three
semantic levels: contextual representation, story-aware representation, and
discriminative representation. Experimental results on the Story Cloze Test
dataset show that the proposed model siginificantly outperforms various systems
by a large margin, and detailed ablation studies are given for better
understanding our model. We also carefully examine the traditional and
BERT-based models on both SCT v1.0 and v1.5 with interesting findings that may
potentially help future studies.
","<1> Story Ending Prediction is a task that needs to select an appropriate ending for the given story, which requires the machine to understand the story and sometimes needs commonsense knowledge. </1>
 <2> To tackle this task, we propose a new neural network called Diff-Net for better modeling the differences of each ending in this task. </2>
 <3> The proposed model could discriminate two endings in three semantic levels: contextual representation, story-aware representation, and discriminative representation. </3>
 <4> Experimental results on the Story Cloze Test dataset show that the proposed model siginificantly outperforms various systems by a large margin, and detailed ablation studies are given for better understanding our model. </4>
 <5> We also carefully examine the traditional and BERT-based models on both SCT v1.0 and v1.5 with interesting findings that may potentially help future studies. </5>
"
2012.07521,"Procode: the Swiss Multilingual Solution for Automatic Coding and
  Recoding of Occupations and Economic Activities","  Objective. Epidemiological studies require data that are in alignment with
the classifications established for occupations or economic activities. The
classifications usually include hundreds of codes and titles. Manual coding of
raw data may result in misclassification and be time consuming. The goal was to
develop and test a web-tool, named Procode, for coding of free-texts against
classifications and recoding between different classifications. Methods. Three
text classifiers, i.e. Complement Naive Bayes (CNB), Support Vector Machine
(SVM) and Random Forest Classifier (RFC), were investigated using a k-fold
cross-validation. 30 000 free-texts with manually assigned classification codes
of French classification of occupations (PCS) and French classification of
activities (NAF) were available. For recoding, Procode integrated a workflow
that converts codes of one classification to another according to existing
crosswalks. Since this is a straightforward operation, only the recoding time
was measured. Results. Among the three investigated text classifiers, CNB
resulted in the best performance, where the classifier predicted accurately
57-81% and 63-83% classification codes for PCS and NAF, respectively. SVM lead
to somewhat lower results (by 1-2%), while RFC coded accurately up to 30% of
the data. The coding operation required one minute per 10 000 records, while
the recoding was faster, i.e. 5-10 seconds. Conclusion. The algorithm
integrated in Procode showed satisfactory performance, since the tool had to
assign the right code by choosing between 500-700 different choices. Based on
the results, the authors decided to implement CNB in Procode. In future, if
another classifier shows a superior performance, an update will include the
required modifications.
","<1> Objective. </1>
 <2> Epidemiological studies require data that are in alignment with the classifications established for occupations or economic activities. </2>
 <3> The classifications usually include hundreds of codes and titles. </3>
 <4> Manual coding of raw data may result in misclassification and be time consuming. </4>
 <5> The goal was to develop and test a web-tool, named Procode, for coding of free-texts against classifications and recoding between different classifications. </5>
 <6> Methods. </6>
 <7> Three text classifiers, i.e. Complement Naive Bayes (CNB), Support Vector Machine (SVM) and Random Forest Classifier (RFC), were investigated using a k-fold cross-validation. </7>
 <8> 30 000 free-texts with manually assigned classification codes of French classification of occupations (PCS) and French classification of activities (NAF) were available. </8>
 <9> For recoding, Procode integrated a workflow that converts codes of one classification to another according to existing crosswalks. </9>
 <10> Since this is a straightforward operation, only the recoding time was measured. </10>
 <11> Results. </11>
 <12> Among the three investigated text classifiers, CNB resulted in the best performance, where the classifier predicted accurately 57-81% and 63-83% classification codes for PCS and NAF, respectively. </12>
 <13> SVM lead to somewhat lower results (by 1-2%), while RFC coded accurately up to 30% of the data. </13>
 <14> The coding operation required one minute per 10 000 records, while the recoding was faster, i.e. 5-10 seconds. </14>
 <15> Conclusion. </15>
 <16> The algorithm integrated in Procode showed satisfactory performance, since the tool had to assign the right code by choosing between 500-700 different choices. </16>
 <17> Based on the results, the authors decided to implement CNB in Procode. </17>
 <18> In future, if another classifier shows a superior performance, an update will include the required modifications. </18>
"
2012.14740,"LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document
  Understanding","  Pre-training of text and layout has proved effective in a variety of
visually-rich document understanding tasks due to its effective model
architecture and the advantage of large-scale unlabeled scanned/digital-born
documents. We propose LayoutLMv2 architecture with new pre-training tasks to
model the interaction among text, layout, and image in a single multi-modal
framework. Specifically, with a two-stream multi-modal Transformer encoder,
LayoutLMv2 uses not only the existing masked visual-language modeling task but
also the new text-image alignment and text-image matching tasks, which make it
better capture the cross-modality interaction in the pre-training stage.
Meanwhile, it also integrates a spatial-aware self-attention mechanism into the
Transformer architecture so that the model can fully understand the relative
positional relationship among different text blocks. Experiment results show
that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new
state-of-the-art results on a wide variety of downstream visually-rich document
understanding tasks, including FUNSD (0.7895 $\to$ 0.8420), CORD (0.9493 $\to$
0.9601), SROIE (0.9524 $\to$ 0.9781), Kleister-NDA (0.8340 $\to$ 0.8520),
RVL-CDIP (0.9443 $\to$ 0.9564), and DocVQA (0.7295 $\to$ 0.8672). We made our
model and code publicly available at \url{https://aka.ms/layoutlmv2}.
","<1> Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. </1>
 <2> We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. </2>
 <3> Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. </3>
 <4> Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. </4>
 <5> Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 $\to$ 0.8420), CORD (0.9493 $\to$ 0.9601), SROIE (0.9524 $\to$ 0.9781), Kleister-NDA (0.8340 $\to$ 0.8520), RVL-CDIP (0.9443 $\to$ 0.9564), and DocVQA (0.7295 $\to$ 0.8672). </5>
 <6> We made our model and code publicly available at \url{https://aka.ms/layoutlmv2}. </6>
 <7>  </7>
"
2112.11850,Multimodal Analysis of memes for sentiment extraction,"  Memes are one of the most ubiquitous forms of social media communication. The
study and processing of memes, which are intrinsically multimedia, is a popular
topic right now. The study presented in this research is based on the Memotion
dataset, which involves categorising memes based on irony, comedy, motivation,
and overall-sentiment. Three separate innovative transformer-based techniques
have been developed, and their outcomes have been thoroughly reviewed.The best
algorithm achieved a macro F1 score of 0.633 for humour classification, 0.55
for motivation classification, 0.61 for sarcasm classification, and 0.575 for
overall sentiment of the meme out of all our techniques.
","<1> Memes are one of the most ubiquitous forms of social media communication. </1>
 <2> The study and processing of memes, which are intrinsically multimedia, is a popular topic right now. </2>
 <3> The study presented in this research is based on the Memotion dataset, which involves categorising memes based on irony, comedy, motivation, and overall-sentiment. </3>
 <4> Three separate innovative transformer-based techniques have been developed, and their outcomes have been thoroughly reviewed. </4>
 <5> The best algorithm achieved a macro F1 score of 0.633 for humour classification, 0.55 for motivation classification, 0.61 for sarcasm classification, and 0.575 for overall sentiment of the meme out of all our techniques. </5>
"
2212.12442,Alignment Entropy Regularization,"  Existing training criteria in automatic speech recognition(ASR) permit the
model to freely explore more than one time alignments between the feature and
label sequences. In this paper, we use entropy to measure a model's
uncertainty, i.e. how it chooses to distribute the probability mass over the
set of allowed alignments. Furthermore, we evaluate the effect of entropy
regularization in encouraging the model to distribute the probability mass only
on a smaller subset of allowed alignments. Experiments show that entropy
regularization enables a much simpler decoding method without sacrificing word
error rate, and provides better time alignment quality.
","<1> Existing training criteria in automatic speech recognition(ASR) permit the model to freely explore more than one time alignments between the feature and label sequences. </1>
 <2> In this paper, we use entropy to measure a model's uncertainty, i.e. how it chooses to distribute the probability mass over the set of allowed alignments. </2>
 <3> Furthermore, we evaluate the effect of entropy regularization in encouraging the model to distribute the probability mass only on a smaller subset of allowed alignments. </3>
 <4> Experiments show that entropy </4>
 <5> regularization enables a much simpler decoding method without sacrificing word error rate, and provides better time alignment quality. </5>
"
2112.05459,Sentiment Analysis on Brazilian Portuguese User Reviews,"  Sentiment Analysis is one of the most classical and primarily studied natural
language processing tasks. This problem had a notable advance with the
proposition of more complex and scalable machine learning models. Despite this
progress, the Brazilian Portuguese language still disposes only of limited
linguistic resources, such as datasets dedicated to sentiment classification,
especially when considering the existence of predefined partitions in training,
testing, and validation sets that would allow a more fair comparison of
different algorithm alternatives. Motivated by these issues, this work analyzes
the predictive performance of a range of document embedding strategies,
assuming the polarity as the system outcome. This analysis includes five
sentiment analysis datasets in Brazilian Portuguese, unified in a single
dataset, and a reference partitioning in training, testing, and validation
sets, both made publicly available through a digital repository. A
cross-evaluation of dataset-specific models over different contexts is
conducted to evaluate their generalization capabilities and the feasibility of
adopting a unique model for addressing all scenarios.
","<1> Sentiment Analysis is one of the most classical and primarily studied natural language processing tasks. </1>
 <2> This problem had a notable advance with the proposition of more complex and scalable machine learning models. </2>
 <3> Despite this progress, the Brazilian Portuguese language still disposes only of limited linguistic resources, such as datasets dedicated to sentiment classification, especially when considering the existence of predefined partitions in training, testing, and validation sets that would allow a more fair comparison of different algorithm alternatives. </3>
 <4> Motivated by these issues, this work analyzes the predictive performance of a range of document embedding strategies, assuming the polarity as the system outcome. </4>
 <5> This analysis includes five sentiment analysis datasets in Brazilian Portuguese, unified in a single dataset, and a reference partitioning in training, testing, and validation sets, both made publicly available through a digital repository. </5>
 <6> A cross-evaluation of dataset-specific models over different contexts is conducted to evaluate their generalization capabilities and the feasibility of adopting a unique model for addressing all scenarios. </6>
"
2112.02095,"Intelligent Trading Systems: A Sentiment-Aware Reinforcement Learning
  Approach","  The feasibility of making profitable trades on a single asset on stock
exchanges based on patterns identification has long attracted researchers.
Reinforcement Learning (RL) and Natural Language Processing have gained
notoriety in these single-asset trading tasks, but only a few works have
explored their combination. Moreover, some issues are still not addressed, such
as extracting market sentiment momentum through the explicit capture of
sentiment features that reflect the market condition over time and assessing
the consistency and stability of RL results in different situations. Filling
this gap, we propose the Sentiment-Aware RL (SentARL) intelligent trading
system that improves profit stability by leveraging market mood through an
adaptive amount of past sentiment features drawn from textual news. We
evaluated SentARL across twenty assets, two transaction costs, and five
different periods and initializations to show its consistent effectiveness
against baselines. Subsequently, this thorough assessment allowed us to
identify the boundary between news coverage and market sentiment regarding the
correlation of price-time series above which SentARL's effectiveness is
outstanding.
","<1>  </1>
 <2> The feasibility of making profitable trades on a single asset on stock exchanges based on patterns identification has long attracted researchers. </2>
 <3>  </3>
 <4> Reinforcement Learning (RL) and Natural Language Processing have gained notoriety in these single-asset trading tasks, but only a few works have explored their combination. </4>
 <5> Moreover, some issues are still not addressed, such as extracting market sentiment momentum through the explicit capture of sentiment features that reflect the market condition over time and assessing the consistency and stability of RL results in different situations. </5>
 <6> Filling this gap, we propose the Sentiment-Aware RL (SentARL) intelligent trading system that improves profit stability by leveraging market mood through an adaptive amount of past sentiment features drawn from textual news. </6>
 <7> We evaluated SentARL across twenty assets, two transaction costs, and five different periods and initializations to show its consistent effectiveness against baselines. </7>
 <8> Subsequently, this thorough assessment allowed us to identify the boundary between news coverage and market sentiment regarding the correlation of price-time series above which SentARL's effectiveness is outstanding. </8>
"
2012.15484,"Seeing is Knowing! Fact-based Visual Question Answering using Knowledge
  Graph Embeddings","  Fact-based Visual Question Answering (FVQA), a challenging variant of VQA,
requires a QA-system to include facts from a diverse knowledge graph (KG) in
its reasoning process to produce an answer. Large KGs, especially common-sense
KGs, are known to be incomplete, i.e., not all non-existent facts are always
incorrect. Therefore, being able to reason over incomplete KGs for QA is a
critical requirement in real-world applications that has not been addressed
extensively in the literature. We develop a novel QA architecture that allows
us to reason over incomplete KGs, something current FVQA state-of-the-art
(SOTA) approaches lack due to their critical reliance on fact retrieval. We use
KG Embeddings, a technique widely used for KG completion, for the downstream
task of FVQA. We also employ a new image representation technique we call
'Image-as-Knowledge' to enable this capability, alongside a simple one-step
CoAttention mechanism to attend to text and image during QA. Our FVQA
architecture is faster during inference time, being O(m), as opposed to
existing FVQA SOTA methods which are O(N log N), where m = number of vertices,
N = number of edges = O(m^2). KG embeddings are shown to hold complementary
information to word embeddings: a combination of both metrics permits
performance comparable to SOTA methods in the standard answer retrieval task,
and significantly better (26% absolute) in the proposed missing-edge reasoning
task.
","<1> Fact-based Visual Question Answering (FVQA), a challenging variant of VQA, requires a QA-system to include facts from a diverse knowledge graph (KG) in its reasoning process to produce an answer. </1>
 <2> Large KGs, especially common-sense KGs, are known to be incomplete, i.e., not all non-existent facts are always incorrect. </2>
 <3> Therefore, being able to reason over incomplete KGs for QA is a critical requirement in real-world applications that has not been addressed extensively in the literature. </3>
 <4> We develop a novel QA architecture that allows us to reason over incomplete KGs, something current FVQA state-of-the-art (SOTA) approaches lack due to their critical reliance on fact retrieval. </4>
 <5> We use KG Embeddings, a technique widely used for KG completion, for the downstream task of FVQA. </5>
 <6> We also employ a new image representation technique we call 'Image-as-Knowledge' to enable this capability, alongside a simple one-step CoAttention mechanism to attend to text and image during QA. </6>
 <7> Our FVQA architecture is faster during inference time, being O(m), as opposed to existing FVQA SOTA methods which are O(N log N), where m = number of vertices, N = number of edges = O(m^2). </7>
 <8> KG embeddings are shown to hold complementary information to word embeddings: a combination of both metrics permits performance comparable to SOTA methods in the standard answer retrieval task, and significantly better (26% absolute) in the proposed missing-edge reasoning task. </8>
"
2212.14149,"Macro-block dropout for improved regularization in training end-to-end
  speech recognition models","  This paper proposes a new regularization algorithm referred to as macro-block
dropout. The overfitting issue has been a difficult problem in training large
neural network models. The dropout technique has proven to be simple yet very
effective for regularization by preventing complex co-adaptations during
training. In our work, we define a macro-block that contains a large number of
units from the input to a Recurrent Neural Network (RNN). Rather than applying
dropout to each unit, we apply random dropout to each macro-block. This
algorithm has the effect of applying different drop out rates for each layer
even if we keep a constant average dropout rate, which has better
regularization effects. In our experiments using Recurrent Neural
Network-Transducer (RNN-T), this algorithm shows relatively 4.30 % and 6.13 %
Word Error Rates (WERs) improvement over the conventional dropout on
LibriSpeech test-clean and test-other. With an Attention-based Encoder-Decoder
(AED) model, this algorithm shows relatively 4.36 % and 5.85 % WERs improvement
over the conventional dropout on the same test sets.
","<1> This paper proposes a new regularization algorithm referred to as macro-block dropout. </1>
 <2> The overfitting issue has been a difficult problem in training large neural network models. </2>
 <3> The dropout technique has proven to be simple yet very effective for regularization by preventing complex co-adaptations during training. </3>
 <4> In our work, we define a macro-block that contains a large number of units from the input to a Recurrent Neural Network (RNN). </4>
 <5> Rather than applying dropout to each unit, we apply random dropout to each macro-block. </5>
 <6> This algorithm has the effect of applying different drop out rates for each layer even if we keep a constant average dropout rate, which has better regularization effects. </6>
 <7> In our experiments using Recurrent Neural Network-Transducer (RNN-T), this algorithm shows relatively 4.30 % and 6.13 % Word Error Rates (WERs) improvement over the conventional dropout on LibriSpeech test-clean and test-other. </7>
 <8> With an Attention-based Encoder-Decoder (AED) model, this algorithm shows relatively 4.36 % and 5.85 % WERs improvement over the conventional dropout on the same test sets. </8>
"
2012.11142,"Towards Incorporating Entity-specific Knowledge Graph Information in
  Predicting Drug-Drug Interactions","  Off-the-shelf biomedical embeddings obtained from the recently released
various pre-trained language models (such as BERT, XLNET) have demonstrated
state-of-the-art results (in terms of accuracy) for the various natural
language understanding tasks (NLU) in the biomedical domain. Relation
Classification (RC) falls into one of the most critical tasks. In this paper,
we explore how to incorporate domain knowledge of the biomedical entities (such
as drug, disease, genes), obtained from Knowledge Graph (KG) Embeddings, for
predicting Drug-Drug Interaction from textual corpus. We propose a new method,
BERTKG-DDI, to combine drug embeddings obtained from its interaction with other
biomedical entities along with domain-specific BioBERT embedding-based RC
architecture. Experiments conducted on the DDIExtraction 2013 corpus clearly
indicate that this strategy improves other baselines architectures by 4.1%
macro F1-score.
","<1> Off-the-shelf biomedical embeddings obtained from the recently released various pre-trained language models (such as BERT, XLNET) have demonstrated state-of-the-art results (in terms of accuracy) for the various natural language understanding tasks (NLU) in the biomedical domain. </1>
 <2> Relation Classification (RC) falls into one of the most critical tasks. </2>
 <3> In this paper, we explore how to incorporate domain knowledge of the biomedical entities (such as drug, disease, genes), obtained from Knowledge Graph (KG) Embeddings, for predicting Drug-Drug Interaction from textual corpus. </3>
 <4> We propose a new method, BERTKG-DDI, to combine drug embeddings obtained from its interaction with other biomedical entities along with domain-specific BioBERT embedding-based RC architecture. </4>
 <5> Experiments conducted on the DDIExtraction 2013 corpus clearly indicate that this strategy improves other baselines architectures by 4.1% macro F1-score. </5>
"
1712.06880,Analogy Mining for Specific Design Needs,"  Finding analogical inspirations in distant domains is a powerful way of
solving problems. However, as the number of inspirations that could be matched
and the dimensions on which that matching could occur grow, it becomes
challenging for designers to find inspirations relevant to their needs.
Furthermore, designers are often interested in exploring specific aspects of a
product-- for example, one designer might be interested in improving the
brewing capability of an outdoor coffee maker, while another might wish to
optimize for portability. In this paper we introduce a novel system for
targeting analogical search for specific needs. Specifically, we contribute a
novel analogical search engine for expressing and abstracting specific design
needs that returns more distant yet relevant inspirations than alternate
approaches.
","<1>  </1>
 <2> Finding analogical inspirations in distant domains is a powerful way of solving problems. </2>
 <3> However, as the number of inspirations that could be matched and the dimensions on which that matching could occur grow, it becomes challenging for designers to find inspirations relevant to their needs. </3>
 <4> Furthermore, designers are often interested in exploring specific aspects of a product-- for example, one designer might be interested in improving the brewing capability of an outdoor coffee maker, while another might wish to optimize for portability. </4>
 <5> In this paper we introduce a novel system for targeting analogical search for specific needs. </5>
 <6> Specifically, we contribute a novel analogical search engine for expressing and abstracting specific design needs that returns more distant yet relevant inspirations than alternate approaches. </6>
"
1812.04722,Context is Key: New Approaches to Neural Coherence Modeling,"  We formulate coherence modeling as a regression task and propose two novel
methods to combine techniques from our setup with pairwise approaches. The
first of our methods is a model that we call ""first-next,"" which operates
similarly to selection sorting but conditions decision-making on information
about already-sorted sentences. The second consists of a technique for adding
context to regression-based models by concatenating sentence-level
representations with an encoding of its corresponding out-of-order paragraph.
This latter model achieves Kendall-tau distance and positional accuracy scores
that match or exceed the current state-of-the-art on these metrics. Our results
suggest that many of the gains that come from more complex, machine-translation
inspired approaches can be achieved with simpler, more efficient models.
","<1> We formulate coherence modeling as a regression task and propose two novel methods to combine techniques from our setup with pairwise approaches. </1>
 <2> The first of our methods is a model that we call ""first-next,"" which operates similarly to selection sorting but conditions decision-making on information about already-sorted sentences. </2>
 <3> The second consists of a technique for adding context to regression-based models by concatenating sentence-level representations with an encoding of its corresponding out-of-order paragraph. </3>
 <4>  </4>
 <5> This latter model achieves Kendall-tau distance and positional accuracy scores that match or exceed the current state-of-the-art on these metrics. </5>
 <6> Our results suggest that many of the gains that come from more complex, machine-translation inspired approaches can be achieved with simpler, more efficient models. </6>
"
2012.09118,Exploring Thematic Coherence in Fake News,"  The spread of fake news remains a serious global issue; understanding and
curtailing it is paramount. One way of differentiating between deceptive and
truthful stories is by analyzing their coherence. This study explores the use
of topic models to analyze the coherence of cross-domain news shared online.
Experimental results on seven cross-domain datasets demonstrate that fake news
shows a greater thematic deviation between its opening sentences and its
remainder.
","<1> The spread of fake news remains a serious global issue; understanding and curtailing it is paramount. </1>
 <2> One way of differentiating between deceptive and truthful stories is by analyzing their coherence. </2>
 <3> This study explores the use of topic models to analyze the coherence of cross-domain news shared online. </3>
 <4>  </4>
 <5> Experimental results on seven cross-domain datasets demonstrate that fake news shows a greater thematic deviation between its opening sentences and its remainder. </5>
"
2012.06690,Yelp Review Rating Prediction: Machine Learning and Deep Learning Models,"  We predict restaurant ratings from Yelp reviews based on Yelp Open Dataset.
Data distribution is presented, and one balanced training dataset is built. Two
vectorizers are experimented for feature engineering. Four machine learning
models including Naive Bayes, Logistic Regression, Random Forest, and Linear
Support Vector Machine are implemented. Four transformer-based models
containing BERT, DistilBERT, RoBERTa, and XLNet are also applied. Accuracy,
weighted F1 score, and confusion matrix are used for model evaluation. XLNet
achieves 70% accuracy for 5-star classification compared with Logistic
Regression with 64% accuracy.
","<1>  </1>
 <2> We predict restaurant ratings from Yelp reviews based on Yelp Open Dataset. </2>
 <3>  </3>
 <4> Data distribution is presented, and one balanced training dataset is built. </4>
 <5> Two vectorizers are experimented for feature engineering. </5>
 <6> Four machine learning models including Naive Bayes, Logistic Regression, Random Forest, and Linear Support Vector Machine are implemented. </6>
 <7> Four transformer-based models containing BERT, DistilBERT, RoBERTa, and XLNet are also applied. </7>
 <8> Accuracy, weighted F1 score, and confusion matrix are used for model evaluation. </8>
 <9> XLNet achieves 70% accuracy for 5-star classification compared with Logistic Regression with 64% accuracy. </9>
"
2112.11739,A Survey of Natural Language Generation,"  This paper offers a comprehensive review of the research on Natural Language
Generation (NLG) over the past two decades, especially in relation to
data-to-text generation and text-to-text generation deep learning methods, as
well as new applications of NLG technology. This survey aims to (a) give the
latest synthesis of deep learning research on the NLG core tasks, as well as
the architectures adopted in the field; (b) detail meticulously and
comprehensively various NLG tasks and datasets, and draw attention to the
challenges in NLG evaluation, focusing on different evaluation methods and
their relationships; (c) highlight some future emphasis and relatively recent
research issues that arise due to the increasing synergy between NLG and other
artificial intelligence areas, such as computer vision, text and computational
creativity.
","<1> This paper offers a comprehensive review of the research on Natural Language Generation (NLG) over the past two decades, especially in relation to data-to-text generation and text-to-text generation deep learning methods, as well as new applications of NLG technology. </1>
 <2> This survey aims to (a) give the latest synthesis of deep learning research on the NLG core tasks, as well as the architectures adopted in the field; (b) detail meticulously and comprehensively various NLG tasks and datasets, and draw attention to the challenges in NLG evaluation, focusing on different evaluation methods and their relationships; (c) highlight some future emphasis and relatively recent research issues that arise due to the increasing synergy between NLG and other artificial intelligence areas, such as computer vision, text and computational creativity. </2>
"
1912.02395,Learning to Predict Explainable Plots for Neural Story Generation,"  Story generation is an important natural language processing task that aims
to generate coherent stories automatically. While the use of neural networks
has proven effective in improving story generation, how to learn to generate an
explainable high-level plot still remains a major challenge. In this work, we
propose a latent variable model for neural story generation. The model treats
an outline, which is a natural language sentence explainable to humans, as a
latent variable to represent a high-level plot that bridges the input and
output. We adopt an external summarization model to guide the latent variable
model to learn how to generate outlines from training data. Experiments show
that our approach achieves significant improvements over state-of-the-art
methods in both automatic and human evaluations.
","<1> Story generation is an important natural language processing task that aims to generate coherent stories automatically. </1>
 <2> While the use of neural networks has proven effective in improving story generation, how to learn to generate an explainable high-level plot still remains a major challenge. </2>
 <3> In this work, we propose a latent variable model for neural story generation. </3>
 <4> The model treats an outline, which is a natural language sentence explainable to humans, as a latent variable to represent a high-level plot that bridges the input and output. </4>
 <5> We adopt an external summarization model to guide the latent variable model to learn how to generate outlines from training data. </5>
 <6> Experiments show that our approach achieves significant improvements over state-of-the-art methods in both automatic and human evaluations. </6>
"
2012.11587,Object-Centric Diagnosis of Visual Reasoning,"  When answering questions about an image, it not only needs knowing what --
understanding the fine-grained contents (e.g., objects, relationships) in the
image, but also telling why -- reasoning over grounding visual cues to derive
the answer for a question. Over the last few years, we have seen significant
progress on visual question answering. Though impressive as the accuracy grows,
it still lags behind to get knowing whether these models are undertaking
grounding visual reasoning or just leveraging spurious correlations in the
training data. Recently, a number of works have attempted to answer this
question from perspectives such as grounding and robustness. However, most of
them are either focusing on the language side or coarsely studying the
pixel-level attention maps. In this paper, by leveraging the step-wise object
grounding annotations provided in the GQA dataset, we first present a
systematical object-centric diagnosis of visual reasoning on grounding and
robustness, particularly on the vision side. According to the extensive
comparisons across different models, we find that even models with high
accuracy are not good at grounding objects precisely, nor robust to visual
content perturbations. In contrast, symbolic and modular models have a
relatively better grounding and robustness, though at the cost of accuracy. To
reconcile these different aspects, we further develop a diagnostic model,
namely Graph Reasoning Machine. Our model replaces purely symbolic visual
representation with probabilistic scene graph and then applies teacher-forcing
training for the visual reasoning module. The designed model improves the
performance on all three metrics over the vanilla neural-symbolic model while
inheriting the transparency. Further ablation studies suggest that this
improvement is mainly due to more accurate image understanding and proper
intermediate reasoning supervisions.
","<1> When answering questions about an image, it not only needs knowing what -- understanding the fine-grained contents (e.g., objects, relationships) in the image, but also telling why -- reasoning over grounding visual cues to derive the answer for a question. </1>
 <2> Over the last few years, we have seen significant progress on visual question answering. </2>
 <3> Though impressive as the accuracy grows, it still lags behind to get knowing whether these models are undertaking grounding visual reasoning or just leveraging spurious correlations in the training data. </3>
 <4> Recently, a number of works have attempted to answer this question from perspectives such as grounding and robustness. </4>
 <5> However, most of them are either focusing on the language side or coarsely studying the pixel-level attention maps. </5>
 <6> In this paper, by leveraging the step-wise object grounding annotations provided in the GQA dataset, we first present a systematical object-centric diagnosis of visual reasoning on grounding and robustness, particularly on the vision side. </6>
 <7> According to the extensive comparisons across different models, we find that even models with high accuracy are not good at grounding objects precisely, nor robust to visual content perturbations. </7>
 <8> In contrast, symbolic and modular models have a relatively better grounding and robustness, though at the cost of accuracy. </8>
 <9> To reconcile these different aspects, we further develop a diagnostic model, namely Graph Reasoning Machine. </9>
 <10> Our model replaces purely symbolic visual representation with probabilistic scene graph and then applies teacher-forcing training for the visual reasoning module. </10>
 <11> The designed model improves the performance on all three metrics over the vanilla neural-symbolic model while inheriting the transparency. </11>
 <12> Further ablation studies suggest that this improvement is mainly due to more accurate image understanding and proper intermediate reasoning supervisions. </12>
"
2212.06369,"Technical Report -- Competition Solution for Prompt Tuning using
  Pretrained Language Model","  Prompt tuning recently becomes a hot-spot in the applications of large
pretrained language models on specific downstream tasks. Regarding the Language
Model as a Service (LMaaS), black-box tuning using derivative-free optimization
(DFO) provides a novel approach to expand the practical scenarios of pretrained
models and enrich the researches of few-shot learning. In this report, we
present our solution in this competition that is based on the LMaaS scenario.
Our solution consists of several modifications to BBTv2, including multiple
label words, selection of P0, rolling update strategy, multi-task loss from MLP
classifier, and finally using the ensemble method to further improve
generalization ability. We also shared some strategies that we tried but didn't
use in the final submission for further discussion. In the end we raised a
question about the SNLI dataset and the impact on the results, as well as our
concerns about the competition.
","<1> Prompt tuning recently becomes a hot-spot in the applications of large pretrained language models on specific downstream tasks. </1>
 <2> Regarding the Language Model as a Service (LMaaS), black-box tuning using derivative-free optimization </2>
 <3> (DFO) provides a novel approach to expand the practical scenarios of pretrained models and enrich the researches of few-shot learning. </3>
 <4> In this report, we present our solution in this competition that is based on the LMaaS scenario. </4>
 <5>  </5>
 <6> Our solution consists of several modifications to BBTv2, including multiple label words, selection of P0, rolling update strategy, multi-task loss from MLP classifier, and finally using the ensemble method to further improve generalization ability. </6>
 <7> We also shared some strategies that we tried but didn't use in the final submission for further discussion. </7>
 <8> In the end we raised a question about the SNLI dataset and the impact on the results, as well as our concerns about the competition. </8>
"
2112.03256,Impact of Target Word and Context on End-to-End Metonymy Detection,"  Metonymy is a figure of speech in which an entity is referred to by another
related entity. The task of metonymy detection aims to distinguish metonymic
tokens from literal ones. Until now, metonymy detection methods attempt to
disambiguate only a single noun phrase in a sentence, typically location names
or organization names. In this paper, we disambiguate every word in a sentence
by reformulating metonymy detection as a sequence labeling task. We also
investigate the impact of target word and context on metonymy detection. We
show that the target word is less useful for detecting metonymy in our dataset.
On the other hand, the entity types that are associated with domain-specific
words in their context are easier to solve. This shows that the context words
are much more relevant for detecting metonymy.
","<1> Metonymy is a figure of speech in which an entity is referred to by another related entity. </1>
 <2> The task of metonymy detection aims to distinguish metonymic tokens from literal ones. </2>
 <3> Until now, metonymy detection methods attempt to disambiguate only a single noun phrase in a sentence, typically location names or organization names. </3>
 <4> In this paper, we disambiguate every word in a sentence by reformulating metonymy detection as a sequence labeling task. </4>
 <5> We also investigate the impact of target word and context on metonymy detection. </5>
 <6> We show that the target word is less useful for detecting metonymy in our dataset. </6>
 <7>  </7>
 <8> On the other hand, the entity types that are associated with domain-specific words in their context are easier to solve. </8>
 <9> This shows that the context words are much more relevant for detecting metonymy. </9>
"
2112.03529,"Ground-Truth, Whose Truth? -- Examining the Challenges with Annotating
  Toxic Text Datasets","  The use of machine learning (ML)-based language models (LMs) to monitor
content online is on the rise. For toxic text identification, task-specific
fine-tuning of these models are performed using datasets labeled by annotators
who provide ground-truth labels in an effort to distinguish between offensive
and normal content. These projects have led to the development, improvement,
and expansion of large datasets over time, and have contributed immensely to
research on natural language. Despite the achievements, existing evidence
suggests that ML models built on these datasets do not always result in
desirable outcomes. Therefore, using a design science research (DSR) approach,
this study examines selected toxic text datasets with the goal of shedding
light on some of the inherent issues and contributing to discussions on
navigating these challenges for existing and future projects. To achieve the
goal of the study, we re-annotate samples from three toxic text datasets and
find that a multi-label approach to annotating toxic text samples can help to
improve dataset quality. While this approach may not improve the traditional
metric of inter-annotator agreement, it may better capture dependence on
context and diversity in annotators. We discuss the implications of these
results for both theory and practice.
","<1>  </1>
 <2> The use of machine learning (ML)-based language models (LMs) to monitor content online is on the rise. </2>
 <3> For toxic text identification, task-specific fine-tuning of these models are performed using datasets labeled by annotators who provide ground-truth labels in an effort to distinguish between offensive and normal content. </3>
 <4> These projects have led to the development, improvement, and expansion of large datasets over time, and have contributed immensely to research on natural language. </4>
 <5> Despite the achievements, existing evidence suggests that ML models built on these datasets do not always result in desirable outcomes. </5>
 <6> Therefore, using a design science research (DSR) approach, this study examines selected toxic text datasets with the goal of shedding light on some of the inherent issues and contributing to discussions on navigating these challenges for existing and future projects. </6>
 <7> To achieve the goal of the study, we re-annotate samples from three toxic text datasets and find that a multi-label approach to annotating toxic text samples can help to improve dataset quality. </7>
 <8> While this approach may not improve the traditional metric of inter-annotator agreement, it may better capture dependence on context and diversity in annotators. </8>
 <9> We discuss the implications of these results for both theory and practice. </9>
"
1812.00049,"The Indus Script and Economics. A Role for Indus Seals and Tablets in
  Rationing and Administration of Labor","  The Indus script remains one of the last major undeciphered scripts of the
ancient world. We focus here on Indus inscriptions on a group of miniature
tablets discovered by Meadow and Kenoyer in Harappa in 1997. By drawing
parallels with proto-Elamite and proto-Cuneiform inscriptions, we explore how
these miniature tablets may have been used to record rations allocated to
porters or laborers. We then show that similar inscriptions are found on stamp
seals, leading to the potentially provocative conclusion that rather than
simply indicating ownership of property, Indus seals may have been used for
generating tokens, tablets and sealings for repetitive economic transactions
such as rations and exchange of canonical amounts of goods, grains, animals,
and labor in a barter-based economy.
","<1> The Indus script remains one of the last major undeciphered scripts of the ancient world. </1>
 <2> We focus here on Indus inscriptions on a group of miniature tablets discovered by Meadow and Kenoyer in Harappa in 1997. </2>
 <3> By drawing parallels with proto-Elamite and proto-Cuneiform inscriptions, we explore how these miniature tablets may have been used to record rations allocated to porters or laborers. </3>
 <4> We then show that similar inscriptions are found on stamp seals, leading to the potentially provocative conclusion that rather than simply indicating ownership of property, Indus seals may have been used for generating tokens, tablets and sealings for repetitive economic transactions such as rations and exchange of canonical amounts of goods, grains, animals, and labor in a barter-based economy. </4>
"
2112.09841,Assessing Post-editing Effort in the English-Hindi Direction,"  We present findings from a first in-depth post-editing effort estimation
study in the English-Hindi direction along multiple effort indicators. We
conduct a controlled experiment involving professional translators, who
complete assigned tasks alternately, in a translation from scratch and a
post-edit condition. We find that post-editing reduces translation time (by
63%), utilizes fewer keystrokes (by 59%), and decreases the number of pauses
(by 63%) when compared to translating from scratch. We further verify the
quality of translations thus produced via a human evaluation task in which we
do not detect any discernible quality differences.
","<1> We present findings from a first in-depth post-editing effort estimation study in the English-Hindi direction along multiple effort indicators. </1>
 <2> We conduct a controlled experiment involving professional translators, who complete assigned tasks alternately, in a translation from scratch and a post-edit condition. </2>
 <3> We find that post-editing reduces translation time (by 63%), utilizes fewer keystrokes (by 59%), and decreases the number of pauses (by 63%) when compared to translating from scratch. </3>
 <4> We further verify the quality of translations thus produced via a human evaluation task in which we do not detect any discernible quality differences. </4>
"
1912.00698,"Fiction Sentence Expansion and Enhancement via Focused Objective and
  Novelty Curve Sampling","  We describe the task of sentence expansion and enhancement, in which a
sentence provided by a human is expanded in some creative way. The expansion
should be understandable, believably grammatical, and optimally
meaning-preserving. Sentence expansion and enhancement may serve as an
authoring tool, or integrate in dynamic media, conversational agents, or
variegated advertising.
  We implement a neural sentence expander trained on sentence compressions
generated from a corpus of modern fiction. We modify an MLE objective to
support the task by focusing on new words, and decode at test time with
controlled curve-like novelty sampling. We run our sentence expander on
sentences provided by human subjects and have humans evaluate these expansions.
We show that, although the generation methods are inferior to professional
human writers, they are comparable to, and as well liked as, our subjects'
original input sentences, and preferred over baselines.
","<1> We describe the task of sentence expansion and enhancement, in which a sentence provided by a human is expanded in some creative way. </1>
 <2> The expansion should be understandable, believably grammatical, and optimally meaning-preserving. </2>
 <3> Sentence expansion and enhancement may serve as an authoring tool, or integrate in dynamic media, conversational agents, or variegated advertising. </3>
 <4> We implement a neural sentence expander trained on sentence compressions generated from a corpus of modern fiction. </4>
 <5> We modify an MLE objective to support the task by focusing on new words, and decode at test time with controlled curve-like novelty sampling. </5>
 <6> We run our sentence expander on sentences provided by human subjects and have humans evaluate these expansions. </6>
 <7>  </7>
 <8> We show that, although the generation methods are inferior to professional human writers, they are comparable to, and as well liked as, our subjects' original input sentences, and preferred over baselines. </8>
"
1812.08658,nocaps: novel object captioning at scale,"  Image captioning models have achieved impressive results on datasets
containing limited visual concepts and large amounts of paired image-caption
training data. However, if these models are to ever function in the wild, a
much larger variety of visual concepts must be learned, ideally from less
supervision. To encourage the development of image captioning models that can
learn visual concepts from alternative data sources, such as object detection
datasets, we present the first large-scale benchmark for this task. Dubbed
'nocaps', for novel object captioning at scale, our benchmark consists of
166,100 human-generated captions describing 15,100 images from the OpenImages
validation and test sets. The associated training data consists of COCO
image-caption pairs, plus OpenImages image-level labels and object bounding
boxes. Since OpenImages contains many more classes than COCO, nearly 400 object
classes seen in test images have no or very few associated training captions
(hence, nocaps). We extend existing novel object captioning models to establish
strong baselines for this benchmark and provide analysis to guide future work
on this task.
","<1> Image captioning models have achieved impressive results on datasets containing limited visual concepts and large amounts of paired image-caption training data. </1>
 <2> However, if these models are to ever function in the wild, a much larger variety of visual concepts must be learned, ideally from less supervision. </2>
 <3> To encourage the development of image captioning models that can learn visual concepts from alternative data sources, such as object detection datasets, we present the first large-scale benchmark for this task. </3>
 <4> Dubbed 'nocaps', for novel object captioning at scale, our benchmark consists of 166,100 human-generated captions describing 15,100 images from the OpenImages validation and test sets. </4>
 <5> The associated training data consists of COCO image-caption pairs, plus OpenImages image-level labels and object bounding boxes. </5>
 <6> Since OpenImages contains many more classes than COCO, nearly 400 object classes seen in test images have no or very few associated training captions (hence, nocaps). </6>
 <7> We extend existing novel object captioning models to establish strong baselines for this benchmark and provide analysis to guide future work on this task. </7>
"
2212.08204,"LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text
  Comprehension","  The application of Natural Language Processing (NLP) to specialized domains,
such as the law, has recently received a surge of interest. As many legal
services rely on processing and analyzing large collections of documents,
automating such tasks with NLP tools emerges as a key challenge. Many popular
language models, such as BERT or RoBERTa, are general-purpose models, which
have limitations on processing specialized legal terminology and syntax. In
addition, legal documents may contain specialized vocabulary from other
domains, such as medical terminology in personal injury text. Here, we propose
LegalRelectra, a legal-domain language model that is trained on mixed-domain
legal and medical corpora. We show that our model improves over general-domain
and single-domain medical and legal language models when processing
mixed-domain (personal injury) text. Our training architecture implements the
Electra framework, but utilizes Reformer instead of BERT for its generator and
discriminator. We show that this improves the model's performance on processing
long passages and results in better long-range text comprehension.
","<1>  </1>
 <2> The application of Natural Language Processing (NLP) to specialized domains, such as the law, has recently received a surge of interest. </2>
 <3> As many legal services rely on processing and analyzing large collections of documents, automating such tasks with NLP tools emerges as a key challenge. </3>
 <4> Many popular language models, such as BERT or RoBERTa, are general-purpose models, which have limitations on processing specialized legal terminology and syntax. </4>
 <5> In addition, legal documents may contain specialized vocabulary from other domains, such as medical terminology in personal injury text. </5>
 <6> Here, we propose LegalRelectra, a legal-domain language model that is trained on mixed-domain legal and medical corpora. </6>
 <7> We show that our model improves over general-domain and single-domain medical and legal language models when processing mixed-domain (personal injury) text. </7>
 <8> Our training architecture implements the Electra framework, but utilizes Reformer instead of BERT for its generator and discriminator. </8>
 <9> We show that this improves the model's performance on processing long passages and results in better long-range text comprehension. </9>
"
2212.04765,"Understanding Online Migration Decisions Following the Banning of
  Radical Communities","  The proliferation of radical online communities and their violent offshoots
has sparked great societal concern. However, the current practice of banning
such communities from mainstream platforms has unintended consequences: (I) the
further radicalization of their members in fringe platforms where they migrate;
and (ii) the spillover of harmful content from fringe back onto mainstream
platforms. Here, in a large observational study on two banned subreddits,
r/The\_Donald and r/fatpeoplehate, we examine how factors associated with the
RECRO radicalization framework relate to users' migration decisions.
Specifically, we quantify how these factors affect users' decisions to post on
fringe platforms and, for those who do, whether they continue posting on the
mainstream platform. Our results show that individual-level factors, those
relating to the behavior of users, are associated with the decision to post on
the fringe platform. Whereas social-level factors, users' connection with the
radical community, only affect the propensity to be coactive on both platforms.
Overall, our findings pave the way for evidence-based moderation policies, as
the decisions to migrate and remain coactive amplify unintended consequences of
community bans.
","<1>  </1>
 <2> The proliferation of radical online communities and their violent offshoots has sparked great societal concern. </2>
 <3> However, the current practice of banning such communities from mainstream platforms has unintended consequences: (I) the further radicalization of their members in fringe platforms where they migrate; and (ii) the spillover of harmful content from fringe back onto mainstream platforms. </3>
 <4> Here, in a large observational study on two banned subreddits, r/The\_Donald and r/fatpeoplehate, we examine how factors associated with the RECRO radicalization framework relate to users' migration decisions. </4>
 <5>  </5>
 <6> Specifically, we quantify how these factors affect users' decisions to post on fringe platforms and, for those who do, whether they continue posting on the mainstream platform. </6>
 <7> Our results show that individual-level factors, those relating to the behavior of users, are associated with the decision to post on the fringe platform. </7>
 <8> Whereas social-level factors, users' connection with the radical community, only affect the propensity to be coactive on both platforms. </8>
 <9>  </9>
 <10> Overall, our findings pave the way for evidence-based moderation policies, as the decisions to migrate and remain coactive amplify unintended consequences of community bans. </10>
"
2112.05785,TempoQR: Temporal Question Reasoning over Knowledge Graphs,"  Knowledge Graph Question Answering (KGQA) involves retrieving facts from a
Knowledge Graph (KG) using natural language queries. A KG is a curated set of
facts consisting of entities linked by relations. Certain facts include also
temporal information forming a Temporal KG (TKG). Although many natural
questions involve explicit or implicit time constraints, question answering
(QA) over TKGs has been a relatively unexplored area. Existing solutions are
mainly designed for simple temporal questions that can be answered directly by
a single TKG fact. This paper puts forth a comprehensive embedding-based
framework for answering complex questions over TKGs. Our method termed temporal
question reasoning (TempoQR) exploits TKG embeddings to ground the question to
the specific entities and time scope it refers to. It does so by augmenting the
question embeddings with context, entity and time-aware information by
employing three specialized modules. The first computes a textual
representation of a given question, the second combines it with the entity
embeddings for entities involved in the question, and the third generates
question-specific time embeddings. Finally, a transformer-based encoder learns
to fuse the generated temporal information with the question representation,
which is used for answer predictions. Extensive experiments show that TempoQR
improves accuracy by 25--45 percentage points on complex temporal questions
over state-of-the-art approaches and it generalizes better to unseen question
types.
","<1> Knowledge Graph Question Answering (KGQA) involves retrieving facts from a Knowledge Graph (KG) using natural language queries. </1>
 <2> A KG is a curated set of facts consisting of entities linked by relations. </2>
 <3> Certain facts include also temporal information forming a Temporal KG (TKG). </3>
 <4> Although many natural questions involve explicit or implicit time constraints, question answering (QA) over TKGs has been a relatively unexplored area. </4>
 <5> Existing solutions are mainly designed for simple temporal questions that can be answered directly by a single TKG fact. </5>
 <6> This paper puts forth a comprehensive embedding-based framework for answering complex questions over TKGs. </6>
 <7> Our method termed temporal question reasoning (TempoQR) exploits TKG embeddings to ground the question to the specific entities and time scope it refers to. </7>
 <8> It does so by augmenting the question embeddings with context, entity and time-aware information by employing three specialized modules. </8>
 <9> The first computes a textual representation of a given question, the second combines it with the entity embeddings for entities involved in the question, and the third generates question-specific time embeddings. </9>
 <10> Finally, a transformer-based encoder learns to fuse the generated temporal information with the question representation, which is used for answer predictions. </10>
 <11> Extensive experiments show that TempoQR improves accuracy by 25--45 percentage points on complex temporal questions over state-of-the-art approaches and it generalizes better to unseen question types. </11>
"
2212.09867,"Detecting Contradictory COVID-19 Drug Efficacy Claims from Biomedical
  Literature","  The COVID-19 pandemic created a deluge of questionable and contradictory
scientific claims about drug efficacy -- an ""infodemic"" with lasting
consequences for science and society. In this work, we argue that NLP models
can help domain experts distill and understand the literature in this complex,
high-stakes area. Our task is to automatically identify contradictory claims
about COVID-19 drug efficacy. We frame this as a natural language inference
problem and offer a new NLI dataset created by domain experts. The NLI framing
allows us to create curricula combining existing datasets and our own. The
resulting models are useful investigative tools. We provide a case study of how
these models help a domain expert summarize and assess evidence concerning
remdisivir and hydroxychloroquine.
","<1>  </1>
 <2> The COVID-19 pandemic created a deluge of questionable and contradictory scientific claims about drug efficacy -- an ""infodemic"" with lasting consequences for science and society. </2>
 <3> In this work, we argue that NLP models can help domain experts distill and understand the literature in this complex, high-stakes area. </3>
 <4> Our task is to automatically identify contradictory claims about COVID-19 drug efficacy. </4>
 <5> We frame this as a natural language inference problem and offer a new NLI dataset created by domain experts. </5>
 <6> The NLI framing allows us to create curricula combining existing datasets and our own. </6>
 <7> The resulting models are useful investigative tools. </7>
 <8> We provide a case study of how these models help a domain expert summarize and assess evidence concerning remdisivir and hydroxychloroquine. </8>
"
2212.01378,ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning,"  Pretraining has been shown to scale well with compute, data size and data
diversity. Multitask learning trains on a mixture of supervised datasets and
produces improved performance compared to self-supervised pretraining. Until
now, massively multitask learning required simultaneous access to all datasets
in the mixture and heavy compute resources that are only available to
well-resourced teams.
  In this paper, we propose ColD Fusion, a method that provides the benefits of
multitask learning but leverages distributed computation and requires limited
communication and no sharing of data. Consequentially, ColD Fusion can create a
synergistic loop, where finetuned models can be recycled to continually improve
the pretrained model they are based on. We show that ColD Fusion yields
comparable benefits to multitask pretraining by producing a model that (a)
attains strong performance on all of the datasets it was multitask trained on
and (b) is a better starting point for finetuning on unseen datasets. We find
ColD Fusion outperforms RoBERTa and even previous multitask models.
Specifically, when training and testing on 35 diverse datasets, ColD
Fusion-based model outperforms RoBERTa by 2.45 points in average without any
changes to the architecture.
","<1>  </1>
 <2> Pretraining has been shown to scale well with compute, data size and data diversity. </2>
 <3> Multitask learning trains on a mixture of supervised datasets and produces improved performance compared to self-supervised pretraining. </3>
 <4> Until now, massively multitask learning required simultaneous access to all datasets in the mixture and heavy compute resources that are only available to well-resourced teams. </4>
 <5> In this paper, we propose ColD Fusion, a method that provides the benefits of multitask learning but leverages distributed computation and requires limited communication and no sharing of data. </5>
 <6> Consequentially, ColD Fusion can create a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based on. </6>
 <7> We show that ColD Fusion yields comparable benefits to multitask pretraining by producing a model that (a) attains strong performance on all of the datasets it was multitask trained on and (b) is a better starting point for finetuning on unseen datasets. </7>
 <8> We find ColD Fusion outperforms RoBERTa and even previous multitask models. </8>
 <9>  </9>
 <10> Specifically, when training and testing on 35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.45 points in average without any changes to the architecture. </10>
 <11>  </11>
"
2212.09939,AnyTOD: A Programmable Task-Oriented Dialog System,"  We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system
capable of handling unseen tasks without task-specific training. We view TOD as
a program executed by a language model (LM), where program logic and ontology
is provided by a designer as a schema. To enable generalization to unseen
schemas and programs without prior training, AnyTOD adopts a neuro-symbolic
approach. A neural LM keeps track of events occurring during a conversation and
a symbolic program implementing the dialog policy is executed to recommend next
actions AnyTOD should take. This approach drastically reduces data annotation
and model training requirements, addressing the enduring challenge of rapidly
adapting a TOD system to unseen tasks and domains. We demonstrate
state-of-the-art results on STAR, ABCD and SGD benchmarks. We also demonstrate
strong zero-shot transfer ability in low-resource settings, such as zero-shot
on MultiWOZ. In addition, we release STARv2, an updated version of the STAR
dataset with richer annotations, for benchmarking zero-shot end-to-end TOD
models.
","<1>  </1>
 <2> We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system capable of handling unseen tasks without task-specific training. </2>
 <3> We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema. </3>
 <4> To enable generalization to unseen schemas and programs without prior training, AnyTOD adopts a neuro-symbolic approach. </4>
 <5> A neural LM keeps track of events occurring during a conversation and a symbolic program implementing the dialog policy is executed to recommend next actions AnyTOD should take. </5>
 <6> This approach drastically reduces data annotation and model training requirements, addressing the enduring challenge of rapidly adapting a TOD system to unseen tasks and domains. </6>
 <7> We demonstrate state-of-the-art results on STAR, ABCD and SGD benchmarks. </7>
 <8> We also demonstrate strong zero-shot transfer ability in low-resource settings, such as zero-shot on MultiWOZ. </8>
 <9> In addition, we release STARv2, an updated version of the STAR dataset with richer annotations, for benchmarking zero-shot end-to-end TOD models. </9>
"
1412.1866,Integer-Programming Ensemble of Temporal-Relations Classifiers,"  The extraction and understanding of temporal events and their relations are
major challenges in natural language processing. Processing text on a
sentence-by-sentence or expression-by-expression basis often fails, in part due
to the challenge of capturing the global consistency of the text. We present an
ensemble method, which reconciles the outputs of multiple classifiers of
temporal expressions across the text using integer programming. Computational
experiments show that the ensemble improves upon the best individual results
from two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and
SemEval-2016 Task 12 (Clinical TempEval).
","<1>  </1>
 <2> The extraction and understanding of temporal events and their relations are major challenges in natural language processing. </2>
 <3> Processing text on a sentence-by-sentence or expression-by-expression basis often fails, in part due to the challenge of capturing the global consistency of the text. </3>
 <4> We present an ensemble method, which reconciles the outputs of multiple classifiers of temporal expressions across the text using integer programming. </4>
 <5> Computational experiments show that the ensemble improves upon the best individual results from two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and SemEval-2016 Task 12 (Clinical TempEval). </5>
"
1712.07108,Improved Regularization Techniques for End-to-End Speech Recognition,"  Regularization is important for end-to-end speech models, since the models
are highly flexible and easy to overfit. Data augmentation and dropout has been
important for improving end-to-end models in other domains. However, they are
relatively under explored for end-to-end speech models. Therefore, we
investigate the effectiveness of both methods for end-to-end trainable, deep
speech recognition models. We augment audio data through random perturbations
of tempo, pitch, volume, temporal alignment, and adding random noise.We further
investigate the effect of dropout when applied to the inputs of all layers of
the network. We show that the combination of data augmentation and dropout give
a relative performance improvement on both Wall Street Journal (WSJ) and
LibriSpeech dataset of over 20%. Our model performance is also competitive with
other end-to-end speech models on both datasets.
","<1>  </1>
 <2> Regularization is important for end-to-end speech models, since the models are highly flexible and easy to overfit. </2>
 <3> Data augmentation and dropout has been important for improving end-to-end models in other domains. </3>
 <4> However, they are relatively under explored for end-to-end speech models. </4>
 <5> Therefore, we investigate the effectiveness of both methods for end-to-end trainable, deep speech recognition models. </5>
 <6> We augment audio data through random perturbations of tempo, pitch, volume, temporal alignment, and adding random noise. </6>
 <7> We further investigate the effect of dropout when applied to the inputs of all layers of the network. </7>
 <8> We show that the combination of data augmentation and dropout give a relative performance improvement on both Wall Street Journal (WSJ) and LibriSpeech dataset of over 20%. </8>
 <9> Our model performance is also competitive with other end-to-end speech models on both datasets. </9>
"
2212.10403,Towards Reasoning in Large Language Models: A Survey,"  Reasoning is a fundamental aspect of human intelligence that plays a crucial
role in activities such as problem solving, decision making, and critical
thinking. In recent years, large language models (LLMs) have made significant
progress in natural language processing, and there is observation that these
models may exhibit reasoning abilities when they are sufficiently large.
However, it is not yet clear to what extent LLMs are capable of reasoning. This
paper provides a comprehensive overview of the current state of knowledge on
reasoning in LLMs, including techniques for improving and eliciting reasoning
in these models, methods and benchmarks for evaluating reasoning abilities,
findings and implications of previous research in this field, and suggestions
on future directions. Our aim is to provide a detailed and up-to-date review of
this topic and stimulate meaningful discussion and future work.
","<1>  </1>
 <2> Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. </2>
 <3> In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. </3>
 <4> However, it is not yet clear to what extent LLMs are capable of reasoning. </4>
 <5> This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. </5>
 <6> Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work. </6>
"
2012.02594,"To Schedule or not to Schedule: Extracting Task Specific Temporal
  Entities and Associated Negation Constraints","  State of the art research for date-time entity extraction from text is task
agnostic. Consequently, while the methods proposed in literature perform well
for generic date-time extraction from texts, they don't fare as well on task
specific date-time entity extraction where only a subset of the date-time
entities present in the text are pertinent to solving the task. Furthermore,
some tasks require identifying negation constraints associated with the
date-time entities to correctly reason over time. We showcase a novel model for
extracting task-specific date-time entities along with their negation
constraints. We show the efficacy of our method on the task of date-time
understanding in the context of scheduling meetings for an email-based digital
AI scheduling assistant. Our method achieves an absolute gain of 19\% f-score
points compared to baseline methods in detecting the date-time entities
relevant to scheduling meetings and a 4\% improvement over baseline methods for
detecting negation constraints over date-time entities.
","<1> State of the art research for date-time entity extraction from text is task agnostic. </1>
 <2> Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they don't fare as well on task specific date-time entity extraction where only a subset of the date-time entities present in the text are pertinent to solving the task. </2>
 <3> Furthermore, some tasks require identifying negation constraints associated with the date-time entities to correctly reason over time. </3>
 <4> We showcase a novel model for extracting task-specific date-time entities along with their negation constraints. </4>
 <5> We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings for an email-based digital AI scheduling assistant. </5>
 <6> Our method achieves an absolute gain of 19\% f-score points compared to baseline methods in detecting the date-time entities relevant to scheduling meetings and a 4\% improvement over baseline methods for detecting negation constraints over date-time entities. </6>
"
2112.00405,NER-BERT: A Pre-trained Model for Low-Resource Entity Tagging,"  Named entity recognition (NER) models generally perform poorly when large
training datasets are unavailable for low-resource domains. Recently,
pre-training a large-scale language model has become a promising direction for
coping with the data scarcity issue. However, the underlying discrepancies
between the language modeling and NER task could limit the models' performance,
and pre-training for the NER task has rarely been studied since the collected
NER datasets are generally small or large but with low quality. In this paper,
we construct a massive NER corpus with a relatively high quality, and we
pre-train a NER-BERT model based on the created dataset. Experimental results
show that our pre-trained model can significantly outperform BERT as well as
other strong baselines in low-resource scenarios across nine diverse domains.
Moreover, a visualization of entity representations further indicates the
effectiveness of NER-BERT for categorizing a variety of entities.
","<1> Named entity recognition (NER) models generally perform poorly when large training datasets are unavailable for low-resource domains. </1>
 <2> Recently, pre-training a large-scale language model has become a promising direction for coping with the data scarcity issue. </2>
 <3> However, the underlying discrepancies between the language modeling and NER task could limit the models' performance, and pre-training for the NER task has rarely been studied since the collected NER datasets are generally small or large but with low quality. </3>
 <4> In this paper, we construct a massive NER corpus with a relatively high quality, and we pre-train a NER-BERT model based on the created dataset. </4>
 <5> Experimental results show that our pre-trained model can significantly outperform BERT as well as other strong baselines in low-resource scenarios across nine diverse domains. </5>
 <6> Moreover, a visualization of entity representations further indicates the effectiveness of NER-BERT for categorizing a variety of entities. </6>
"
2012.15699,"Better Robustness by More Coverage: Adversarial Training with Mixup
  Augmentation for Robust Fine-tuning","  Pretrained language models (PLMs) perform poorly under adversarial attacks.
To improve the adversarial robustness, adversarial data augmentation (ADA) has
been widely adopted to cover more search space of adversarial attacks by adding
textual adversarial examples during training. However, the number of
adversarial examples for text augmentation is still extremely insufficient due
to the exponentially large attack search space. In this work, we propose a
simple and effective method to cover a much larger proportion of the attack
search space, called Adversarial and Mixup Data Augmentation (AMDA).
Specifically, AMDA linearly interpolates the representations of pairs of
training samples to form new virtual samples, which are more abundant and
diverse than the discrete text adversarial examples in conventional ADA.
Moreover, to fairly evaluate the robustness of different models, we adopt a
challenging evaluation setup, which generates a new set of adversarial examples
targeting each model. In text classification experiments of BERT and RoBERTa,
AMDA achieves significant robustness gains under two strong adversarial attacks
and alleviates the performance degradation of ADA on the clean data. Our code
is available at: https://github.com/thunlp/MixADA .
","<1> Pretrained language models (PLMs) perform poorly under adversarial attacks. </1>
 <2>  </2>
 <3> To improve the adversarial robustness, adversarial data augmentation (ADA) has been widely adopted to cover more search space of adversarial attacks by adding textual adversarial examples during training. </3>
 <4> However, the number of adversarial examples for text augmentation is still extremely insufficient due to the exponentially large attack search space. </4>
 <5> In this work, we propose a simple and effective method to cover a much larger proportion of the attack search space, called Adversarial and Mixup Data Augmentation (AMDA). </5>
 <6>  </6>
 <7> Specifically, AMDA linearly interpolates the representations of pairs of training samples to form new virtual samples, which are more abundant and diverse than the discrete text adversarial examples in conventional ADA. </7>
 <8>  </8>
 <9> Moreover, to fairly evaluate the robustness of different models, we adopt a challenging evaluation setup, which generates a new set of adversarial examples targeting each model. </9>
 <10> In text classification experiments of BERT and RoBERTa, AMDA achieves significant robustness gains under two strong adversarial attacks and alleviates the performance degradation of ADA on the clean data. </10>
 <11> Our code is available at: https://github.com/thunlp/MixADA . </11>
 <12>  </12>
"
2012.08773,Building domain specific lexicon based on TikTok comment dataset,"  In the sentiment analysis task, predicting the sentiment tendency of a
sentence is an important branch. Previous research focused more on sentiment
analysis in English, for example, analyzing the sentiment tendency of sentences
based on Valence, Arousal, Dominance of sentences. the emotional tendency is
different between the two languages. For example, the sentence order between
Chinese and English may present different emotions. This paper tried a method
that builds a domain-specific lexicon. In this way, the model can classify
Chinese words with emotional tendency. In this approach, based on the [13], an
ultra-dense space embedding table is trained through word embedding of Chinese
TikTok review and emotional lexicon sources(seed words). The result of the
model is a domain-specific lexicon, which presents the emotional tendency of
words. I collected Chinese TikTok comments as training data. By comparing The
training results with the PCA method to evaluate the performance of the model
in Chinese sentiment classification, the results show that the model has done
well in Chinese. The source code has released on
github:https://github.com/h2222/douyin_comment_dataset
","<1>  </1>
 <2> In the sentiment analysis task, predicting the sentiment tendency of a sentence is an important branch. </2>
 <3> Previous research focused more on sentiment analysis in English, for example, analyzing the sentiment tendency of sentences based on Valence, Arousal, Dominance of sentences. </3>
 <4> the emotional tendency is different between the two languages. </4>
 <5> For example, the sentence order between Chinese and English may present different emotions. </5>
 <6> This paper tried a method that builds a domain-specific lexicon. </6>
 <7> In this way, the model can classify </7>
 <8> Chinese words with emotional tendency. </8>
 <9> In this approach, based on the [13], an ultra-dense space embedding table is trained through word embedding of Chinese TikTok review and emotional lexicon sources(seed words). </9>
 <10> The result of the model is a domain-specific lexicon, which presents the emotional tendency of words. </10>
 <11> I collected Chinese TikTok comments as training data. </11>
 <12> By comparing The training results with the PCA method to evaluate the performance of the model in Chinese sentiment classification, the results show that the model has done well in Chinese. </12>
 <13> The source code has released on github:https://github.com/h2222/douyin_comment_dataset </13>
"
1912.01774,Acquiring Knowledge from Pre-trained Model to Neural Machine Translation,"  Pre-training and fine-tuning have achieved great success in the natural
language process field. The standard paradigm of exploiting them includes two
steps: first, pre-training a model, e.g. BERT, with a large scale unlabeled
monolingual data. Then, fine-tuning the pre-trained model with labeled data
from downstream tasks. However, in neural machine translation (NMT), we address
the problem that the training objective of the bilingual task is far different
from the monolingual pre-trained model. This gap leads that only using
fine-tuning in NMT can not fully utilize prior language knowledge. In this
paper, we propose an APT framework for acquiring knowledge from the pre-trained
model to NMT. The proposed approach includes two modules: 1). a dynamic fusion
mechanism to fuse task-specific features adapted from general knowledge into
NMT network, 2). a knowledge distillation paradigm to learn language knowledge
continuously during the NMT training process. The proposed approach could
integrate suitable knowledge from pre-trained models to improve the NMT.
Experimental results on WMT English to German, German to English and Chinese to
English machine translation tasks show that our model outperforms strong
baselines and the fine-tuning counterparts.
","<1> Pre-training and fine-tuning have achieved great success in the natural language process field. </1>
 <2> The standard paradigm of exploiting them includes two steps: first, pre-training a model, e.g. BERT, with a large scale unlabeled monolingual data. </2>
 <3> Then, fine-tuning the pre-trained model with labeled data from downstream tasks. </3>
 <4> However, in neural machine translation (NMT), we address the problem that the training objective of the bilingual task is far different from the monolingual pre-trained model. </4>
 <5> This gap leads that only using fine-tuning in NMT can not fully utilize prior language knowledge. </5>
 <6> In this paper, we propose an APT framework for acquiring knowledge from the pre-trained model to NMT. </6>
 <7> The proposed approach includes two modules: 1). </7>
 <8> a dynamic fusion mechanism to fuse task-specific features adapted from general knowledge into NMT network, 2). </8>
 <9> a knowledge distillation paradigm to learn language knowledge continuously during the NMT training process. </9>
 <10> The proposed approach could integrate suitable knowledge from pre-trained models to improve the NMT. </10>
 <11>  </11>
 <12> Experimental results on WMT English to German, German to English and Chinese to English machine translation tasks show that our model outperforms strong baselines and the fine-tuning counterparts. </12>
"
1812.00436,Learning Representations of Social Media Users,"  User representations are routinely used in recommendation systems by platform
developers, targeted advertisements by marketers, and by public policy
researchers to gauge public opinion across demographic groups. Computer
scientists consider the problem of inferring user representations more
abstractly; how does one extract a stable user representation - effective for
many downstream tasks - from a medium as noisy and complicated as social media?
  The quality of a user representation is ultimately task-dependent (e.g. does
it improve classifier performance, make more accurate recommendations in a
recommendation system) but there are proxies that are less sensitive to the
specific task. Is the representation predictive of latent properties such as a
person's demographic features, socioeconomic class, or mental health state? Is
it predictive of the user's future behavior?
  In this thesis, we begin by showing how user representations can be learned
from multiple types of user behavior on social media. We apply several
extensions of generalized canonical correlation analysis to learn these
representations and evaluate them at three tasks: predicting future hashtag
mentions, friending behavior, and demographic features. We then show how user
features can be employed as distant supervision to improve topic model fit.
Finally, we show how user features can be integrated into and improve existing
classifiers in the multitask learning framework. We treat user representations
- ground truth gender and mental health features - as auxiliary tasks to
improve mental health state prediction. We also use distributed user
representations learned in the first chapter to improve tweet-level stance
classifiers, showing that distant user information can inform classification
tasks at the granularity of a single message.
","<1> User representations are routinely used in recommendation systems by platform developers, targeted advertisements by marketers, and by public policy researchers to gauge public opinion across demographic groups. </1>
 <2> Computer scientists consider the problem of inferring user representations more abstractly; how does one extract a stable user representation - effective for many downstream tasks - from a medium as noisy and complicated as social media? </2>
 <3>  </3>
 <4> The quality of a user representation is ultimately task-dependent (e.g. does it improve classifier performance, make more accurate recommendations in a recommendation system) but there are proxies that are less sensitive to the specific task. </4>
 <5> Is the representation predictive of latent properties such as a person's demographic features, socioeconomic class, or mental health state? </5>
 <6> Is it predictive of the user's future behavior? </6>
 <7> In this thesis, we begin by showing how user representations can be learned from multiple types of user behavior on social media. </7>
 <8> We apply several extensions of generalized canonical correlation analysis to learn these representations and evaluate them at three tasks: predicting future hashtag mentions, friending behavior, and demographic features. </8>
 <9> We then show how user features can be employed as distant supervision to improve topic model fit. </9>
 <10>  </10>
 <11> Finally, we show how user features can be integrated into and improve existing classifiers in the multitask learning framework. </11>
 <12> We treat user representations - ground truth gender and mental health features - as auxiliary tasks to improve mental health state prediction. </12>
 <13> We also use distributed user representations learned in the first chapter to improve tweet-level stance classifiers, showing that distant user information can inform classification tasks at the granularity of a single message. </13>
"
2012.15671,Vocabulary Learning via Optimal Transport for Neural Machine Translation,"  The choice of token vocabulary affects the performance of machine
translation. This paper aims to figure out what is a good vocabulary and
whether one can find the optimal vocabulary without trial training. To answer
these questions, we first provide an alternative understanding of the role of
vocabulary from the perspective of information theory. Motivated by this, we
formulate the quest of vocabularization -- finding the best token dictionary
with a proper size -- as an optimal transport (OT) problem. We propose VOLT, a
simple and efficient solution without trial training. Empirical results show
that VOLT outperforms widely-used vocabularies in diverse scenarios, including
WMT-14 English-German and TED's 52 translation directions. For example, VOLT
achieves almost 70% vocabulary size reduction and 0.5 BLEU gain on
English-German translation. Also, compared to BPE-search, VOLT reduces the
search time from 384 GPU hours to 30 GPU hours on English-German translation.
Codes are available at https://github.com/Jingjing-NLP/VOLT .
","<1> The choice of token vocabulary affects the performance of machine translation. </1>
 <2> This paper aims to figure out what is a good vocabulary and whether one can find the optimal vocabulary without trial training. </2>
 <3> To answer these questions, we first provide an alternative understanding of the role of vocabulary from the perspective of information theory. </3>
 <4> Motivated by this, we formulate the quest of vocabularization -- finding the best token dictionary with a proper size -- as an optimal transport (OT) problem. </4>
 <5> We propose VOLT, a simple and efficient solution without trial training. </5>
 <6> Empirical results show that VOLT outperforms widely-used vocabularies in diverse scenarios, including WMT-14 English-German and TED's 52 translation directions. </6>
 <7> For example, VOLT achieves almost 70% vocabulary size reduction and 0.5 BLEU gain on English-German translation. </7>
 <8> Also, compared to BPE-search, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation. </8>
 <9>  </9>
 <10> Codes are available at https://github.com/Jingjing-NLP/VOLT . </10>
 <11>  </11>
"
2112.01822,Translating Politeness Across Cultures: Case of Hindi and English,"  In this paper, we present a corpus based study of politeness across two
languages-English and Hindi. It studies the politeness in a translated parallel
corpus of Hindi and English and sees how politeness in a Hindi text is
translated into English. We provide a detailed theoretical background in which
the comparison is carried out, followed by a brief description of the
translated data within this theoretical model. Since politeness may become one
of the major reasons of conflict and misunderstanding, it is a very important
phenomenon to be studied and understood cross-culturally, particularly for such
purposes as machine translation.
","<1>  </1>
 <2> In this paper, we present a corpus based study of politeness across two languages-English and Hindi. </2>
 <3> It studies the politeness in a translated parallel corpus of Hindi and English and sees how politeness in a Hindi text is translated into English. </3>
 <4> We provide a detailed theoretical background in which the comparison is carried out, followed by a brief description of the translated data within this theoretical model. </4>
 <5> Since politeness may become one of the major reasons of conflict and misunderstanding, it is a very important phenomenon to be studied and understood cross-culturally, particularly for such purposes as machine translation. </5>
"
2212.01083,Cross-Modal Mutual Learning for Cued Speech Recognition,"  Automatic Cued Speech Recognition (ACSR) provides an intelligent
human-machine interface for visual communications, where the Cued Speech (CS)
system utilizes lip movements and hand gestures to code spoken language for
hearing-impaired people. Previous ACSR approaches often utilize direct feature
concatenation as the main fusion paradigm. However, the asynchronous modalities
i.e., lip, hand shape and hand position) in CS may cause interference for
feature concatenation. To address this challenge, we propose a transformer
based cross-modal mutual learning framework to prompt multi-modal interaction.
Compared with the vanilla self-attention, our model forces modality-specific
information of different modalities to pass through a modality-invariant
codebook, collating linguistic representations for tokens of each modality.
Then the shared linguistic knowledge is used to re-synchronize multi-modal
sequences. Moreover, we establish a novel large-scale multi-speaker CS dataset
for Mandarin Chinese. To our knowledge, this is the first work on ACSR for
Mandarin Chinese. Extensive experiments are conducted for different languages
i.e., Chinese, French, and British English). Results demonstrate that our model
exhibits superior recognition performance to the state-of-the-art by a large
margin.
","<1> Automatic Cued Speech Recognition (ACSR) provides an intelligent human-machine interface for visual communications, where the Cued Speech (CS) system utilizes lip movements and hand gestures to code spoken language for hearing-impaired people. </1>
 <2> Previous ACSR approaches often utilize direct feature concatenation as the main fusion paradigm. </2>
 <3> However, the asynchronous modalities i.e., lip, hand shape and hand position) in CS may cause interference for feature concatenation. </3>
 <4> To address this challenge, we propose a transformer based cross-modal mutual learning framework to prompt multi-modal interaction. </4>
 <5>  </5>
 <6> Compared with the vanilla self-attention, our model forces modality-specific information of different modalities to pass through a modality-invariant codebook, collating linguistic representations for tokens of each modality. </6>
 <7>  </7>
 <8> Then the shared linguistic knowledge is used to re-synchronize multi-modal sequences. </8>
 <9> Moreover, we establish a novel large-scale multi-speaker CS dataset for Mandarin Chinese. </9>
 <10> To our knowledge, this is the first work on ACSR for Mandarin Chinese. </10>
 <11> Extensive experiments are conducted for different languages i.e., Chinese, French, and British English). </11>
 <12> Results demonstrate that our model exhibits superior recognition performance to the state-of-the-art by a large margin. </12>
"
2112.07610,"Improving Compositional Generalization with Latent Structure and Data
  Augmentation","  Generic unstructured neural networks have been shown to struggle on
out-of-distribution compositional generalization. Compositional data
augmentation via example recombination has transferred some prior knowledge
about compositionality to such black-box neural models for several semantic
parsing tasks, but this often required task-specific engineering or provided
limited gains.
  We present a more powerful data recombination method using a model called
Compositional Structure Learner (CSL). CSL is a generative model with a
quasi-synchronous context-free grammar backbone, which we induce from the
training data. We sample recombined examples from CSL and add them to the
fine-tuning data of a pre-trained sequence-to-sequence model (T5). This
procedure effectively transfers most of CSL's compositional bias to T5 for
diagnostic tasks, and results in a model even stronger than a T5-CSL ensemble
on two real world compositional generalization tasks. This results in new
state-of-the-art performance for these challenging semantic parsing tasks
requiring generalization to both natural language variation and novel
compositions of elements.
","<1> Generic unstructured neural networks have been shown to struggle on out-of-distribution compositional generalization. </1>
 <2> Compositional data augmentation via example recombination has transferred some prior knowledge about compositionality to such black-box neural models for several semantic parsing tasks, but this often required task-specific engineering or provided limited gains. </2>
 <3> We present a more powerful data recombination method using a model called Compositional Structure Learner (CSL). </3>
 <4> CSL is a generative model with a quasi-synchronous context-free grammar backbone, which we induce from the training data. </4>
 <5> We sample recombined examples from CSL and add them to the fine-tuning data of a pre-trained sequence-to-sequence model (T5). </5>
 <6> This procedure effectively transfers most of CSL's compositional bias to T5 for diagnostic tasks, and results in a model even stronger than a T5-CSL ensemble on two real world compositional generalization tasks. </6>
 <7> This results in new state-of-the-art performance for these challenging semantic parsing tasks requiring generalization to both natural language variation and novel compositions of elements. </7>
"
1912.10435,BERTQA -- Attention on Steroids,"  In this work, we extend the Bidirectional Encoder Representations from
Transformers (BERT) with an emphasis on directed coattention to obtain an
improved F1 performance on the SQUAD2.0 dataset. The Transformer architecture
on which BERT is based places hierarchical global attention on the
concatenation of the context and query. Our additions to the BERT architecture
augment this attention with a more focused context to query (C2Q) and query to
context (Q2C) attention via a set of modified Transformer encoder units. In
addition, we explore adding convolution-based feature extraction within the
coattention architecture to add localized information to self-attention. We
found that coattention significantly improves the no answer F1 by 4 points in
the base and 1 point in the large architecture. After adding skip connections
the no answer F1 improved further without causing an additional loss in has
answer F1. The addition of localized feature extraction added to attention
produced an overall dev F1 of 77.03 in the base architecture. We applied our
findings to the large BERT model which contains twice as many layers and
further used our own augmented version of the SQUAD 2.0 dataset created by back
translation, which we have named SQUAD 2.Q. Finally, we performed
hyperparameter tuning and ensembled our best models for a final F1/EM of
82.317/79.442 (Attention on Steroids, PCE Test Leaderboard).
","<1>  </1>
 <2> In this work, we extend the Bidirectional Encoder Representations from Transformers (BERT) with an emphasis on directed coattention to obtain an improved F1 performance on the SQUAD2.0 dataset. </2>
 <3> The Transformer architecture on which BERT is based places hierarchical global attention on the concatenation of the context and query. </3>
 <4> Our additions to the BERT architecture augment this attention with a more focused context to query (C2Q) and query to context (Q2C) attention via a set of modified Transformer encoder units. </4>
 <5> In addition, we explore adding convolution-based feature extraction within the coattention architecture to add localized information to self-attention. </5>
 <6> We found that coattention significantly improves the no answer F1 by 4 points in the base and 1 point in the large architecture. </6>
 <7> After adding skip connections the no answer F1 improved further without causing an additional loss in has answer F1. </7>
 <8> The addition of localized feature extraction added to attention produced an overall dev F1 of 77.03 in the base architecture. </8>
 <9> We applied our findings to the large BERT model which contains twice as many layers and further used our own augmented version of the SQUAD 2.0 dataset created by back translation, which we have named SQUAD 2.Q. </9>
 <10> Finally, we performed hyperparameter tuning and ensembled our best models for a final F1/EM of 82.317/79.442 </10>
 <11> (Attention on Steroids, PCE Test Leaderboard). </11>
"
2112.02970,"Fast and Accurate End-to-End Span-based Semantic Role Labeling as
  Word-based Graph Parsing","  This paper proposes to cast end-to-end span-based SRL as a word-based graph
parsing task. The major challenge is how to represent spans at the word level.
Borrowing ideas from research on Chinese word segmentation and named entity
recognition, we propose and compare four different schemata of graph
representation, i.e., BES, BE, BIES, and BII, among which we find that the BES
schema performs the best. We further gain interesting insights through detailed
analysis. Moreover, we propose a simple constrained Viterbi procedure to ensure
the legality of the output graph according to the constraints of the SRL
structure. We conduct experiments on two widely used benchmark datasets, i.e.,
CoNLL05 and CoNLL12. Results show that our word-based graph parsing approach
achieves consistently better performance than previous results, under all
settings of end-to-end and predicate-given, without and with pre-trained
language models (PLMs). More importantly, our model can parse 669/252 sentences
per second, without and with PLMs respectively.
","<1> This paper proposes to cast end-to-end span-based SRL as a word-based graph parsing task. </1>
 <2> The major challenge is how to represent spans at the word level. </2>
 <3>  </3>
 <4> Borrowing ideas from research on Chinese word segmentation and named entity recognition, we propose and compare four different schemata of graph representation, i.e., BES, BE, BIES, and BII, among which we find that the BES schema performs the best. </4>
 <5> We further gain interesting insights through detailed analysis. </5>
 <6> Moreover, we propose a simple constrained Viterbi procedure to ensure the legality of the output graph according to the constraints of the SRL structure. </6>
 <7> We conduct experiments on two widely used benchmark datasets, i.e., CoNLL05 and CoNLL12. </7>
 <8> Results show that our word-based graph parsing approach achieves consistently better performance than previous results, under all settings of end-to-end and predicate-given, without and with pre-trained language models (PLMs). </8>
 <9> More importantly, our model can parse 669/252 sentences per second, without and with PLMs respectively. </9>
"
1612.04988,"TeKnowbase: Towards Construction of a Knowledge-base of Technical
  Concepts","  In this paper, we describe the construction of TeKnowbase, a knowledge-base
of technical concepts in computer science. Our main information sources are
technical websites such as Webopedia and Techtarget as well as Wikipedia and
online textbooks. We divide the knowledge-base construction problem into two
parts -- the acquisition of entities and the extraction of relationships among
these entities. Our knowledge-base consists of approximately 100,000 triples.
We conducted an evaluation on a sample of triples and report an accuracy of a
little over 90\%. We additionally conducted classification experiments on
StackOverflow data with features from TeKnowbase and achieved improved
classification accuracy.
","<1>  </1>
 <2> In this paper, we describe the construction of TeKnowbase, a knowledge-base of technical concepts in computer science. </2>
 <3> Our main information sources are technical websites such as Webopedia and Techtarget as well as Wikipedia and online textbooks. </3>
 <4> We divide the knowledge-base construction problem into two parts -- the acquisition of entities and the extraction of relationships among these entities. </4>
 <5> Our knowledge-base consists of approximately 100,000 triples. </5>
 <6>  </6>
 <7> We conducted an evaluation on a sample of triples and report an accuracy of a little over 90\%. </7>
 <8> We additionally conducted classification experiments on StackOverflow data with features from TeKnowbase and achieved improved classification accuracy. </8>
"
1812.06876,Multi-task learning to improve natural language understanding,"  Recently advancements in sequence-to-sequence neural network architectures
have led to an improved natural language understanding. When building a neural
network-based Natural Language Understanding component, one main challenge is
to collect enough training data. The generation of a synthetic dataset is an
inexpensive and quick way to collect data. Since this data often has less
variety than real natural language, neural networks often have problems to
generalize to unseen utterances during testing. In this work, we address this
challenge by using multi-task learning. We train out-of-domain real data
alongside in-domain synthetic data to improve natural language understanding.
We evaluate this approach in the domain of airline travel information with two
synthetic datasets. As out-of-domain real data, we test two datasets based on
the subtitles of movies and series. By using an attention-based encoder-decoder
model, we were able to improve the F1-score over strong baselines from 80.76 %
to 84.98 % in the smaller synthetic dataset.
","<1> Recently advancements in sequence-to-sequence neural network architectures have led to an improved natural language understanding. </1>
 <2> When building a neural network-based Natural Language Understanding component, one main challenge is to collect enough training data. </2>
 <3> The generation of a synthetic dataset is an inexpensive and quick way to collect data. </3>
 <4> Since this data often has less variety than real natural language, neural networks often have problems to generalize to unseen utterances during testing. </4>
 <5> In this work, we address this challenge by using multi-task learning. </5>
 <6> We train out-of-domain real data alongside in-domain synthetic data to improve natural language understanding. </6>
 <7>  </7>
 <8> We evaluate this approach in the domain of airline travel information with two synthetic datasets. </8>
 <9> As out-of-domain real data, we test two datasets based on the subtitles of movies and series. </9>
 <10> By using an attention-based encoder-decoder model, we were able to improve the F1-score over strong baselines from 80.76 % to 84.98 % in the smaller synthetic dataset. </10>
"
2212.12407,"Text classification in shipping industry using unsupervised models and
  Transformer based supervised models","  Obtaining labelled data in a particular context could be expensive and time
consuming. Although different algorithms, including unsupervised learning,
semi-supervised learning, self-learning have been adopted, the performance of
text classification varies with context. Given the lack of labelled dataset, we
proposed a novel and simple unsupervised text classification model to classify
cargo content in international shipping industry using the Standard
International Trade Classification (SITC) codes. Our method stems from
representing words using pretrained Glove Word Embeddings and finding the most
likely label using Cosine Similarity. To compare unsupervised text
classification model with supervised classification, we also applied several
Transformer models to classify cargo content. Due to lack of training data, the
SITC numerical codes and the corresponding textual descriptions were used as
training data. A small number of manually labelled cargo content data was used
to evaluate the classification performances of the unsupervised classification
and the Transformer based supervised classification. The comparison reveals
that unsupervised classification significantly outperforms Transformer based
supervised classification even after increasing the size of the training
dataset by 30%. Lacking training data is a key bottleneck that prohibits deep
learning models (such as Transformers) from successful practical applications.
Unsupervised classification can provide an alternative efficient and effective
method to classify text when there is scarce training data.
","<1>  </1>
 <2> Obtaining labelled data in a particular context could be expensive and time consuming. </2>
 <3> Although different algorithms, including unsupervised learning, semi-supervised learning, self-learning have been adopted, the performance of text classification varies with context. </3>
 <4> Given the lack of labelled dataset, we proposed a novel and simple unsupervised text classification model to classify cargo content in international shipping industry using the Standard International Trade Classification (SITC) codes. </4>
 <5> Our method stems from representing words using pretrained Glove Word Embeddings and finding the most likely label using Cosine Similarity. </5>
 <6> To compare unsupervised text classification model with supervised classification, we also applied several Transformer models to classify cargo content. </6>
 <7> Due to lack of training data, the SITC numerical codes and the corresponding textual descriptions were used as training data. </7>
 <8> A small number of manually labelled cargo content data was used to evaluate the classification performances of the unsupervised classification and the Transformer based supervised classification. </8>
 <9> The comparison reveals that unsupervised classification significantly outperforms Transformer based supervised classification even after increasing the size of the training dataset by 30%. </9>
 <10> Lacking training data is a key bottleneck that prohibits deep learning models (such as Transformers) from successful practical applications. </10>
 <11>  </11>
 <12> Unsupervised classification can provide an alternative efficient and effective method to classify text when there is scarce training data. </12>
"
2012.05786,Exploring Pair-Wise NMT for Indian Languages,"  In this paper, we address the task of improving pair-wise machine translation
for specific low resource Indian languages. Multilingual NMT models have
demonstrated a reasonable amount of effectiveness on resource-poor languages.
In this work, we show that the performance of these models can be significantly
improved upon by using back-translation through a filtered back-translation
process and subsequent fine-tuning on the limited pair-wise language corpora.
The analysis in this paper suggests that this method can significantly improve
a multilingual model's performance over its baseline, yielding state-of-the-art
results for various Indian languages.
","<1>  </1>
 <2> In this paper, we address the task of improving pair-wise machine translation for specific low resource Indian languages. </2>
 <3> Multilingual NMT models have demonstrated a reasonable amount of effectiveness on resource-poor languages. </3>
 <4>  </4>
 <5> In this work, we show that the performance of these models can be significantly improved upon by using back-translation through a filtered back-translation process and subsequent fine-tuning on the limited pair-wise language corpora. </5>
 <6>  </6>
 <7> The analysis in this paper suggests that this method can significantly improve a multilingual model's performance over its baseline, yielding state-of-the-art results for various Indian languages. </7>
"
1112.6286,"Visualization and Analysis of Frames in Collections of Messages: Content
  Analysis and the Measurement of Meaning","  A step-to-step introduction is provided on how to generate a semantic map
from a collection of messages (full texts, paragraphs or statements) using
freely available software and/or SPSS for the relevant statistics and the
visualization. The techniques are discussed in the various theoretical contexts
of (i) linguistics (e.g., Latent Semantic Analysis), (ii) sociocybernetics and
social systems theory (e.g., the communication of meaning), and (iii)
communication studies (e.g., framing and agenda-setting). We distinguish
between the communication of information in the network space (social network
analysis) and the communication of meaning in the vector space. The vector
space can be considered a generated as an architecture by the network of
relations in the network space; words are then not only related, but also
positioned. These positions are expected rather than observed and therefore one
can communicate meaning. Knowledge can be generated when these meanings can
recursively be communicated and therefore also further codified.
","<1> A step-to-step introduction is provided on how to generate a semantic map from a collection of messages (full texts, paragraphs or statements) using freely available software and/or SPSS for the relevant statistics and the visualization. </1>
 <2> The techniques are discussed in the various theoretical contexts of (i) linguistics (e.g., Latent Semantic Analysis), (ii) sociocybernetics and social systems theory (e.g., the communication of meaning), and (iii) communication studies (e.g., framing and agenda-setting). </2>
 <3> We distinguish between the communication of information in the network space (social network analysis) and the communication of meaning in the vector space. </3>
 <4> The vector space can be considered a generated as an architecture by the network of relations in the network space; words are then not only related, but also positioned. </4>
 <5> These positions are expected rather than observed and therefore one can communicate meaning. </5>
 <6> Knowledge can be generated when these meanings can recursively be communicated and therefore also further codified. </6>
"
2212.07156,"MIST: a Large-Scale Annotated Resource and Neural Models for Functions
  of Modal Verbs in English Scientific Text","  Modal verbs (e.g., ""can"", ""should"", or ""must"") occur highly frequently in
scientific articles. Decoding their function is not straightforward: they are
often used for hedging, but they may also denote abilities and restrictions.
Understanding their meaning is important for various NLP tasks such as writing
assistance or accurate information extraction from scientific text.
  To foster research on the usage of modals in this genre, we introduce the
MIST (Modals In Scientific Text) dataset, which contains 3737 modal instances
in five scientific domains annotated for their semantic, pragmatic, or
rhetorical function. We systematically evaluate a set of competitive neural
architectures on MIST. Transfer experiments reveal that leveraging
non-scientific data is of limited benefit for modeling the distinctions in
MIST. Our corpus analysis provides evidence that scientific communities differ
in their usage of modal verbs, yet, classifiers trained on scientific data
generalize to some extent to unseen scientific domains.
","<1> Modal verbs (e.g., ""can"", ""should"", or ""must"") occur highly frequently in scientific articles. </1>
 <2> Decoding their function is not straightforward: they are often used for hedging, but they may also denote abilities and restrictions. </2>
 <3>  </3>
 <4> Understanding their meaning is important for various NLP tasks such as writing assistance or accurate information extraction from scientific text. </4>
 <5>  </5>
 <6> To foster research on the usage of modals in this genre, we introduce the MIST (Modals In Scientific Text) dataset, which contains 3737 modal instances in five scientific domains annotated for their semantic, pragmatic, or rhetorical function. </6>
 <7> We systematically evaluate a set of competitive neural architectures on MIST. </7>
 <8> Transfer experiments reveal that leveraging non-scientific data is of limited benefit for modeling the distinctions in MIST. </8>
 <9> Our corpus analysis provides evidence that scientific communities differ in their usage of modal verbs, yet, classifiers trained on scientific data generalize to some extent to unseen scientific domains. </9>
"
2212.04088,"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large
  Language Models","  This study focuses on using large language models (LLMs) as a planner for
embodied agents that can follow natural language instructions to complete
complex tasks in a visually-perceived environment. The high data cost and poor
sample efficiency of existing methods hinders the development of versatile
agents that are capable of many tasks and can learn new tasks quickly. In this
work, we propose a novel method, LLM-Planner, that harnesses the power of large
language models to do few-shot planning for embodied agents. We further propose
a simple but effective way to enhance LLMs with physical grounding to generate
and update plans that are grounded in the current environment. Experiments on
the ALFRED dataset show that our method can achieve very competitive few-shot
performance: Despite using less than 0.5% of paired training data, LLM-Planner
achieves competitive performance with recent baselines that are trained using
the full training data. Existing methods can barely complete any task
successfully under the same few-shot setting. Our work opens the door for
developing versatile and sample-efficient embodied agents that can quickly
learn many tasks. Website: https://dki-lab.github.io/LLM-Planner
","<1> This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. </1>
 <2> The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. </2>
 <3> In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. </3>
 <4> We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. </4>
 <5> Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. </5>
 <6> Existing methods can barely complete any task successfully under the same few-shot setting. </6>
 <7> Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. </7>
 <8> Website: https://dki-lab.github.io/LLM-Planner </8>
"
2212.09248,"Natural Language to Code Generation in Interactive Data Science
  Notebooks","  Computational notebooks, such as Jupyter notebooks, are interactive computing
environments that are ubiquitous among data scientists to perform data
wrangling and analytic tasks. To measure the performance of AI pair programmers
that automatically synthesize programs for those tasks given natural language
(NL) intents from users, we build ARCADE, a benchmark of 1082 code generation
problems using the pandas data analysis framework in data science notebooks.
ARCADE features multiple rounds of NL-to-code problems from the same notebook.
It requires a model to understand rich multi-modal contexts, such as existing
notebook cells and their execution states as well as previous turns of
interaction. To establish a strong baseline on this challenging task, we
develop PaChiNCo, a 62B code language model (LM) for Python computational
notebooks, which significantly outperforms public code LMs. Finally, we explore
few-shot prompting strategies to elicit better code with step-by-step
decomposition and NL explanation, showing the potential to improve the
diversity and explainability of model predictions.
","<1> Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. </1>
 <2> To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1082 code generation problems using the pandas data analysis framework in data science notebooks. </2>
 <3>  </3>
 <4> ARCADE features multiple rounds of NL-to-code problems from the same notebook. </4>
 <5>  </5>
 <6> It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. </6>
 <7> To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. </7>
 <8> Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. </8>
"
1912.11701,Hybrid MemNet for Extractive Summarization,"  Extractive text summarization has been an extensive research problem in the
field of natural language understanding. While the conventional approaches rely
mostly on manually compiled features to generate the summary, few attempts have
been made in developing data-driven systems for extractive summarization. To
this end, we present a fully data-driven end-to-end deep network which we call
as Hybrid MemNet for single document summarization task. The network learns the
continuous unified representation of a document before generating its summary.
It jointly captures local and global sentential information along with the
notion of summary worthy sentences. Experimental results on two different
corpora confirm that our model shows significant performance gains compared
with the state-of-the-art baselines.
","<1> Extractive text summarization has been an extensive research problem in the field of natural language understanding. </1>
 <2> While the conventional approaches rely mostly on manually compiled features to generate the summary, few attempts have been made in developing data-driven systems for extractive summarization. </2>
 <3> To this end, we present a fully data-driven end-to-end deep network which we call as Hybrid MemNet for single document summarization task. </3>
 <4> The network learns the continuous unified representation of a document before generating its summary. </4>
 <5>  </5>
 <6> It jointly captures local and global sentential information along with the notion of summary worthy sentences. </6>
 <7> Experimental results on two different corpora confirm that our model shows significant performance gains compared with the state-of-the-art baselines. </7>
"
1512.03549,"Words are not Equal: Graded Weighting Model for building Composite
  Document Vectors","  Despite the success of distributional semantics, composing phrases from word
vectors remains an important challenge. Several methods have been tried for
benchmark tasks such as sentiment classification, including word vector
averaging, matrix-vector approaches based on parsing, and on-the-fly learning
of paragraph vectors. Most models usually omit stop words from the composition.
Instead of such an yes-no decision, we consider several graded schemes where
words are weighted according to their discriminatory relevance with respect to
its use in the document (e.g., idf). Some of these methods (particularly
tf-idf) are seen to result in a significant improvement in performance over
prior state of the art. Further, combining such approaches into an ensemble
based on alternate classifiers such as the RNN model, results in an 1.6%
performance improvement on the standard IMDB movie review dataset, and a 7.01%
improvement on Amazon product reviews. Since these are language free models and
can be obtained in an unsupervised manner, they are of interest also for
under-resourced languages such as Hindi as well and many more languages. We
demonstrate the language free aspects by showing a gain of 12% for two review
datasets over earlier results, and also release a new larger dataset for future
testing (Singh,2015).
","<1>  </1>
 <2> Despite the success of distributional semantics, composing phrases from word vectors remains an important challenge. </2>
 <3> Several methods have been tried for benchmark tasks such as sentiment classification, including word vector averaging, matrix-vector approaches based on parsing, and on-the-fly learning of paragraph vectors. </3>
 <4> Most models usually omit stop words from the composition. </4>
 <5>  </5>
 <6> Instead of such an yes-no decision, we consider several graded schemes where words are weighted according to their discriminatory relevance with respect to its use in the document (e.g., idf). </6>
 <7> Some of these methods (particularly tf-idf) are seen to result in a significant improvement in performance over prior state of the art. </7>
 <8> Further, combining such approaches into an ensemble based on alternate classifiers such as the RNN model, results in an 1.6% performance improvement on the standard IMDB movie review dataset, and a 7.01% improvement on Amazon product reviews. </8>
 <9> Since these are language free models and can be obtained in an unsupervised manner, they are of interest also for under-resourced languages such as Hindi as well and many more languages. </9>
 <10> We demonstrate the language free aspects by showing a gain of 12% for two review datasets over earlier results, and also release a new larger dataset for future testing (Singh,2015). </10>
 <11>  </11>
"
1712.07745,Context-aware Path Ranking for Knowledge Base Completion,"  Knowledge base (KB) completion aims to infer missing facts from existing ones
in a KB. Among various approaches, path ranking (PR) algorithms have received
increasing attention in recent years. PR algorithms enumerate paths between
entity pairs in a KB and use those paths as features to train a model for
missing fact prediction. Due to their good performances and high model
interpretability, several methods have been proposed. However, most existing
methods suffer from scalability (high RAM consumption) and feature explosion
(trains on an exponentially large number of features) problems. This paper
proposes a Context-aware Path Ranking (C-PR) algorithm to solve these problems
by introducing a selective path exploration strategy. C-PR learns global
semantics of entities in the KB using word embedding and leverages the
knowledge of entity semantics to enumerate contextually relevant paths using
bidirectional random walk. Experimental results on three large KBs show that
the path features (fewer in number) discovered by C-PR not only improve
predictive performance but also are more interpretable than existing baselines.
","<1> Knowledge base (KB) completion aims to infer missing facts from existing ones in a KB. </1>
 <2> Among various approaches, path ranking (PR) algorithms have received increasing attention in recent years. </2>
 <3> PR algorithms enumerate paths between entity pairs in a KB and use those paths as features to train a model for missing fact prediction. </3>
 <4> Due to their good performances and high model interpretability, several methods have been proposed. </4>
 <5> However, most existing methods suffer from scalability (high RAM consumption) and feature explosion (trains on an exponentially large number of features) problems. </5>
 <6> This paper proposes a Context-aware Path Ranking (C-PR) algorithm to solve these problems by introducing a selective path exploration strategy. </6>
 <7> C-PR learns global semantics of entities in the KB using word embedding and leverages the knowledge of entity semantics to enumerate contextually relevant paths using bidirectional random walk. </7>
 <8> Experimental results on three large KBs show that the path features (fewer in number) discovered by C-PR not only improve predictive performance but also are more interpretable than existing baselines. </8>
"
2012.02819,On-Device Sentence Similarity for SMS Dataset,"  Determining the sentence similarity between Short Message Service (SMS)
texts/sentences plays a significant role in mobile device industry. Gauging the
similarity between SMS data is thus necessary for various applications like
enhanced searching and navigation, clubbing together SMS of similar type when
given a custom label or tag is provided by user irrespective of their sender
etc. The problem faced with SMS data is its incomplete structure and
grammatical inconsistencies. In this paper, we propose a unique pipeline for
evaluating the text similarity between SMS texts. We use Part of Speech (POS)
model for keyword extraction by taking advantage of the partial structure
embedded in SMS texts and similarity comparisons are carried out using
statistical methods. The proposed pipeline deals with major semantic variations
across SMS data as well as makes it effective for its application on-device
(mobile phone). To showcase the capabilities of our work, our pipeline has been
designed with an inclination towards one of the possible applications of SMS
text similarity discussed in one of the following sections but nonetheless
guarantees scalability for other applications as well.
","<1> Determining the sentence similarity between Short Message Service (SMS) </1>
 <2>  </2>
 <3> texts/sentences plays a significant role in mobile device industry. </3>
 <4> Gauging the similarity between SMS data is thus necessary for various applications like enhanced searching and navigation, clubbing together SMS of similar type when given a custom label or tag is provided by user irrespective of their sender etc. </4>
 <5> The problem faced with SMS data is its incomplete structure and grammatical inconsistencies. </5>
 <6> In this paper, we propose a unique pipeline for evaluating the text similarity between SMS texts. </6>
 <7> We use Part of Speech (POS) model for keyword extraction by taking advantage of the partial structure embedded in SMS texts and similarity comparisons are carried out using statistical methods. </7>
 <8> The proposed pipeline deals with major semantic variations across SMS data as well as makes it effective for its application on-device (mobile phone). </8>
 <9> To showcase the capabilities of our work, our pipeline has been designed with an inclination towards one of the possible applications of SMS text similarity discussed in one of the following sections but nonetheless guarantees scalability for other applications as well. </9>
"
1412.5448,"Extended Recommendation Framework: Generating the Text of a User Review
  as a Personalized Summary","  We propose to augment rating based recommender systems by providing the user
with additional information which might help him in his choice or in the
understanding of the recommendation. We consider here as a new task, the
generation of personalized reviews associated to items. We use an extractive
summary formulation for generating these reviews. We also show that the two
information sources, ratings and items could be used both for estimating
ratings and for generating summaries, leading to improved performance for each
system compared to the use of a single source. Besides these two contributions,
we show how a personalized polarity classifier can integrate the rating and
textual aspects. Overall, the proposed system offers the user three
personalized hints for a recommendation: rating, text and polarity. We evaluate
these three components on two datasets using appropriate measures for each
task.
","<1>  </1>
 <2> We propose to augment rating based recommender systems by providing the user with additional information which might help him in his choice or in the understanding of the recommendation. </2>
 <3> We consider here as a new task, the generation of personalized reviews associated to items. </3>
 <4> We use an extractive summary formulation for generating these reviews. </4>
 <5> We also show that the two information sources, ratings and items could be used both for estimating ratings and for generating summaries, leading to improved performance for each system compared to the use of a single source. </5>
 <6> Besides these two contributions, we show how a personalized polarity classifier can integrate the rating and textual aspects. </6>
 <7> Overall, the proposed system offers the user three personalized hints for a recommendation: rating, text and polarity. </7>
 <8> We evaluate these three components on two datasets using appropriate measures for each task. </8>
"
2112.08808,Simple Questions Generate Named Entity Recognition Datasets,"  Recent named entity recognition (NER) models often rely on human-annotated
datasets, requiring the significant engagement of professional knowledge on the
target domain and entities. This research introduces an ask-to-generate
approach that automatically generates NER datasets by asking questions in
simple natural language to an open-domain question answering system (e.g.,
""Which disease?""). Despite using fewer in-domain resources, our models, solely
trained on the generated datasets, largely outperform strong low-resource
models by an average F1 score of 19.4 for six popular NER benchmarks.
Furthermore, our models provide competitive performance with rich-resource
models that additionally leverage in-domain dictionaries provided by domain
experts. In few-shot NER, we outperform the previous best model by an F1 score
of 5.2 on three benchmarks and achieve new state-of-the-art performance.
","<1> Recent named entity recognition (NER) models often rely on human-annotated datasets, requiring the significant engagement of professional knowledge on the target domain and entities. </1>
 <2> This research introduces an ask-to-generate approach that automatically generates NER datasets by asking questions in simple natural language to an open-domain question answering system (e.g., ""Which disease?""). </2>
 <3> Despite using fewer in-domain resources, our models, solely trained on the generated datasets, largely outperform strong low-resource models by an average F1 score of 19.4 for six popular NER benchmarks. </3>
 <4>  </4>
 <5> Furthermore, our models provide competitive performance with rich-resource models that additionally leverage in-domain dictionaries provided by domain experts. </5>
 <6> In few-shot NER, we outperform the previous best model by an F1 score of 5.2 on three benchmarks and achieve new state-of-the-art performance. </6>
"
2012.06561,Comprehension and Knowledge,"  The ability of an agent to comprehend a sentence is tightly connected to the
agent's prior experiences and background knowledge. The paper suggests to
interpret comprehension as a modality and proposes a complete bimodal logical
system that describes an interplay between comprehension and knowledge
modalities.
","<1>  </1>
 <2> The ability of an agent to comprehend a sentence is tightly connected to the agent's prior experiences and background knowledge. </2>
 <3> The paper suggests to interpret comprehension as a modality and proposes a complete bimodal logical system that describes an interplay between comprehension and knowledge modalities. </3>
"
1812.01193,e-SNLI: Natural Language Inference with Natural Language Explanations,"  In order for machine learning to garner widespread public adoption, models
must be able to provide interpretable and robust explanations for their
decisions, as well as learn from human-provided explanations at train time. In
this work, we extend the Stanford Natural Language Inference dataset with an
additional layer of human-annotated natural language explanations of the
entailment relations. We further implement models that incorporate these
explanations into their training process and output them at test time. We show
how our corpus of explanations, which we call e-SNLI, can be used for various
goals, such as obtaining full sentence justifications of a model's decisions,
improving universal sentence representations and transferring to out-of-domain
NLI datasets. Our dataset thus opens up a range of research directions for
using natural language explanations, both for improving models and for
asserting their trust.
","<1>  </1>
 <2> In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from human-provided explanations at train time. </2>
 <3> In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations. </3>
 <4> We further implement models that incorporate these explanations into their training process and output them at test time. </4>
 <5> We show how our corpus of explanations, which we call e-SNLI, can be used for various goals, such as obtaining full sentence justifications of a model's decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets. </5>
 <6> Our dataset thus opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust. </6>
"
2212.00629,"Analyzing the State of Computer Science Research with the DBLP Discovery
  Dataset","  The number of scientific publications continues to rise exponentially,
especially in Computer Science (CS). However, current solutions to analyze
those publications restrict access behind a paywall, offer no features for
visual analysis, limit access to their data, only focus on niches or
sub-fields, and/or are not flexible and modular enough to be transferred to
other datasets. In this thesis, we conduct a scientometric analysis to uncover
the implicit patterns hidden in CS metadata and to determine the state of CS
research. Specifically, we investigate trends of the quantity, impact, and
topics for authors, venues, document types (conferences vs. journals), and
fields of study (compared to, e.g., medicine). To achieve this we introduce the
CS-Insights system, an interactive web application to analyze CS publications
with various dashboards, filters, and visualizations. The data underlying this
system is the DBLP Discovery Dataset (D3), which contains metadata from 5
million CS publications. Both D3 and CS-Insights are open-access, and
CS-Insights can be easily adapted to other datasets in the future. The most
interesting findings of our scientometric analysis include that i) there has
been a stark increase in publications, authors, and venues in the last two
decades, ii) many authors only recently joined the field, iii) the most cited
authors and venues focus on computer vision and pattern recognition, while the
most productive prefer engineering-related topics, iv) the preference of
researchers to publish in conferences over journals dwindles, v) on average,
journal articles receive twice as many citations compared to conference papers,
but the contrast is much smaller for the most cited conferences and journals,
and vi) journals also get more citations in all other investigated fields of
study, while only CS and engineering publish more in conferences than journals.
","<1>  </1>
 <2> The number of scientific publications continues to rise exponentially, especially in Computer Science (CS). </2>
 <3> However, current solutions to analyze </3>
 <4> those publications restrict access behind a paywall, offer no features for visual analysis, limit access to their data, only focus on niches or sub-fields, and/or are not flexible and modular enough to be transferred to other datasets. </4>
 <5> In this thesis, we conduct a scientometric analysis to uncover the implicit patterns hidden in CS metadata and to determine the state of CS research. </5>
 <6> Specifically, we investigate trends of the quantity, impact, and topics for authors, venues, document types (conferences vs. journals), and fields of study (compared to, e.g., medicine). </6>
 <7> To achieve this we introduce the CS-Insights system, an interactive web application to analyze CS publications with various dashboards, filters, and visualizations. </7>
 <8> The data underlying this system is the DBLP Discovery Dataset (D3), which contains metadata from 5 million CS publications. </8>
 <9> Both D3 and CS-Insights are open-access, and CS-Insights can be easily adapted to other datasets in the future. </9>
 <10> The most interesting findings of our scientometric analysis include that i) there has been a stark increase in publications, authors, and venues in the last two decades, ii) many authors only recently joined the field, iii) the most cited authors and venues focus on computer vision and pattern recognition, while the most productive prefer engineering-related topics, iv) the preference of researchers to publish in conferences over journals dwindles, v) on average, journal articles receive twice as many citations compared to conference papers, but the contrast is much smaller for the most cited conferences and journals, and vi) journals also get more citations in all other investigated fields of study, while only CS and engineering publish more in conferences than journals. </10>
"
2112.11389,Supervised Graph Contrastive Pretraining for Text Classification,"  Contrastive pretraining techniques for text classification has been largely
studied in an unsupervised setting. However, oftentimes labeled data from
related tasks which share label semantics with current task is available. We
hypothesize that using this labeled data effectively can lead to better
generalization on current task. In this paper, we propose a novel way to
effectively utilize labeled data from related tasks with a graph based
supervised contrastive learning approach. We formulate a token-graph by
extrapolating the supervised information from examples to tokens. Our
formulation results in an embedding space where tokens with high/low
probability of belonging to same class are near/further-away from one another.
We also develop detailed theoretical insights which serve as a motivation for
our method. In our experiments with $13$ datasets, we show our method
outperforms pretraining schemes by $2.5\%$ and also example-level contrastive
learning based formulation by $1.8\%$ on average. In addition, we show
cross-domain effectiveness of our method in a zero-shot setting by $3.91\%$ on
average. Lastly, we also demonstrate our method can be used as a noisy teacher
in a knowledge distillation setting to significantly improve performance of
transformer based models in low labeled data regime by $4.57\%$ on average.
","<1> Contrastive pretraining techniques for text classification has been largely studied in an unsupervised setting. </1>
 <2> However, oftentimes labeled data from related tasks which share label semantics with current task is available. </2>
 <3> We hypothesize that using this labeled data effectively can lead to better generalization on current task. </3>
 <4> In this paper, we propose a novel way to effectively utilize labeled data from related tasks with a graph based supervised contrastive learning approach. </4>
 <5> We formulate a token-graph by extrapolating the supervised information from examples to tokens. </5>
 <6> Our formulation results in an embedding space where tokens with high/low probability of belonging to same class are near/further-away from one another. </6>
 <7>  </7>
 <8> We also develop detailed theoretical insights which serve as a motivation for our method. </8>
 <9> In our experiments with $13$ datasets, we show our method outperforms pretraining schemes by $2.5\%$ and also example-level contrastive learning based formulation by $1.8\%$ on average. </9>
 <10> In addition, we show cross-domain effectiveness of our method in a zero-shot setting by $3.91\%$ on average. </10>
 <11> Lastly, we also demonstrate our method can be used as a noisy teacher in a knowledge distillation setting to significantly improve performance of transformer based models in low labeled data regime by $4.57\%$ on average. </11>
"
2012.08117,"Writing Polishment with Simile: Task, Dataset and A Neural Approach","  A simile is a figure of speech that directly makes a comparison, showing
similarities between two different things, e.g. ""Reading papers can be dull
sometimes,like watching grass grow"". Human writers often interpolate
appropriate similes into proper locations of the plain text to vivify their
writings. However, none of existing work has explored neural simile
interpolation, including both locating and generation. In this paper, we
propose a new task of Writing Polishment with Simile (WPS) to investigate
whether machines are able to polish texts with similes as we human do.
Accordingly, we design a two-staged Locate&Gen model based on transformer
architecture. Our model firstly locates where the simile interpolation should
happen, and then generates a location-specific simile. We also release a
large-scale Chinese Simile (CS) dataset containing 5 million similes with
context. The experimental results demonstrate the feasibility of WPS task and
shed light on the future research directions towards better automatic text
polishment.
","<1> A simile is a figure of speech that directly makes a comparison, showing similarities between two different things, e.g. ""Reading papers can be dull sometimes,like watching grass grow"". </1>
 <2> Human writers often interpolate appropriate similes into proper locations of the plain text to vivify their writings. </2>
 <3> However, none of existing work has explored neural simile interpolation, including both locating and generation. </3>
 <4> In this paper, we propose a new task of Writing Polishment with Simile (WPS) to investigate whether machines are able to polish texts with similes as we human do. </4>
 <5> Accordingly, we design a two-staged Locate&Gen model based on transformer architecture. </5>
 <6> Our model firstly locates where the simile interpolation should happen, and then generates a location-specific simile. </6>
 <7> We also release a large-scale Chinese Simile (CS) dataset containing 5 million similes with context. </7>
 <8> The experimental results demonstrate the feasibility of WPS task and shed light on the future research directions towards better automatic text polishment. </8>
"
2012.07410,"Reasoning in Dialog: Improving Response Generation by Context Reading
  Comprehension","  In multi-turn dialog, utterances do not always take the full form of
sentences \cite{Carbonell1983DiscoursePA}, which naturally makes understanding
the dialog context more difficult. However, it is essential to fully grasp the
dialog context to generate a reasonable response. Hence, in this paper, we
propose to improve the response generation performance by examining the model's
ability to answer a reading comprehension question, where the question is
focused on the omitted information in the dialog. Enlightened by the multi-task
learning scheme, we propose a joint framework that unifies these two tasks,
sharing the same encoder to extract the common and task-invariant features with
different decoders to learn task-specific features. To better fusing
information from the question and the dialog history in the encoding part, we
propose to augment the Transformer architecture with a memory updater, which is
designed to selectively store and update the history dialog information so as
to support downstream tasks. For the experiment, we employ human annotators to
write and examine a large-scale dialog reading comprehension dataset. Extensive
experiments are conducted on this dataset, and the results show that the
proposed model brings substantial improvements over several strong baselines on
both tasks. In this way, we demonstrate that reasoning can indeed help better
response generation and vice versa. We release our large-scale dataset for
further research.
","<1>  </1>
 <2> In multi-turn dialog, utterances do not always take the full form of sentences \cite{Carbonell1983DiscoursePA}, which naturally makes understanding the dialog context more difficult. </2>
 <3> However, it is essential to fully grasp the dialog context to generate a reasonable response. </3>
 <4> Hence, in this paper, we propose to improve the response generation performance by examining the model's ability to answer a reading comprehension question, where the question is focused on the omitted information in the dialog. </4>
 <5> Enlightened by the multi-task learning scheme, we propose a joint framework that unifies these two tasks, sharing the same encoder to extract the common and task-invariant features with different decoders to learn task-specific features. </5>
 <6> To better fusing information from the question and the dialog history in the encoding part, we propose to augment the Transformer architecture with a memory updater, which is designed to selectively store and update the history dialog information so as to support downstream tasks. </6>
 <7> For the experiment, we employ human annotators to write and examine a large-scale dialog reading comprehension dataset. </7>
 <8> Extensive experiments are conducted on this dataset, and the results show that the proposed model brings substantial improvements over several strong baselines on both tasks. </8>
 <9> In this way, we demonstrate that reasoning can indeed help better response generation and vice versa. </9>
 <10> We release our large-scale dataset for further research. </10>
"
2012.11988,"Graph-Evolving Meta-Learning for Low-Resource Medical Dialogue
  Generation","  Human doctors with well-structured medical knowledge can diagnose a disease
merely via a few conversations with patients about symptoms. In contrast,
existing knowledge-grounded dialogue systems often require a large number of
dialogue instances to learn as they fail to capture the correlations between
different diseases and neglect the diagnostic experience shared among them. To
address this issue, we propose a more natural and practical paradigm, i.e.,
low-resource medical dialogue generation, which can transfer the diagnostic
experience from source diseases to target ones with a handful of data for
adaptation. It is capitalized on a commonsense knowledge graph to characterize
the prior disease-symptom relations. Besides, we develop a Graph-Evolving
Meta-Learning (GEML) framework that learns to evolve the commonsense graph for
reasoning disease-symptom correlations in a new disease, which effectively
alleviates the needs of a large number of dialogues. More importantly, by
dynamically evolving disease-symptom graphs, GEML also well addresses the
real-world challenges that the disease-symptom correlations of each disease may
vary or evolve along with more diagnostic cases. Extensive experiment results
on the CMDD dataset and our newly-collected Chunyu dataset testify the
superiority of our approach over state-of-the-art approaches. Besides, our GEML
can generate an enriched dialogue-sensitive knowledge graph in an online
manner, which could benefit other tasks grounded on knowledge graph.
","<1> Human doctors with well-structured medical knowledge can diagnose a disease merely via a few conversations with patients about symptoms. </1>
 <2> In contrast, existing knowledge-grounded dialogue systems often require a large number of dialogue instances to learn as they fail to capture the correlations between different diseases and neglect the diagnostic experience shared among them. </2>
 <3> To address this issue, we propose a more natural and practical paradigm, i.e., low-resource medical dialogue generation, which can transfer the diagnostic experience from source diseases to target ones with a handful of data for adaptation. </3>
 <4> It is capitalized on a commonsense knowledge graph to characterize the prior disease-symptom relations. </4>
 <5> Besides, we develop a Graph-Evolving Meta-Learning (GEML) framework that learns to evolve the commonsense graph for reasoning disease-symptom correlations in a new disease, which effectively alleviates the needs of a large number of dialogues. </5>
 <6> More importantly, by dynamically evolving disease-symptom graphs, GEML also well addresses the real-world challenges that the disease-symptom correlations of each disease may vary or evolve along with more diagnostic cases. </6>
 <7> Extensive experiment results on the CMDD dataset and our newly-collected Chunyu dataset testify the superiority of our approach over state-of-the-art approaches. </7>
 <8> Besides, our GEML can generate an enriched dialogue-sensitive knowledge graph in an online manner, which could benefit other tasks grounded on knowledge graph. </8>
"
2112.08550,Neural Content Extraction for Poster Generation of Scientific Papers,"  The problem of poster generation for scientific papers is under-investigated.
Posters often present the most important information of papers, and the task
can be considered as a special form of document summarization. Previous studies
focus mainly on poster layout and panel composition, while neglecting the
importance of content extraction. Besides, their datasets are not publicly
available, which hinders further research. In this paper, we construct a
benchmark dataset from scratch for this task. Then we propose a three-step
framework to tackle this task and focus on the content extraction step in this
study. To get both textual and visual elements of a poster panel, a neural
extractive model is proposed to extract text, figures and tables of a paper
section simultaneously. We conduct experiments on the dataset and also perform
ablation study. Results demonstrate the efficacy of our proposed model. The
dataset and code will be released.
","<1>  </1>
 <2> The problem of poster generation for scientific papers is under-investigated. </2>
 <3>  </3>
 <4> Posters often present the most important information of papers, and the task can be considered as a special form of document summarization. </4>
 <5> Previous studies focus mainly on poster layout and panel composition, while neglecting the importance of content extraction. </5>
 <6> Besides, their datasets are not publicly available, which hinders further research. </6>
 <7> In this paper, we construct a benchmark dataset from scratch for this task. </7>
 <8> Then we propose a three-step framework to tackle this task and focus on the content extraction step in this study. </8>
 <9> To get both textual and visual elements of a poster panel, a neural extractive model is proposed to extract text, figures and tables of a paper section simultaneously. </9>
 <10> We conduct experiments on the dataset and also perform ablation study. </10>
 <11> Results demonstrate the efficacy of our proposed model. </11>
 <12> The dataset and code will be released. </12>
"
2212.06346,"The Massively Multilingual Natural Language Understanding 2022
  (MMNLU-22) Workshop and Competition","  Despite recent progress in Natural Language Understanding (NLU), the creation
of multilingual NLU systems remains a challenge. It is common to have NLU
systems limited to a subset of languages due to lack of available data. They
also often vary widely in performance. We launch a three-phase approach to
address the limitations in NLU and help propel NLU technology to new heights.
We release a 52 language dataset called the Multilingual Amazon SLU resource
package (SLURP) for Slot-filling, Intent classification, and Virtual assistant
Evaluation, or MASSIVE, in an effort to address parallel data availability for
voice assistants. We organize the Massively Multilingual NLU 2022 Challenge to
provide a competitive environment and push the state-of-the art in the
transferability of models into other languages. Finally, we host the first
Massively Multilingual NLU workshop which brings these components together. The
MMNLU workshop seeks to advance the science behind multilingual NLU by
providing a platform for the presentation of new research in the field and
connecting teams working on this research direction. This paper summarizes the
dataset, workshop and the competition and the findings of each phase.
","<1>  </1>
 <2> Despite recent progress in Natural Language Understanding (NLU), the creation of multilingual NLU systems remains a challenge. </2>
 <3> It is common to have NLU systems limited to a subset of languages due to lack of available data. </3>
 <4> They also often vary widely in performance. </4>
 <5> We launch a three-phase approach to address the limitations in NLU and help propel NLU technology to new heights. </5>
 <6>  </6>
 <7> We release a 52 language dataset called the Multilingual Amazon SLU resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation, or MASSIVE, in an effort to address parallel data availability for voice assistants. </7>
 <8> We organize the Massively Multilingual NLU 2022 Challenge to provide a competitive environment and push the state-of-the art in the transferability of models into other languages. </8>
 <9> Finally, we host the first </9>
 <10> Massively Multilingual NLU workshop which brings these components together. </10>
 <11> The MMNLU workshop seeks to advance the science behind multilingual NLU by providing a platform for the presentation of new research in the field and connecting teams working on this research direction. </11>
 <12> This paper summarizes the dataset, workshop and the competition and the findings of each phase. </12>
"
1612.04609,Neural Emoji Recommendation in Dialogue Systems,"  Emoji is an essential component in dialogues which has been broadly utilized
on almost all social platforms. It could express more delicate feelings beyond
plain texts and thus smooth the communications between users, making dialogue
systems more anthropomorphic and vivid. In this paper, we focus on
automatically recommending appropriate emojis given the contextual information
in multi-turn dialogue systems, where the challenges locate in understanding
the whole conversations. More specifically, we propose the hierarchical long
short-term memory model (H-LSTM) to construct dialogue representations,
followed by a softmax classifier for emoji classification. We evaluate our
models on the task of emoji classification in a real-world dataset, with some
further explorations on parameter sensitivity and case study. Experimental
results demonstrate that our method achieves the best performances on all
evaluation metrics. It indicates that our method could well capture the
contextual information and emotion flow in dialogues, which is significant for
emoji recommendation.
","<1>  </1>
 <2> Emoji is an essential component in dialogues which has been broadly utilized on almost all social platforms. </2>
 <3> It could express more delicate feelings beyond plain texts and thus smooth the communications between users, making dialogue systems more anthropomorphic and vivid. </3>
 <4> In this paper, we focus on automatically recommending appropriate emojis given the contextual information in multi-turn dialogue systems, where the challenges locate in understanding the whole conversations. </4>
 <5> More specifically, we propose the hierarchical long short-term memory model (H-LSTM) to construct dialogue representations, followed by a softmax classifier for emoji classification. </5>
 <6> We evaluate our models on the task of emoji classification in a real-world dataset, with some further explorations on parameter sensitivity and case study. </6>
 <7> Experimental results demonstrate that our method achieves the best performances on all evaluation metrics. </7>
 <8> It indicates that our method could well capture the contextual information and emotion flow in dialogues, which is significant for emoji recommendation. </8>
"
2112.11776,The Importance of the Current Input in Sequence Modeling,"  The last advances in sequence modeling are mainly based on deep learning
approaches. The current state of the art involves the use of variations of the
standard LSTM architecture, combined with several tricks that improve the final
prediction rates of the trained neural networks. However, in some cases, these
adaptations might be too much tuned to the particular problems being addressed.
In this article, we show that a very simple idea, to add a direct connection
between the input and the output, skipping the recurrent module, leads to an
increase of the prediction accuracy in sequence modeling problems related to
natural language processing. Experiments carried out on different problems show
that the addition of this kind of connection to a recurrent network always
improves the results, regardless of the architecture and training-specific
details. When this idea is introduced into the models that lead the field, the
resulting networks achieve a new state-of-the-art perplexity in language
modeling problems.
","<1> The last advances in sequence modeling are mainly based on deep learning approaches. </1>
 <2> The current state of the art involves the use of variations of the standard LSTM architecture, combined with several tricks that improve the final prediction rates of the trained neural networks. </2>
 <3> However, in some cases, these adaptations might be too much tuned to the particular problems being addressed. </3>
 <4>  </4>
 <5> In this article, we show that a very simple idea, to add a direct connection between the input and the output, skipping the recurrent module, leads to an increase of the prediction accuracy in sequence modeling problems related to natural language processing. </5>
 <6> Experiments carried out on different problems show that the addition of this kind of connection to a recurrent network always improves the results, regardless of the architecture and training-specific details. </6>
 <7> When this idea is introduced into the models that lead the field, the resulting networks achieve a new state-of-the-art perplexity in language modeling problems. </7>
"
2112.06776,Keyphrase Generation Beyond the Boundaries of Title and Abstract,"  Keyphrase generation aims at generating important phrases (keyphrases) that
best describe a given document. In scholarly domains, current approaches have
largely used only the title and abstract of the articles to generate
keyphrases. In this paper, we comprehensively explore whether the integration
of additional information from the full text of a given article or from
semantically similar articles can be helpful for a neural keyphrase generation
model or not. We discover that adding sentences from the full text,
particularly in the form of the extractive summary of the article can
significantly improve the generation of both types of keyphrases that are
either present or absent from the text. Experimental results with three widely
used models for keyphrase generation along with one of the latest transformer
models suitable for longer documents, Longformer Encoder-Decoder (LED) validate
the observation. We also present a new large-scale scholarly dataset FullTextKP
for keyphrase generation. Unlike prior large-scale datasets, FullTextKP
includes the full text of the articles along with the title and abstract. We
release the source code at https://github.com/kgarg8/FullTextKP.
","<1> Keyphrase generation aims at generating important phrases (keyphrases) that best describe a given document. </1>
 <2> In scholarly domains, current approaches have largely used only the title and abstract of the articles to generate keyphrases. </2>
 <3> In this paper, we comprehensively explore whether the integration of additional information from the full text of a given article or from semantically similar articles can be helpful for a neural keyphrase generation model or not. </3>
 <4> We discover that adding sentences from the full text, particularly in the form of the extractive summary of the article can significantly improve the generation of both types of keyphrases that are either present or absent from the text. </4>
 <5> Experimental results with three widely used models for keyphrase generation along with one of the latest transformer models suitable for longer documents, Longformer Encoder-Decoder (LED) validate the observation. </5>
 <6> We also present a new large-scale scholarly dataset FullTextKP for keyphrase generation. </6>
 <7> Unlike prior large-scale datasets, FullTextKP includes the full text of the articles along with the title and abstract. </7>
 <8> We release the source code at https://github.com/kgarg8/FullTextKP. </8>
"
2212.09284,"An Investigation of Indian Native Language Phonemic Influences on L2
  English Pronunciations","  Speech systems are sensitive to accent variations. This is especially
challenging in the Indian context, with an abundance of languages but a dearth
of linguistic studies characterising pronunciation variations. The growing
number of L2 English speakers in India reinforces the need to study accents and
L1-L2 interactions. We investigate the accents of Indian English (IE) speakers
and report in detail our observations, both specific and common to all regions.
In particular, we observe the phonemic variations and phonotactics occurring in
the speakers' native languages and apply this to their English pronunciations.
We demonstrate the influence of 18 Indian languages on IE by comparing the
native language pronunciations with IE pronunciations obtained jointly from
existing literature studies and phonetically annotated speech of 80 speakers.
Consequently, we are able to validate the intuitions of Indian language
influences on IE pronunciations by justifying pronunciation rules from the
perspective of Indian language phonology. We obtain a comprehensive description
in terms of universal and region-specific characteristics of IE, which
facilitates accent conversion and adaptation of existing ASR and TTS systems to
different Indian accents.
","<1> Speech systems are sensitive to accent variations. </1>
 <2> This is especially challenging in the Indian context, with an abundance of languages but a dearth of linguistic studies characterising pronunciation variations. </2>
 <3> The growing number of L2 English speakers in India reinforces the need to study accents and L1-L2 interactions. </3>
 <4> We investigate the accents of Indian English (IE) speakers and report in detail our observations, both specific and common to all regions. </4>
 <5>  </5>
 <6> In particular, we observe the phonemic variations and phonotactics occurring in the speakers' native languages and apply this to their English pronunciations. </6>
 <7>  </7>
 <8> We demonstrate the influence of 18 Indian languages on IE by comparing the native language pronunciations with IE pronunciations obtained jointly from existing literature studies and phonetically annotated speech of 80 speakers. </8>
 <9> Consequently, we are able to validate the intuitions of Indian language influences on IE pronunciations by justifying pronunciation rules from the perspective of Indian language phonology. </9>
 <10> We obtain a comprehensive description in terms of universal and region-specific characteristics of IE, which facilitates accent conversion and adaptation of existing ASR and TTS systems to different Indian accents. </10>
"
2112.07899,Large Dual Encoders Are Generalizable Retrievers,"  It has been shown that dual encoders trained on one domain often fail to
generalize to other domains for retrieval tasks. One widespread belief is that
the bottleneck layer of a dual encoder, where the final score is simply a
dot-product between a query vector and a passage vector, is too limited to make
dual encoders an effective retrieval model for out-of-domain generalization. In
this paper, we challenge this belief by scaling up the size of the dual encoder
model {\em while keeping the bottleneck embedding size fixed.} With multi-stage
training, surprisingly, scaling up the model size brings significant
improvement on a variety of retrieval tasks, especially for out-of-domain
generalization. Experimental results show that our dual encoders,
\textbf{G}eneralizable \textbf{T}5-based dense \textbf{R}etrievers (GTR),
outperform %ColBERT~\cite{khattab2020colbert} and existing sparse and dense
retrievers on the BEIR dataset~\cite{thakur2021beir} significantly. Most
surprisingly, our ablation study finds that GTR is very data efficient, as it
only needs 10\% of MS Marco supervised data to achieve the best out-of-domain
performance. All the GTR models are released at
https://tfhub.dev/google/collections/gtr/1.
","<1> It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. </1>
 <2> One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited to make dual encoders an effective retrieval model for out-of-domain generalization. </2>
 <3> In this paper, we challenge this belief by scaling up the size of the dual encoder model {\em while keeping the bottleneck embedding size fixed.} </3>
 <4> With multi-stage training, surprisingly, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. </4>
 <5> Experimental results show that our dual encoders, \textbf{G}eneralizable \textbf{T}5-based dense \textbf{R}etrievers (GTR), outperform %ColBERT~\cite{khattab2020colbert} and existing sparse and dense retrievers on the BEIR dataset~\cite{thakur2021beir} significantly. </5>
 <6> Most </6>
 <7> surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10\% of MS Marco supervised data to achieve the best out-of-domain performance. </7>
 <8> All the GTR models are released at https://tfhub.dev/google/collections/gtr/1. </8>
"
1912.01982,Neural Academic Paper Generation,"  In this work, we tackle the problem of structured text generation,
specifically academic paper generation in $\LaTeX{}$, inspired by the
surprisingly good results of basic character-level language models. Our
motivation is using more recent and advanced methods of language modeling on a
more complex dataset of $\LaTeX{}$ source files to generate realistic academic
papers. Our first contribution is preparing a dataset with $\LaTeX{}$ source
files on recent open-source computer vision papers. Our second contribution is
experimenting with recent methods of language modeling and text generation such
as Transformer and Transformer-XL to generate consistent $\LaTeX{}$ code. We
report cross-entropy and bits-per-character (BPC) results of the trained
models, and we also discuss interesting points on some examples of the
generated $\LaTeX{}$ code.
","<1>  </1>
 <2> In this work, we tackle the problem of structured text generation, specifically academic paper generation in $\LaTeX{}$, inspired by the surprisingly good results of basic character-level language models. </2>
 <3> Our motivation is using more recent and advanced methods of language modeling on a more complex dataset of $\LaTeX{}$ source files to generate realistic academic papers. </3>
 <4> Our first contribution is preparing a dataset with $\LaTeX{}$ source files on recent open-source computer vision papers. </4>
 <5> Our second contribution is experimenting with recent methods of language modeling and text generation such as Transformer and Transformer-XL to generate consistent $\LaTeX{}$ code. </5>
 <6> We report cross-entropy and bits-per-character (BPC) results of the trained models, and we also discuss interesting points on some examples of the generated $\LaTeX{}$ code. </6>
"
1612.06139,Neural Machine Translation from Simplified Translations,"  Text simplification aims at reducing the lexical, grammatical and structural
complexity of a text while keeping the same meaning. In the context of machine
translation, we introduce the idea of simplified translations in order to boost
the learning ability of deep neural translation models. We conduct preliminary
experiments showing that translation complexity is actually reduced in a
translation of a source bi-text compared to the target reference of the bi-text
while using a neural machine translation (NMT) system learned on the exact same
bi-text. Based on knowledge distillation idea, we then train an NMT system
using the simplified bi-text, and show that it outperforms the initial system
that was built over the reference data set. Performance is further boosted when
both reference and automatic translations are used to learn the network. We
perform an elementary analysis of the translated corpus and report accuracy
results of the proposed approach on English-to-French and English-to-German
translation tasks.
","<1> Text simplification aims at reducing the lexical, grammatical and structural complexity of a text while keeping the same meaning. </1>
 <2> In the context of machine translation, we introduce the idea of simplified translations in order to boost the learning ability of deep neural translation models. </2>
 <3> We conduct preliminary experiments showing that translation complexity is actually reduced in a translation of a source bi-text compared to the target reference of the bi-text while using a neural machine translation (NMT) system learned on the exact same bi-text. </3>
 <4> Based on knowledge distillation idea, we then train an NMT system using the simplified bi-text, and show that it outperforms the initial system that was built over the reference data set. </4>
 <5> Performance is further boosted when both reference and automatic translations are used to learn the network. </5>
 <6> We perform an elementary analysis of the translated corpus and report accuracy results of the proposed approach on English-to-French and English-to-German translation tasks. </6>
"
1812.02802,End-to-End Streaming Keyword Spotting,"  We present a system for keyword spotting that, except for a frontend
component for feature generation, it is entirely contained in a deep neural
network (DNN) model trained ""end-to-end"" to predict the presence of the keyword
in a stream of audio. The main contributions of this work are, first, an
efficient memoized neural network topology that aims at making better use of
the parameters and associated computations in the DNN by holding a memory of
previous activations distributed over the depth of the DNN. The second
contribution is a method to train the DNN, end-to-end, to produce the keyword
spotting score. This system significantly outperforms previous approaches both
in terms of quality of detection as well as size and computation.
","<1> We present a system for keyword spotting that, except for a frontend component for feature generation, it is entirely contained in a deep neural network (DNN) model trained ""end-to-end"" to predict the presence of the keyword in a stream of audio. </1>
 <2> The main contributions of this work are, first, an efficient memoized neural network topology that aims at making better use of the parameters and associated computations in the DNN by holding a memory of previous activations distributed over the depth of the DNN. </2>
 <3> The second contribution is a method to train the DNN, end-to-end, to produce the keyword spotting score. </3>
 <4> This system significantly outperforms previous approaches both in terms of quality of detection as well as size and computation. </4>
"
2212.14453,Learning Multimodal Data Augmentation in Feature Space,"  The ability to jointly learn from multiple modalities, such as text, audio,
and visual data, is a defining feature of intelligent systems. While there have
been promising advances in designing neural networks to harness multimodal
data, the enormous success of data augmentation currently remains limited to
single-modality tasks like image classification. Indeed, it is particularly
difficult to augment each modality while preserving the overall semantic
structure of the data; for example, a caption may no longer be a good
description of an image after standard augmentations have been applied, such as
translation. Moreover, it is challenging to specify reasonable transformations
that are not tailored to a particular modality. In this paper, we introduce
LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that
automatically learns to jointly augment multimodal data in feature space, with
no constraints on the identities of the modalities or the relationship between
modalities. We show that LeMDA can (1) profoundly improve the performance of
multimodal deep learning architectures, (2) apply to combinations of modalities
that have not been previously considered, and (3) achieve state-of-the-art
results on a wide range of applications comprised of image, text, and tabular
data.
","<1>  </1>
 <2> The ability to jointly learn from multiple modalities, such as text, audio, and visual data, is a defining feature of intelligent systems. </2>
 <3> While there have been promising advances in designing neural networks to harness multimodal data, the enormous success of data augmentation currently remains limited to single-modality tasks like image classification. </3>
 <4> Indeed, it is particularly difficult to augment each modality while preserving the overall semantic structure of the data; for example, a caption may no longer be a good description of an image after standard augmentations have been applied, such as translation. </4>
 <5> Moreover, it is challenging to specify reasonable transformations that are not tailored to a particular modality. </5>
 <6> In this paper, we introduce </6>
 <7> LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that automatically learns to jointly augment multimodal data in feature space, with no constraints on the identities of the modalities or the relationship between modalities. </7>
 <8> We show that LeMDA can (1) profoundly improve the performance of multimodal deep learning architectures, (2) apply to combinations of modalities that have not been previously considered, and (3) achieve state-of-the-art results on a wide range of applications comprised of image, text, and tabular data. </8>
"
