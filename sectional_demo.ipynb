{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json_into_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# save lm papers into train_lm_papers.json\n",
    "# lm_papers = []\n",
    "# llm_papers = []\n",
    "# file_index = [0, 1, 2, 3, 4, 5]\n",
    "# llm_arxiv_ids = ['1810.04805', '1907.11692', '1909.11942', ] # bert, roberta, albert\n",
    "# for i in file_index:\n",
    "#     arxiv_val = '../arxiv-dataset/train' + str(i) +  '.txt'\n",
    "\n",
    "#     with open(arxiv_val, \"r\") as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "\n",
    "#     for line in lines:\n",
    "#         paper = json.loads(line)\n",
    "#         if paper['article_id'] in llm_arxiv_ids:\n",
    "#             llm_papers.append(paper)\n",
    "\n",
    "#         if paper[\"labels\"] is not None:\n",
    "#             print(paper['labels'])\n",
    "#         if \"language model\" in ' '.join(paper['article_text']):\n",
    "#             lm_papers.append(paper)\n",
    "#     print(len(lm_papers))\n",
    "\n",
    "# with open('train_lm_papers.json', 'w') as output_file:\n",
    "#     for d in lm_papers:\n",
    "#         output_file.write(json.dumps(d)+'\\n')\n",
    "\n",
    "# load lm papers\n",
    "lm_papers = read_json_into_list('train_lm_papers.json')\n",
    "print(len(lm_papers))\n",
    "\n",
    "# lm_papers[0].keys() # 'article_id', 'article_text', 'abstract_text', 'labels', 'section_names', 'sections'\n",
    "# for i, paper in enumerate(lm_papers[:27]):\n",
    "#     print(i, paper['article_id'], '\\t', paper['section_names'], '\\n')\n",
    "\n",
    "# 3 ['introduction', 'related work', 'proposed approach', 'experimental results', 'conclusions and future work', 'acknowledgments']\n",
    "# Multi-Document Abstractive Summarization Using ILP based Multi-Sentence Compression > 100 citations\n",
    "\n",
    "# 13 ['introduction', 'background and related work', 'method', 'results', 'conclusion',]\n",
    "# Unsupervised Clustering of Commercial Domains for Adaptive Machine Translation\n",
    "\n",
    "# 11 ['introduction', 'related work', 'on the clustering algorithms', 'the hac algorithm', 'working setup', 'experimental results', 'summary and future work', 'acknowledgments']\n",
    "# Geotagging One Hundred Million Twitter Accounts with Total Variation Minimization\n",
    "\n",
    "# 18 ['introduction', 'related work', 'proposed approach', 'experiments', 'conclusions']\n",
    "# Meta-Prod2Vec - Product Embeddings Using Side-Information for Recommendation\n",
    "\n",
    "# 19 ['introduction', 'data analysis on sequential dependency', 'the proposed framework', 'experiments', 'conclusion and future work']\n",
    "# Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks\n",
    "\n",
    "# 20 ['introduction', 'related work', 'problem formulation', 'proposed models', 'experimental evaluation', 'conclusions']\n",
    "# Improving Tweet Representations using Temporal and User Contex\n",
    "\n",
    "# 21 ['introduction', 'measuring semantic regularities', 'models', 'experiments', 'conclusion']\n",
    "# Semantic Regularities in Document Representations\n",
    "\n",
    "# 23 ['introduction', 'notations and model', 'preliminary experiments', 'related work', 'conclusion and perspectives']\n",
    "# Reinforced Decision Trees  2 citations\n",
    "\n",
    "# Some puts related work into introduction\n",
    "# 24 ['introduction', 'the architecture of model', 'experiments', 'conclusion']\n",
    "# Generate Image Descriptions based on Deep RNN and Memory Cells for Images Features\n",
    "\n",
    "# 25 ['introduction and related work', 'model description', 'data', 'experiments', 'conclusion']\n",
    "# DEEP MULTIMODAL SEMANTIC EMBEDDINGS FOR SPEECH AND IMAGES\n",
    "\n",
    "# No separate section for proposed methods\n",
    "# 26 ['introduction', 'related work', 'experiments', 'conclusions']\n",
    "\n",
    "\n",
    "# for  paper_id in [3, 11, 13, 18, 19, 20, 21, 23, 24, 25]:\n",
    "#     print(paper_id, lm_papers[paper_id]['article_id'], '\\t', lm_papers[paper_id]['section_names'], '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sectional Summarization via Ctrl-Summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_each_section(paper):\n",
    "    # content = ' '.join(sne_paper['sections'][sne_paper['section_names'].index('experiments')])\n",
    "    # summary_by_allennlp( \"findings or experimental results => \", content )\n",
    "    summaries = {}\n",
    "    summaries['article_id'] = paper['article_id']\n",
    "    for name, content in zip(paper['section_names'][:-1], paper['sections'][:-1]):\n",
    "        print('Section name: ', name)\n",
    "        summaries[name] = summary_by_allennlp( \"\", ' '.join(content) )\n",
    "    \n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "summaries.append(summary_each_section(lm_papers[72])) # 72: SNE: Signed Network Embedding\n",
    "summaries.append(summary_each_section(lm_papers[3]) ) # 3: Multi-Document Abstractive Summarization Using ILP based Multi-Sentence Compression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'introduction': ' in this paper, we develop a signed network embedding model called sne, which uses the log - bilinear model, uses node representations of all nodes along a given path, and further incorporates two signed - type vectors to capture the positive or negative',\n",
       "  'preliminary': ' in this paper, we propose an unsigned network embedding model based on the skip - gram model. the proposed unsigned network embedding model outperforms the state - of - the - art unsigned network embedding models based on the skip - gram model in',\n",
       "  'sne: signed network embedding': ' in this paper, we present a network embedding model for signed directed graphs based on the log - bilinear model and the adagrad method. we show that our model can outperform the state - of - the - art unsigned network embed',\n",
       "  'experiments': ' in this paper, we propose a signed network embedding ( sne ) and evaluate its performance on two data mining tasks, node classification and link prediction. we show that our sne outperform the baseline sne on both datasets.',\n",
       "  'related work': ' in this paper, we explore the signed network embedding by considering the node attribute information and present a semi - supervised model to learn the network embedding. * keywords : * signed network, network embedding, graph mining, graph analysis',\n",
       "  'conclusion': ' in this paper, we present a new class of neural networks ( sne ) for signed network embedding. our sne adopts the log - bilinear model to combine the edge sign information and node representations of all nodes along a given path. experimental',\n",
       "  'article_id': '1703.04837'},\n",
       " {'article_id': '1609.07034',\n",
       "  'introduction': ' we propose a novel approach to abstractive summarization that jointly maximizes information content and readability.',\n",
       "  'related work': ' in this paper, we present an unsupervised method for abstractive summarization based on a natural - language - generation ( nlg ) system. we apply our method to news summarization tasks and show that our method outperforms other state -',\n",
       "  'proposed approach': ' in this paper, we present an abstractive summarization approach based on sentence clustering and a directed word - graph structure. we propose several techniques to identify the most important document in a document set, and then align sentences from other documents to the sentences',\n",
       "  'experimental results': ' in this paper, we propose a new approach for multi - document summarization and evaluate it on the duc 2004 and duc 2005 datasets.',\n",
       "  'conclusions and future work': ' we propose an approach to generate abstractive summaries from a document collection.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('ctrlsumm_summaries.json', 'w') as output_file:\n",
    "#     for d in summaries:\n",
    "#         output_file.write(json.dumps(d)+'\\n')\n",
    "\n",
    "read_json_into_list('ctrlsumm_summaries.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sectional Summarization via BigBird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.05k/1.05k [00:00<00:00, 800kB/s]\n",
      "Downloading: 100%|██████████| 1.92M/1.92M [00:01<00:00, 1.68MB/s]\n",
      "Downloading: 100%|██████████| 3.51M/3.51M [00:13<00:00, 262kB/s]\n",
      "Downloading: 100%|██████████| 775/775 [00:00<00:00, 262kB/s]\n",
      "Downloading: 100%|██████████| 1.19k/1.19k [00:00<00:00, 404kB/s]\n",
      "Downloading: 100%|██████████| 2.31G/2.31G [02:02<00:00, 18.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "\n",
    "# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64\n",
    "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\") # 2.31G\n",
    "\n",
    "# decoder attention type can't be changed & will be \"original_full\"\n",
    "# you can change `attention_type` (encoder only) to full attention like this:\n",
    "# model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", attention_type=\"original_full\")\n",
    "\n",
    "# you can change `block_size` & `num_random_blocks` like this:\n",
    "# model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", block_size=16, num_random_blocks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section name:  introduction\n",
      "Section name:  preliminary\n",
      "Section name:  sne: signed network embedding\n",
      "Section name:  experiments\n",
      "Section name:  related work\n",
      "Section name:  conclusion\n",
      "Section name:  introduction\n",
      "Section name:  related work\n",
      "Section name:  proposed approach\n",
      "Section name:  experimental results\n",
      "Section name:  conclusions and future work\n"
     ]
    }
   ],
   "source": [
    "def summary_each_section(paper):\n",
    "    summaries = {}\n",
    "    summaries['article_id'] = paper['article_id']\n",
    "    for name, content in zip(paper['section_names'][:-1], paper['sections'][:-1]):\n",
    "        print('Section name: ', name)\n",
    "        inputs = tokenizer(' '.join(content), return_tensors='pt')\n",
    "        prediction = model.generate(**inputs)\n",
    "        prediction = tokenizer.batch_decode(prediction)\n",
    "        summaries[name] = prediction[0]\n",
    "    return summaries\n",
    "\n",
    "summaries = []\n",
    "summaries.append(summary_each_section(lm_papers[72])) # 72: SNE: Signed Network Embedding\n",
    "summaries.append(summary_each_section(lm_papers[3]) ) \n",
    "\n",
    "\n",
    "# def summary_by_allennlp(context ):\n",
    "#     print(\"Keyword/Prompt \\n \\t \", prompt)\n",
    "#     vocab = Vocabulary.from_pretrained_transformer(\"hyunwoongko/ctrlsum-arxiv\") \n",
    "#     bart_model = Bart(model_name=\"hyunwoongko/ctrlsum-arxiv\", vocab=vocab)\n",
    "#     data = tokenizer([context+prompt], return_tensors=\"pt\", padding=\"max_length\", truncation=True )\n",
    "#     input_ids, attention_mask = data[\"input_ids\"], data[\"attention_mask\"]\n",
    "#     beam_search = BeamSearch(bart_model._end_id, vocab=vocab, beam_size=5)\n",
    "#     initial_decoder_id = torch.tensor( [[bart_model._decoder_start_id]], dtype=input_ids.dtype, device=input_ids.device ).repeat(input_ids.shape[0], 1)\n",
    "#     initial_state = {\n",
    "#         \"input_ids\": input_ids,\n",
    "#         \"input_mask\": attention_mask\n",
    "#     }\n",
    "\n",
    "#     beam_result = beam_search.search(initial_decoder_id, initial_state, bart_model.take_step)\n",
    "#     predictions = beam_result[0] # (bsz, beam_size, seq_len)\n",
    "\n",
    "#     max_pred_indices = (\n",
    "#         beam_result[1].argmax(dim=-1).view(-1, 1, 1).expand(-1, -1, predictions.shape[-1])\n",
    "#     ) # (bsz, 1, seq_len)\n",
    "#     out = predictions.gather(dim=1, index=max_pred_indices)# (bsz, 1, seq_len)\n",
    "#     out = out.squeeze(dim=1) \n",
    "\n",
    "#     predicted_text = bart_model.make_output_human_readable({\"predictions\": out})['predicted_text']\n",
    "\n",
    "#     print(\"Summary \\n \\t \", predicted_text[0])\n",
    "#     print('\\n')\n",
    "#     return predicted_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'article_id': '1703.04837',\n",
       "  'introduction': '<s> in this paper, we develop a signed network embedding model called sne. to the best of our knowledge, this is the first research on signed network embedding.<n> our sne model adopts the log - bilinear model, uses node representations of all nodes along a given path, and further incorporates two signed - type vectors to capture the positive or negative relationship of each edge along the path.<n> we conduct two experiments to evaluate our model, node classification and link prediction, on both an undirected signed network and a directed signed network built from real - world data.<n> we compare with four baselines including a matrix factorization method and three state - of - the - art network embedding models designed for unsigned networks.<n> the experimental results demonstrate the effectiveness of our signed network embedding.</s>',\n",
       "  'preliminary': '<s> neural language models such as the skip - gram model are widely used to train word embeddings in networks. in this paper<n>, we propose an extension of the skip - gram model to train unsigned network embeddings.<n> we show that the proposed model can achieve state - of - the - art results without assuming conditional independence.</s>',\n",
       "  'sne: signed network embedding': '<s> in this paper, we present a novel network embedding model for signed networks. for each node s embedding, we introduce the use of both source embedding and target embedding to capture the two potential roles of each node. our goal is to learn node embedding for each vertex in a signed network while capturing as much topological information as possible. we develop our signed network embedding by adapting the log - bilinear model such that the trained node embedding can capture node s path and sign information. recall that existing unsigned network embedding models are based on the skip - gram which only captures node s neighbour information and can not deal with the edge sign.<n> we adopt the log - bilinear model to predict the target node based on its predecessors along a path.<n> experiments on three real - world networks show that our model is competitive with state - of - the - art unsigned network embedding models. network embedding, signed network, signed directed graph, log - bilinear model, sne algorithm</s>',\n",
       "  'experiments': '<s> in this paper, we propose a new network embedding approach, signedlaplacian @xcite, based on eigenvectors of the signed laplacian matrix. in our approach, each node in the network is associated with a known class label and we use node embeddings to build classifiers and link predictions. we conduct our evaluation on two data mining tasks, node classification and link prediction.<n> we observe that our signedlaplacian based approach can achieve the best performance on both node classification and link prediction. <n> * keywords : * laplacian, signed graph, network embedding, node classification, link prediction. <n> laplacian @xcite has been widely used to model complex networks @xcite.<n> however, it has been shown that eigenvectors of the signed laplacian matrix can not be treated as node embeddings @xcite. in this paper, we propose a signed network embedding approach, @xmath0, based on eigenvectors of the signed laplacian matrix.<n> each node in the network is associated with a known class label and we use node embeddings to build classifiers and link',\n",
       "  'related work': '<s> in this paper, we present a signed network embedding algorithm for tasks such as spectral graph analysis, community detection, and link prediction.<n> the algorithm is based on a supervised learning method which learns the network embedding by using a stacked denoising auto - encoder.<n> experimental results show that the proposed model outperforms the current state - of - the - art network embedding methods. <n> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave.4 setgray fill grestore stroke grestore _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ',\n",
       "  'conclusion': '<s> in this paper, we present a new method for signed network embedding called signed node embedding ( sne ).<n> our sne adopts the log - bilinear model to combine the edge sign information and node representations of all nodes along a given path.<n> thus, the learned node embeddings capture the information of positive and negative links in signed networks.<n> experimental results on node classification and link prediction showed the effectiveness of sne.</s>'},\n",
       " {'article_id': '1609.07034',\n",
       "  'introduction': '<s> abstractive summarization is a powerful technique to generate novel text from text documents.<n> readability or linguistic quality is an important indicator of the quality of a summary. in this work, we address readability by assigning a log probability score from a language model as an indicator of linguistic quality. more specifically, we build a novel optimization model for summarization that jointly maximizes information content and readability. our proposed approach consists of the following two steps : ( 1 ) aligning similar sentences from multiple - documents and ( 2 ) generating the most informative and linguistically well - formed sentence from each cluster, and then appending them together. our first step estimates the importance of a document in the whole dataset using _ lexrank _ @xcite, _ pairwise cosine similarity _ and _ overall document collection similarity_. each sentence from the most important document are initialized into separate clusters. thereafter, each sentence from the other documents are assigned to the cluster that has the highest similarity with the sentence. in the generation step, we first generate a word - graph structure from the sentences in each cluster and construct @xmath0 shortest paths from the graph between the start and end nodes.<n>',\n",
       "  'related work': '<s> in this paper, we present a supervised abstractive summarizer for news topics. unlike previous summarization methods, our method generates informative, well - formed and readable abstractive summaries in an unsupervised manner by combining information from several sentences on the same topic.<n> our method selects sentences by jointly maximizing informativeness and readability and generates informative, well - formed and readable summaries.</s>',\n",
       "  'proposed approach': '<s> we propose a simple, yet effective abstractive summarization approach that first generates clusters of similar sentences, and then uses the individual clusters to create word - graphs.<n> a maximum of one novel sentence is generated from each word - graph with the goal of maximizing information content and linguistic quality of the entire summary. experiments with real - world data show that our approach outperforms state - of - the - art approaches.</s>',\n",
       "  'experimental results': '<s> we propose and evaluate a novel approach to multi - document summarization on the cpp duc 2004 and 2005 datasets on abstract summarizers.<n> we propose three document importance measures, two different sentence ordering techniques and six systems in total.<n> we show that our approach outperforms all other systems.<n> we find that the most informative document measure outperforms all other measures and ranked by majority ordering as the most informative document.<n> the clustering scheme that works best with msc @xcite is the central idea of our proposed abstract summarization.<n> the ilp framework that works best with msc @xcite is the central idea of our proposed abstract summarization.<n> we evaluate our approach on the duc 2004 and 2005 datasets on multi - document summarization.<n> we show that our approach outperforms all other systems on the duc 2004 and 2005 datasets on abstract summarizers.<n> we find that the most informative document measure and ranked by majority ordering as the most informative document outperforms all other measures and ranked by majority ordering as the most informative document.<n> the ilp framework that works best with msc @xcite is the central idea of our proposed abstract summarization.<n> the clustering scheme that works',\n",
       "  'conclusions and future work': '<s> in this paper, we propose an approach to generate abstractive summaries from document collections. we capture the redundant information using a simple yet effective clustering technique.<n> we also propose a novel ilp based technique to select the best shortest paths in a word - graph to maximize information content and linguistic quality of a summary.<n> experimental results on the duc 2004 and 2005 datasets show that our proposed approach outperforms all the baselines and the state - of - the - art extractive summarizers.</s>'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigbird_summaries.json', 'w') as output_file:\n",
    "    for d in summaries:\n",
    "        output_file.write(json.dumps(d)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (default, Aug 31 2020, 12:42:55) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa722aa51baa7ca1a14ae10c51947100c8742c9283df2adfc9442d206b591bbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
