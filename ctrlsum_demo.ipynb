{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statistics of article length: count      3.000000\n",
      "mean     133.333333\n",
      "std        8.504901\n",
      "min      125.000000\n",
      "25%      129.000000\n",
      "50%      133.000000\n",
      "75%      137.500000\n",
      "max      142.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from utils import read_json_into_list\n",
    "lm_papers = read_json_into_list('train_lm_papers.json')\n",
    "abstracts = {\n",
    "\"bert\":\n",
    " \"\"\" We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
    "\"\"\",\n",
    "\"roberta\":\n",
    " \"\"\"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.\n",
    "\"\"\",\n",
    " \"albert\":\n",
    " \"\"\"\n",
    " Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.\n",
    " F\"\"\"\n",
    "}\n",
    "\n",
    "abs_intro = {\n",
    " \"bert\":\n",
    " \"\"\" We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
    "Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. \n",
    "The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI\n",
    "GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n",
    "We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to-right architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). \n",
    "Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.\n",
    "In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.\n",
    "BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations. \n",
    "The contributions of our paper are as follows:\n",
    "We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.\n",
    "We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.\n",
    "BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/google-research/bert.\n",
    "\"\"\",\n",
    "\"roberta\":\n",
    " \"\"\"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.\n",
    " Self-training methods such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019), and XLNet (Yang et al., 2019) have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.\n",
    " We present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods. \n",
    " Our modifications are simple, they include: \n",
    " (1) training the model longer, with bigger batches, over more data; \n",
    " (2) removing the next sentence prediction objective; \n",
    " (3) training on longer sequences; and \n",
    " (4) dynamically changing the masking pattern applied to the training data. \n",
    " We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). \n",
    " Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS-B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT’s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Yang et al., 2019). \n",
    " In summary, the contributions of this paper are: \n",
    " (1) We present a set of important BERT design choices and training strategies and introduce alternatives that lead to better downstream task performance; \n",
    " (2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017)\"\"\",\n",
    " \"albert\":\n",
    " \"\"\"\n",
    " Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.\n",
    " Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations. \n",
    " Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models?\n",
    " An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model.\n",
    " Existing solutions to the aforementioned problems include model parallelization (Shazeer et al., 2018; Shoeybi et al., 2019) and clever memory management (Chen et al., 2016; Gomez et al., 2017). These solutions address the memory limitation problem, but not the communication overhead. \n",
    " In this paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT) architecture that has significantly fewer parameters than a traditional BERT architecture. ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models. \n",
    " The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. \n",
    " The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster.\n",
    " The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. \n",
    " To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT. \n",
    " As a result of these design decisions, we are able to scale up to much larger ALBERT configurations that still have fewer parameters than BERT-large but achieve significantly better performance. We establish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks for natural language understanding. Specifically, we push the RACE accuracy to 89.4%, the GLUE benchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2.\n",
    " \"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "from utils import print_statitiscs\n",
    "print_statitiscs(abstracts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statistics of article length: count      3.000000\n",
      "mean     650.666667\n",
      "std      111.634821\n",
      "min      527.000000\n",
      "25%      604.000000\n",
      "50%      681.000000\n",
      "75%      712.500000\n",
      "max      744.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_statitiscs(abs_intro.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CtrlSumm - keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 18:10:42.839105: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-27 18:10:43.051779: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-27 18:10:43.864064: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-27 18:10:43.864105: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-27 18:10:43.864109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, PreTrainedTokenizerFast\n",
    "from transformers.models.bart.modeling_bart import BartForConditionalGeneration\n",
    "import pandas as pd\n",
    "from allennlp_models.generation.models import Bart\n",
    "from allennlp.data import Vocabulary\n",
    "import torch\n",
    "from allennlp.nn.beam_search import BeamSearch\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"hyunwoongko/ctrlsum-arxiv\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"hyunwoongko/ctrlsum-arxiv\")\n",
    "\n",
    "def summary_by_hf(keyword, context, keyword_as_prompt=False ):\n",
    "    data = tokenizer([context+keyword], return_tensors=\"pt\", padding=\"max_length\", truncation=True )\n",
    "    input_ids, attention_mask = data[\"input_ids\"], data[\"attention_mask\"]\n",
    "\n",
    "    if keyword_as_prompt:\n",
    "        output = model.generate(input_ids, attention_mask=attention_mask, num_beams=5, decoder_input_ids=tokenizer(keyword, return_tensors=\"pt\")[\"input_ids\"][:, :-1])\n",
    "    else:\n",
    "        output = model.generate(input_ids, attention_mask=attention_mask, num_beams=5, )\n",
    "    return tokenizer.batch_decode(output)[0]\n",
    "\n",
    "# for allennlp\n",
    "vocab = Vocabulary.from_pretrained_transformer(\"hyunwoongko/ctrlsum-arxiv\")\n",
    "bart_model = Bart(model_name=\"hyunwoongko/ctrlsum-arxiv\", vocab=vocab)\n",
    "def summary_by_allennlp( keyword, context, keyword_as_prompt=False, print_info=False, keyword_first=True ):\n",
    "    \n",
    "    if keyword_first:\n",
    "        encoder_input = context+keyword\n",
    "    else:\n",
    "        encoder_input = keyword+context\n",
    "    data = tokenizer([encoder_input], return_tensors=\"pt\", padding=\"max_length\", truncation=True )\n",
    "    input_ids, attention_mask = data[\"input_ids\"], data[\"attention_mask\"]\n",
    "    \n",
    "    if keyword_as_prompt:\n",
    "        prompt_tokens = tokenizer.tokenize(keyword)\n",
    "        prompt_ids = tokenizer.convert_tokens_to_ids( prompt_tokens )\n",
    "        initial_decoder_id = torch.tensor( [[model.config.decoder_start_token_id] + prompt_ids], dtype=input_ids.dtype, device=input_ids.device ).repeat(input_ids.shape[0], 1)\n",
    "    else:\n",
    "        initial_decoder_id = torch.tensor( [[model.config.decoder_start_token_id]], dtype=input_ids.dtype, device=input_ids.device ).repeat(input_ids.shape[0], 1)\n",
    "    initial_state = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"input_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "    beam_search = BeamSearch(bart_model._end_id, vocab=vocab, beam_size=5)\n",
    "    beam_result = beam_search.search(initial_decoder_id, initial_state, bart_model.take_step)\n",
    "    predictions = beam_result[0] # (bsz, beam_size, seq_len)\n",
    "\n",
    "    max_pred_indices = (\n",
    "        beam_result[1].argmax(dim=-1).view(-1, 1, 1).expand(-1, -1, predictions.shape[-1])\n",
    "    ) # (bsz, 1, seq_len)\n",
    "    out = predictions.gather(dim=1, index=max_pred_indices)# (bsz, 1, seq_len)\n",
    "    out = out.squeeze(dim=1) \n",
    "\n",
    "    predicted_text = bart_model.make_output_human_readable({\"predictions\": out})['predicted_text']\n",
    "    if print_info:\n",
    "        print(\"Keyword/Prompt \\n \\t \", keyword)\n",
    "        print(\"Summary \\n \\t \", predicted_text[0])\n",
    "        print('\\n')\n",
    "    return predicted_text[0]\n",
    "\n",
    "    # import numpy as np\n",
    "    # print(max_pred_indices.numpy().shape)\n",
    "    # print(predictions.numpy().shape)\n",
    "    # np.take(predictions.numpy(), indices=max_pred_indices.numpy(), axis=1) # (2, 2, 1, 50, 50)\n",
    "\n",
    "def output_structured_summary(documents):\n",
    "    results = {}\n",
    "    \n",
    "    for paper_title, content in documents.items():\n",
    "        \n",
    "        method = summary_by_allennlp(\"Q:What is the proposed method or technique in the paper? A: => \", content)\n",
    "        empirical_result = summary_by_allennlp(\"findings or experimental results => \", content)\n",
    "        contributions = summary_by_allennlp(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=False )\n",
    "        results[paper_title] = [method, empirical_result, contributions]\n",
    "    \n",
    "    return pd.DataFrame.from_dict(results, orient='index',\n",
    "                           columns=['Method', 'Result', 'Contributions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword/Prompt \n",
      " \t  Q:What is the proposed method or technique in the paper? A: => \n",
      "Summary \n",
      " \t   in this paper, we introduce a new language representation model called the bidirectional language representation model called the bidirectional language representation model called the bidirectional language representation model called the bidirectional language representation model called the bidirectional language representation\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  findings or experimental results => \n",
      "Summary \n",
      " \t   in this paper, we introduce a new language representation model called the bidirectional language representation model called the bidirectional language representation model called the bidirectional language representation model called the bidirectional language representation model called the bidirectional language representation\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "Summary \n",
      " \t   in this paper, we introduce a new language representation model called the bidirectional language representation model called the bidirectional language representation model called the bidirectional language representation model called the bidirectional language representation model called the bidirectional language representation\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  Q:What is the proposed method or technique in the paper? A: => \n",
      "Summary \n",
      " \t   we present a new approach to training a language model that achieves state - of - the - art results on various benchmarks. our approach is based on training the language model with a set of training data, and then training the language model using the language model\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  findings or experimental results => \n",
      "Summary \n",
      " \t   we present the results of a systematic study of the impact of many key hyperparameters and training data size on the performance of our best model, which achieves state - of - the - art results on various benchmarks.\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "Summary \n",
      " \t   in this paper we present our best model, which achieves state - of - the - art results on the compass benchmark, and is significantly outperformed by all models published after it.\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  Q:What is the proposed method or technique in the paper? A: => \n",
      "Summary \n",
      " \t   in this paper, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of the well - known @xmath0-transformation of the english language. empirical evidence shows that our proposed methods lead to models that scale\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  findings or experimental results => \n",
      "Summary \n",
      " \t   in this paper, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of the well - known @xmath0-transformation of the english language. we also use a self -supervised loss that focuses on\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "Summary \n",
      " \t   in this paper, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of the well - known @xmath0-transformation of the english language. we also use a self -supervised loss that focuses on\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "structured_summary = output_structured_summary(abstracts)\n",
    "structured_summary.to_csv('abstract_summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword/Prompt \n",
      " \t  Q:What is the proposed method or technique in the paper? A: => \n",
      "Summary \n",
      " \t   in this paper, we introduce a new language representation model called bidirectional language models for pre- training that achieves state-of - the - art performance on a large suite of natural language processing tasks, including sentence-level tasks such as natural language\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  findings or experimental results => \n",
      "Summary \n",
      " \t   in this paper, we introduce a new language representation model called bidirectional language models for pre- training that achieves state-of - the - art performance on a large suite of natural language processing tasks, including sentence-level tasks such as natural language\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "Summary \n",
      " \t   in this paper, we introduce a new language representation model called bidirectional language models for pre- training that achieves state-of - the - art performance on a large suite of natural language processing tasks, including sentence-level tasks such as natural language\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  Q:What is the proposed method or technique in the paper? A: => \n",
      "Summary \n",
      " \t   in this paper, we present a set of important design choices and training strategies that lead to better downstream task performance. we also present a replication study of the pretraining methods published after our model, and propose an improved recipe for training the new version of\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  findings or experimental results => \n",
      "Summary \n",
      " \t   in this paper, we present a set of important design choices and training strategies that lead to better downstream task performance. our best model achieves state- of - the - art results on the liquidational equilibria test ( liquidation task ), the\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "Summary \n",
      " \t   in this paper, we present a set of important design choices and training strategies that lead to better downstream task performance. we also present a replication study of the success rate of pretraining the dialect of the english language ( dialect of the english language ) with\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  Q:What is the proposed method or technique in the paper? A: => \n",
      "Summary \n",
      " \t   in recent years, it has become common practice to pre - train large models and distill them down to smaller ones for real applications, such as learning natural language representations ( nlp ). it has become clear that a large network is crucial for achieving\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  findings or experimental results => \n",
      "Summary \n",
      " \t   in recent years, it has become common practice to pre - train large models and distill them down to smaller ones for real applications, such as learning natural language representations ( nlp ). it has become clear that a large network is crucial for achieving\n",
      "\n",
      "\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "Summary \n",
      " \t   in recent years, it has become common practice to pre - train large models and distill them down to smaller ones for real applications, such as learning natural language representations ( nlp ). it has become clear that a large network is crucial for achieving\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "structured_summary = output_structured_summary(abs_intro)\n",
    "structured_summary.to_csv('abs_intro_summaries.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does the models cannot generate faceted summarization according to keywords?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, keywords are put in front of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in this paper, we introduce a new language representation model, called, which is designed to pre - train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. with this pre - trained model\n",
      " in this paper, we introduce a new language representation model called urnhardened belief propagation ( urnhardened belief propagation ) that pre- trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in\n"
     ]
    }
   ],
   "source": [
    "print(summary_by_allennlp(\"state-of-the-art results => \", abstracts['bert'], keyword_first=True))\n",
    "print(summary_by_allennlp(\"state-of-the-art results => \", abs_intro['bert'], keyword_first=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_id': '1401.2943',\n",
       " 'article_text': ['being able to read news from other countries and written in other languages allows readers to be better informed .',\n",
       "  'it allows them to detect national news bias and thus improves transparency and democracy .',\n",
       "  'existing online translation systems such as _ google translate _ and _ _ bing translator _ _ are thus a great service , but the number of documents that can be submitted is restricted ( google will even entirely stop their service in 2012 ) and submitting documents means disclosing the users interests and their ( possibly sensitive ) data to the service - providing company .    for these reasons , we have developed our in - house machine translation system onts .',\n",
       "  'its translation results will be publicly accessible as part of the europe media monitor family of applications , @xcite , which gather and process about 100,000 news articles per day in about fifty languages .',\n",
       "  'onts is based on the open source phrase - based statistical machine translation toolkit moses  @xcite , trained mostly on freely available parallel corpora and optimised for the news domain , as stated above .',\n",
       "  \"the main objective of developing our in - house system is thus not to improve translation quality over the existing services ( this would be beyond our possibilities ) , but to offer our users a rough translation ( a `` gist '' ) that allows them to get an idea of the main contents of the article and to determine whether the news item at hand is relevant for their field of interest or not .\",\n",
       "  \"a similar news - focused translation service is `` found in translation ''  @xcite , which gathers articles in 23 languages and translates them into english .\",\n",
       "  \"`` found in translation '' is also based on moses , but it categorises the news after translation and the translation process is not optimised for the news domain .\",\n",
       "  'europe media monitor ( emm ) gathers a daily average of 100,000 news articles in approximately 50 languages , from about 3,400 hand - selected web news sources , from a couple of hundred specialist and government websites , as well as from about twenty commercial news providers .',\n",
       "  'it visits the news web sites up to every five minutes to search for the latest articles .',\n",
       "  'when news sites offer rss feeds , it makes use of these , otherwise it extracts the news text from the often complex html pages .',\n",
       "  'all news items are converted to unicode .',\n",
       "  'they are processed in a pipeline structure , where each module adds additional information .',\n",
       "  'independently of how files are written , the system uses utf-8-encoded rss format .    inside the pipeline , different algorithms',\n",
       "  'are implemented to produce monolingual and multilingual clusters and to extract various types of information such as named entities , quotations , categories and more .',\n",
       "  'onts uses two modules of emm : the named entity recognition and the categorization parts .      named entity recognition ( ner ) is performed using manually constructed language - independent rules that make use of language - specific lists of trigger words such as titles ( president ) , professions or occupations ( tennis player , playboy ) , references to countries , regions , ethnic or religious groups ( french , bavarian , berber , muslim ) , age expressions ( 57-year - old ) , verbal phrases ( deceased ) , modifiers ( former ) and more .',\n",
       "  'these patterns can also occur in combination and patterns can be nested to capture more complex titles , @xcite . in order to be able to cover many different languages , no other dictionaries and no parsers or part - of - speech taggers are used .    to identify which of the names newly found',\n",
       "  'every day are new entities and which ones are merely variant spellings of entities already contained in the database , we apply a language - independent name similarity measure to decide which name variants should be automatically merged , for details see @xcite .',\n",
       "  'this allows us to maintain a database containing over 1,15 million named entities and 200,000 variants .',\n",
       "  'the major part of this resource can be downloaded from http://langtech.jrc.it/jrc-names.html      all news items are categorized into hundreds of categories .',\n",
       "  'category definitions are multilingual , created by humans and they include geographic regions such as each country of the world , organizations , themes such as natural disasters or security , and more specific classes such as earthquake , terrorism or tuberculosis ,    articles fall into a given category if they satisfy the category definition , which consists of boolean operators with optional vicinity operators and wild cards .',\n",
       "  'alternatively , cumulative positive or negative weights and a threshold can be used .',\n",
       "  'uppercase letters in the category definition only match uppercase words , while lowercase words in the definition match both uppercase and lowercase words .',\n",
       "  'many categories are defined with input from the users themselves .',\n",
       "  'this method to categorize the articles is rather simple and user - friendly , and it lends itself to dealing with many languages , @xcite .',\n",
       "  'in this section , we describe our statistical machine translation ( smt ) service based on the open - source toolkit moses  @xcite and its adaptation to translation of news items .    * which is the most suitable smt system for our requirements ? * the main goal of our system is to help the user understand the content of an article .',\n",
       "  'this means that a translated article is evaluated positively even if it is not perfect in the target language .',\n",
       "  'dealing with such a large number of source languages and articles per day , our system should take into account the translation speed , and try to avoid using language - dependent tools such as part - of - speech taggers .    inside the moses toolkit',\n",
       "  ', three different statistical approaches have been implemented : _ phrase based statistical machine translation _ ( pbsmt )',\n",
       "  '@xcite , _ hierarchical phrase based statistical machine translation _  @xcite and _ syntax - based statistical machine translation _  @xcite . to identify the most suitable system for our requirements , we run a set of experiments training the three models with europarl v4 german - english  @xcite and optimizing and testing on the news corpus  @xcite . for all of them',\n",
       "  ', we use their default configurations and they are run under the same condition on the same machine to better evaluate translation time . for the syntax model we use linguistic information only on the target side . according to our experiments , in terms of performance the hierarchical model performs better than pbsmt and syntax ( 18.31 , 18.09 , 17.62 bleu points ) , but in terms of translation speed pbsmt is better than hierarchical and syntax ( 1.02 , 4.5 , 49 second per sentence ) .',\n",
       "  'although , the hierarchical model has the best bleu score , we prefer to use the pbsmt system in our translation service , because it is four times faster .    * which training data can we use ?',\n",
       "  '* it is known in statistical machine translation that more training data implies better translation .',\n",
       "  'although , the number of parallel corpora has been is growing in the last years , the amounts of training data vary from language pair to language pair .',\n",
       "  'to train our models we use the freely available corpora ( when possible ) : europarl  @xcite , jrc - acquis  @xcite , dgt - tm , opus  @xcite , se - times  @xcite , tehran english - persian parallel corpus  @xcite , news corpus  @xcite , un corpus  @xcite , czeng0.9  @xcite , english - persian parallel corpus distributed by elra and two arabic - english datasets distributed by ldc .',\n",
       "  'this results in some language pairs with a large coverage , ( more than 4 million sentences ) , and other with a very small coverage , ( less than 1 million ) .',\n",
       "  'the language models are trained using 12 model sentences for the content model and 4.7 million for the title model .',\n",
       "  'both sets are extracted from english news .    for less resourced languages such as farsi and turkish',\n",
       "  ', we tried to extend the available corpora . for farsi',\n",
       "  ', we applied the methodology proposed by  @xcite , where we used a large language model and an english - farsi smt model to produce new sentence pairs .',\n",
       "  'for turkish we added the movie subtitles corpus  @xcite , which allowed the smt system to increase its translation capability , but included several slang words and spoken phrases .    * how to deal with named entities in translation ?',\n",
       "  '* news articles are related to the most important events .',\n",
       "  'these names need to be efficiently translated to correctly understand the content of an article . from an smt point of view',\n",
       "  \", two main issues are related to named entity translation : ( 1 ) such a name is not in the training data or ( 2 ) part of the name is a common word in the target language and it is wrongly translated , e.g. the french name `` bruno le maire '' which risks to be translated into english as `` bruno mayor '' . to mitigate both the effects we use our multilingual named entity database . in the source language , each news item is analysed to identify possible entities ; if an entity is recognised , its correct translation into english is retrieved from the database , and suggested to the smt system enriching the source sentence using the xml markup option in moses .\",\n",
       "  'this approach allows us to complement the training data increasing the translation capability of our system .    * how to deal with different language styles in the news ?',\n",
       "  'news title writing style contains more gerund verbs , no or few linking verbs , prepositions and adverbs than normal sentences , while content sentences include more preposition , adverbs and different verbal tenses .',\n",
       "  'starting from this assumption , we investigated if this phenomenon can affect the translation performance of our system .',\n",
       "  'we trained two smt systems , @xmath0 and @xmath1 , using the europarl v4 german - english data as training corpus , and two different development sets : one made of content sentences , news commentaries  @xcite , and the other made of news titles in the source language which were translated into english using a commercial translation system . with the same strategy we generated also a title test set .',\n",
       "  'the @xmath1 used a language model created using only english news titles .',\n",
       "  'the news and title test sets were translated by both the systems . although the performance obtained translating the news and title corpora are not comparable , we were interested in analysing how the same test set is translated by the two systems .',\n",
       "  'we noticed that translating a test set with a system that was optimized with the same type of data resulted in almost 2 blue score improvements : title - testset : 0.3706 ( @xmath1 ) , 0.3511 ( @xmath0 ) ; news - testset : 0.1768 ( @xmath1 ) , 0.1945 ( @xmath0 ) .',\n",
       "  'this behaviour was present also in different language pairs . according to these results we decided to use two different translation systems for each language pair , one optimized using title data and the other using normal content sentences .',\n",
       "  'even though this implementation choice requires more computational power to run in memory two moses servers , it allows us to mitigate the workload of each single instance reducing translation time of each single article and to improve translation quality .',\n",
       "  'to evaluate the translation performance of onts , we run a set of experiments where we translate a test set for each language pair using our system and google translate .',\n",
       "  'lack of human translated parallel titles obliges us to test only the content based model .',\n",
       "  'for german , spanish and czech we use the news test sets proposed in @xcite , for french and italian the news test sets presented in @xcite , for arabic , farsi and turkish , sets of 2,000 news sentences extracted from the arabic - english and english - persian datasets and the se - times corpus . for the other languages we use 2,000 sentences which are not news but a mixture of jrc - acquis , europarl and dgt - tm data . it is not guarantee that our test sets are not part of the training data of google translate .',\n",
       "  'each test set is translated by google translate - translator toolkit , and by our system .',\n",
       "  'bleu score is used to evaluate the performance of both systems .',\n",
       "  'results , see table [ results ] , show that google translate produces better translation for those languages for which large amounts of data are available such as french , german , italian and spanish .',\n",
       "  'surprisingly , for danish , portuguese and polish , onts has better performance , this depends on the choice of the test sets which are not made of news data but of data that is fairly homogeneous in terms of style and genre with the training sets .',\n",
       "  'the impact of the named entity module is evident for arabic and farsi , where each english suggested entity results in a larger coverage of the source language and better translations .',\n",
       "  'for highly inflected and agglutinative languages such as turkish , the output proposed by onts is poor .',\n",
       "  'we are working on gathering more training data coming from the news domain and on the possibility of applying a linguistic pre - processing of the documents .    .[results ] automatic evaluation . [ cols=\"<,^,^ \" , ]',\n",
       "  'the translation service is made of two components : the connection module and the moses server .',\n",
       "  'the connection module is a servlet implemented in java .',\n",
       "  'it receives the rss files , isolates each single news article , identifies each source language and pre - processes it .',\n",
       "  'each news item is split into sentences , each sentence is tokenized , lowercased , passed through a statistical compound word splitter , @xcite , and the named entity annotator module . for language modelling',\n",
       "  'we use the kenlm implementation , @xcite .    according to the language , the correct moses servers , title and content ,',\n",
       "  'are fed in a multi - thread manner .',\n",
       "  'we use the multi - thread version of moses  @xcite . when all the sentences of each article are translated , the inverse process is run : they are detokenized , recased , and untranslated / unknown words are listed . the translated title and content of each article',\n",
       "  'are uploaded into the rss file and it is passed to the next modules .',\n",
       "  'the full system including the translation modules is running in a 2xquad - core with intel hyper - threading technology processors with 48 gb of memory .',\n",
       "  'it is our intention to locate the moses servers on different machines .',\n",
       "  'this is possible thanks to the high modularity and customization of the connection module . at the moment , the translation models are available for the following source languages : arabic , czech , danish , farsi , french , german , italian , polish , portuguese , spanish and turkish .      our translation service is currently presented on a demo web site , see figure [ fig::demo ] , which is available at http://optima.jrc.it / translate/. news articles can be retrieved selecting one of the topics and the language .',\n",
       "  'all the topics are assigned to each article using the methodology described in [ cat ] .',\n",
       "  'these articles are shown in the left column of the interface .',\n",
       "  \"when the button `` translate '' is pressed , the translation process starts and the translated articles appear in the right column of the page .\",\n",
       "  'the translation system can be customized from the interface enabling or disabling the named entity , compound , recaser , detokenizer and unknown word modules .',\n",
       "  'each translated article is enriched showing the translation time in milliseconds per character and , if enabled , the list of unknown words .',\n",
       "  'the interface is linked to the connection module and data is transferred using rss structure .',\n",
       "  'in this paper we present the optima news translation system and how it is connected to europe media monitor application .',\n",
       "  'different strategies are applied to increase the translation performance taking advantage of the document structure and other resources available in our research group .',\n",
       "  'we believe that the experiments described in this work can result very useful for the development of other similar systems .',\n",
       "  'translations produced by our system will soon be available as part of the main emm applications .',\n",
       "  'the performance of our system is encouraging , but not as good as the performance of web services such as google translate , mostly because we use less training data and we have reduced computational power . on the other hand , our in - house system can be fed with a large number of articles per day and sensitive data without including third parties in the translation process .',\n",
       "  'performance and translation time vary according to the number and complexity of sentences and language pairs .    the domain of news articles dynamically changes according to the main events in the world , while existing parallel data is static and usually associated to governmental domains .',\n",
       "  'it is our intention to investigate how to adapt our translation system updating the language model with the english articles of the day .',\n",
       "  'the authors thank the jrc s optima team for its support during the development of onts .          c. callison - burch , and p. koehn and c. monz and k. peterson and m. przybocki and o. zaidan . 2009 . .',\n",
       "  'proceedings of the joint fifth workshop on statistical machine translation and metricsmatr , pages 1753 .',\n",
       "  'uppsala , sweden .',\n",
       "  'p. koehn and f. j. och and d. marcu .',\n",
       "  'proceedings of the 2003 conference of the north american chapter of the association for computational linguistics on human language technology , pages 4854 .',\n",
       "  'edmonton , canada .',\n",
       "  'p. koehn and h. hoang and a. birch and c. callison - burch and m. federico and n. bertoldi and b. cowan and w. shen and c. moran and r. zens and c. dyer and o. bojar and a. constantin and e. herbst 2007 . .',\n",
       "  'proceedings of the annual meeting of the association for computational linguistics , demonstration session , pages 177180 .',\n",
       "  'columbus , oh , usa .                r. steinberger and b. pouliquen and a. widiger and c. ignat and t. erjavec and d. tufi and d. varga .',\n",
       "  'proceedings of the 5th international conference on language resources and evaluation , pages 21422147 .',\n",
       "  'genova , italy .',\n",
       "  'm. turchi and i. flaounas and o. ali and t. debie and t. snowsill and n. cristianini .',\n",
       "  'proceedings of the european conference on machine learning and knowledge discovery in databases , pages 746749 .',\n",
       "  'bled , slovenia .'],\n",
       " 'abstract_text': ['<S> we propose a real - time machine translation system that allows users to select a news category and to translate the related live news articles from arabic , czech , danish , farsi , french , german , italian , polish , portuguese , spanish and turkish into english . </S>',\n",
       "  '<S> the moses - based system was optimised for the news domain and differs from other available systems in four ways : ( 1 ) news items are automatically categorised on the source side , before translation ; ( 2 ) named entity translation is optimised by recognising and extracting them on the source side and by re - inserting their translation in the target language , making use of a separate entity repository ; ( 3 ) news titles are translated with a separate translation system which is optimised for the specific style of news titles ; ( 4 ) the system was optimised for speed in order to cope with the large volume of daily news articles . </S>'],\n",
       " 'labels': None,\n",
       " 'section_names': ['introduction',\n",
       "  'europe media monitor',\n",
       "  'news translation system',\n",
       "  'technical implementation',\n",
       "  'discussion',\n",
       "  'acknowledgments'],\n",
       " 'sections': [['being able to read news from other countries and written in other languages allows readers to be better informed .',\n",
       "   'it allows them to detect national news bias and thus improves transparency and democracy .',\n",
       "   'existing online translation systems such as _ google translate _ and _ _ bing translator _ _ are thus a great service , but the number of documents that can be submitted is restricted ( google will even entirely stop their service in 2012 ) and submitting documents means disclosing the users interests and their ( possibly sensitive ) data to the service - providing company .    for these reasons , we have developed our in - house machine translation system onts .',\n",
       "   'its translation results will be publicly accessible as part of the europe media monitor family of applications , @xcite , which gather and process about 100,000 news articles per day in about fifty languages .',\n",
       "   'onts is based on the open source phrase - based statistical machine translation toolkit moses  @xcite , trained mostly on freely available parallel corpora and optimised for the news domain , as stated above .',\n",
       "   \"the main objective of developing our in - house system is thus not to improve translation quality over the existing services ( this would be beyond our possibilities ) , but to offer our users a rough translation ( a `` gist '' ) that allows them to get an idea of the main contents of the article and to determine whether the news item at hand is relevant for their field of interest or not .\",\n",
       "   \"a similar news - focused translation service is `` found in translation ''  @xcite , which gathers articles in 23 languages and translates them into english .\",\n",
       "   \"`` found in translation '' is also based on moses , but it categorises the news after translation and the translation process is not optimised for the news domain .\"],\n",
       "  ['europe media monitor ( emm ) gathers a daily average of 100,000 news articles in approximately 50 languages , from about 3,400 hand - selected web news sources , from a couple of hundred specialist and government websites , as well as from about twenty commercial news providers .',\n",
       "   'it visits the news web sites up to every five minutes to search for the latest articles .',\n",
       "   'when news sites offer rss feeds , it makes use of these , otherwise it extracts the news text from the often complex html pages .',\n",
       "   'all news items are converted to unicode .',\n",
       "   'they are processed in a pipeline structure , where each module adds additional information .',\n",
       "   'independently of how files are written , the system uses utf-8-encoded rss format .    inside the pipeline , different algorithms',\n",
       "   'are implemented to produce monolingual and multilingual clusters and to extract various types of information such as named entities , quotations , categories and more .',\n",
       "   'onts uses two modules of emm : the named entity recognition and the categorization parts .      named entity recognition ( ner ) is performed using manually constructed language - independent rules that make use of language - specific lists of trigger words such as titles ( president ) , professions or occupations ( tennis player , playboy ) , references to countries , regions , ethnic or religious groups ( french , bavarian , berber , muslim ) , age expressions ( 57-year - old ) , verbal phrases ( deceased ) , modifiers ( former ) and more .',\n",
       "   'these patterns can also occur in combination and patterns can be nested to capture more complex titles , @xcite . in order to be able to cover many different languages , no other dictionaries and no parsers or part - of - speech taggers are used .    to identify which of the names newly found',\n",
       "   'every day are new entities and which ones are merely variant spellings of entities already contained in the database , we apply a language - independent name similarity measure to decide which name variants should be automatically merged , for details see @xcite .',\n",
       "   'this allows us to maintain a database containing over 1,15 million named entities and 200,000 variants .',\n",
       "   'the major part of this resource can be downloaded from http://langtech.jrc.it/jrc-names.html      all news items are categorized into hundreds of categories .',\n",
       "   'category definitions are multilingual , created by humans and they include geographic regions such as each country of the world , organizations , themes such as natural disasters or security , and more specific classes such as earthquake , terrorism or tuberculosis ,    articles fall into a given category if they satisfy the category definition , which consists of boolean operators with optional vicinity operators and wild cards .',\n",
       "   'alternatively , cumulative positive or negative weights and a threshold can be used .',\n",
       "   'uppercase letters in the category definition only match uppercase words , while lowercase words in the definition match both uppercase and lowercase words .',\n",
       "   'many categories are defined with input from the users themselves .',\n",
       "   'this method to categorize the articles is rather simple and user - friendly , and it lends itself to dealing with many languages , @xcite .'],\n",
       "  ['in this section , we describe our statistical machine translation ( smt ) service based on the open - source toolkit moses  @xcite and its adaptation to translation of news items .    * which is the most suitable smt system for our requirements ? * the main goal of our system is to help the user understand the content of an article .',\n",
       "   'this means that a translated article is evaluated positively even if it is not perfect in the target language .',\n",
       "   'dealing with such a large number of source languages and articles per day , our system should take into account the translation speed , and try to avoid using language - dependent tools such as part - of - speech taggers .    inside the moses toolkit',\n",
       "   ', three different statistical approaches have been implemented : _ phrase based statistical machine translation _ ( pbsmt )',\n",
       "   '@xcite , _ hierarchical phrase based statistical machine translation _  @xcite and _ syntax - based statistical machine translation _  @xcite . to identify the most suitable system for our requirements , we run a set of experiments training the three models with europarl v4 german - english  @xcite and optimizing and testing on the news corpus  @xcite . for all of them',\n",
       "   ', we use their default configurations and they are run under the same condition on the same machine to better evaluate translation time . for the syntax model we use linguistic information only on the target side . according to our experiments , in terms of performance the hierarchical model performs better than pbsmt and syntax ( 18.31 , 18.09 , 17.62 bleu points ) , but in terms of translation speed pbsmt is better than hierarchical and syntax ( 1.02 , 4.5 , 49 second per sentence ) .',\n",
       "   'although , the hierarchical model has the best bleu score , we prefer to use the pbsmt system in our translation service , because it is four times faster .    * which training data can we use ?',\n",
       "   '* it is known in statistical machine translation that more training data implies better translation .',\n",
       "   'although , the number of parallel corpora has been is growing in the last years , the amounts of training data vary from language pair to language pair .',\n",
       "   'to train our models we use the freely available corpora ( when possible ) : europarl  @xcite , jrc - acquis  @xcite , dgt - tm , opus  @xcite , se - times  @xcite , tehran english - persian parallel corpus  @xcite , news corpus  @xcite , un corpus  @xcite , czeng0.9  @xcite , english - persian parallel corpus distributed by elra and two arabic - english datasets distributed by ldc .',\n",
       "   'this results in some language pairs with a large coverage , ( more than 4 million sentences ) , and other with a very small coverage , ( less than 1 million ) .',\n",
       "   'the language models are trained using 12 model sentences for the content model and 4.7 million for the title model .',\n",
       "   'both sets are extracted from english news .    for less resourced languages such as farsi and turkish',\n",
       "   ', we tried to extend the available corpora . for farsi',\n",
       "   ', we applied the methodology proposed by  @xcite , where we used a large language model and an english - farsi smt model to produce new sentence pairs .',\n",
       "   'for turkish we added the movie subtitles corpus  @xcite , which allowed the smt system to increase its translation capability , but included several slang words and spoken phrases .    * how to deal with named entities in translation ?',\n",
       "   '* news articles are related to the most important events .',\n",
       "   'these names need to be efficiently translated to correctly understand the content of an article . from an smt point of view',\n",
       "   \", two main issues are related to named entity translation : ( 1 ) such a name is not in the training data or ( 2 ) part of the name is a common word in the target language and it is wrongly translated , e.g. the french name `` bruno le maire '' which risks to be translated into english as `` bruno mayor '' . to mitigate both the effects we use our multilingual named entity database . in the source language , each news item is analysed to identify possible entities ; if an entity is recognised , its correct translation into english is retrieved from the database , and suggested to the smt system enriching the source sentence using the xml markup option in moses .\",\n",
       "   'this approach allows us to complement the training data increasing the translation capability of our system .    * how to deal with different language styles in the news ?',\n",
       "   'news title writing style contains more gerund verbs , no or few linking verbs , prepositions and adverbs than normal sentences , while content sentences include more preposition , adverbs and different verbal tenses .',\n",
       "   'starting from this assumption , we investigated if this phenomenon can affect the translation performance of our system .',\n",
       "   'we trained two smt systems , @xmath0 and @xmath1 , using the europarl v4 german - english data as training corpus , and two different development sets : one made of content sentences , news commentaries  @xcite , and the other made of news titles in the source language which were translated into english using a commercial translation system . with the same strategy we generated also a title test set .',\n",
       "   'the @xmath1 used a language model created using only english news titles .',\n",
       "   'the news and title test sets were translated by both the systems . although the performance obtained translating the news and title corpora are not comparable , we were interested in analysing how the same test set is translated by the two systems .',\n",
       "   'we noticed that translating a test set with a system that was optimized with the same type of data resulted in almost 2 blue score improvements : title - testset : 0.3706 ( @xmath1 ) , 0.3511 ( @xmath0 ) ; news - testset : 0.1768 ( @xmath1 ) , 0.1945 ( @xmath0 ) .',\n",
       "   'this behaviour was present also in different language pairs . according to these results we decided to use two different translation systems for each language pair , one optimized using title data and the other using normal content sentences .',\n",
       "   'even though this implementation choice requires more computational power to run in memory two moses servers , it allows us to mitigate the workload of each single instance reducing translation time of each single article and to improve translation quality .',\n",
       "   'to evaluate the translation performance of onts , we run a set of experiments where we translate a test set for each language pair using our system and google translate .',\n",
       "   'lack of human translated parallel titles obliges us to test only the content based model .',\n",
       "   'for german , spanish and czech we use the news test sets proposed in @xcite , for french and italian the news test sets presented in @xcite , for arabic , farsi and turkish , sets of 2,000 news sentences extracted from the arabic - english and english - persian datasets and the se - times corpus . for the other languages we use 2,000 sentences which are not news but a mixture of jrc - acquis , europarl and dgt - tm data . it is not guarantee that our test sets are not part of the training data of google translate .',\n",
       "   'each test set is translated by google translate - translator toolkit , and by our system .',\n",
       "   'bleu score is used to evaluate the performance of both systems .',\n",
       "   'results , see table [ results ] , show that google translate produces better translation for those languages for which large amounts of data are available such as french , german , italian and spanish .',\n",
       "   'surprisingly , for danish , portuguese and polish , onts has better performance , this depends on the choice of the test sets which are not made of news data but of data that is fairly homogeneous in terms of style and genre with the training sets .',\n",
       "   'the impact of the named entity module is evident for arabic and farsi , where each english suggested entity results in a larger coverage of the source language and better translations .',\n",
       "   'for highly inflected and agglutinative languages such as turkish , the output proposed by onts is poor .',\n",
       "   'we are working on gathering more training data coming from the news domain and on the possibility of applying a linguistic pre - processing of the documents .    .[results ] automatic evaluation . [ cols=\"<,^,^ \" , ]'],\n",
       "  ['the translation service is made of two components : the connection module and the moses server .',\n",
       "   'the connection module is a servlet implemented in java .',\n",
       "   'it receives the rss files , isolates each single news article , identifies each source language and pre - processes it .',\n",
       "   'each news item is split into sentences , each sentence is tokenized , lowercased , passed through a statistical compound word splitter , @xcite , and the named entity annotator module . for language modelling',\n",
       "   'we use the kenlm implementation , @xcite .    according to the language , the correct moses servers , title and content ,',\n",
       "   'are fed in a multi - thread manner .',\n",
       "   'we use the multi - thread version of moses  @xcite . when all the sentences of each article are translated , the inverse process is run : they are detokenized , recased , and untranslated / unknown words are listed . the translated title and content of each article',\n",
       "   'are uploaded into the rss file and it is passed to the next modules .',\n",
       "   'the full system including the translation modules is running in a 2xquad - core with intel hyper - threading technology processors with 48 gb of memory .',\n",
       "   'it is our intention to locate the moses servers on different machines .',\n",
       "   'this is possible thanks to the high modularity and customization of the connection module . at the moment , the translation models are available for the following source languages : arabic , czech , danish , farsi , french , german , italian , polish , portuguese , spanish and turkish .      our translation service is currently presented on a demo web site , see figure [ fig::demo ] , which is available at http://optima.jrc.it / translate/. news articles can be retrieved selecting one of the topics and the language .',\n",
       "   'all the topics are assigned to each article using the methodology described in [ cat ] .',\n",
       "   'these articles are shown in the left column of the interface .',\n",
       "   \"when the button `` translate '' is pressed , the translation process starts and the translated articles appear in the right column of the page .\",\n",
       "   'the translation system can be customized from the interface enabling or disabling the named entity , compound , recaser , detokenizer and unknown word modules .',\n",
       "   'each translated article is enriched showing the translation time in milliseconds per character and , if enabled , the list of unknown words .',\n",
       "   'the interface is linked to the connection module and data is transferred using rss structure .'],\n",
       "  ['in this paper we present the optima news translation system and how it is connected to europe media monitor application .',\n",
       "   'different strategies are applied to increase the translation performance taking advantage of the document structure and other resources available in our research group .',\n",
       "   'we believe that the experiments described in this work can result very useful for the development of other similar systems .',\n",
       "   'translations produced by our system will soon be available as part of the main emm applications .',\n",
       "   'the performance of our system is encouraging , but not as good as the performance of web services such as google translate , mostly because we use less training data and we have reduced computational power . on the other hand , our in - house system can be fed with a large number of articles per day and sensitive data without including third parties in the translation process .',\n",
       "   'performance and translation time vary according to the number and complexity of sentences and language pairs .    the domain of news articles dynamically changes according to the main events in the world , while existing parallel data is static and usually associated to governmental domains .',\n",
       "   'it is our intention to investigate how to adapt our translation system updating the language model with the english articles of the day .'],\n",
       "  ['the authors thank the jrc s optima team for its support during the development of onts .          c. callison - burch , and p. koehn and c. monz and k. peterson and m. przybocki and o. zaidan . 2009 . .',\n",
       "   'proceedings of the joint fifth workshop on statistical machine translation and metricsmatr , pages 1753 .',\n",
       "   'uppsala , sweden .',\n",
       "   'p. koehn and f. j. och and d. marcu .',\n",
       "   'proceedings of the 2003 conference of the north american chapter of the association for computational linguistics on human language technology , pages 4854 .',\n",
       "   'edmonton , canada .',\n",
       "   'p. koehn and h. hoang and a. birch and c. callison - burch and m. federico and n. bertoldi and b. cowan and w. shen and c. moran and r. zens and c. dyer and o. bojar and a. constantin and e. herbst 2007 . .',\n",
       "   'proceedings of the annual meeting of the association for computational linguistics , demonstration session , pages 177180 .',\n",
       "   'columbus , oh , usa .                r. steinberger and b. pouliquen and a. widiger and c. ignat and t. erjavec and d. tufi and d. varga .',\n",
       "   'proceedings of the 5th international conference on language resources and evaluation , pages 21422147 .',\n",
       "   'genova , italy .',\n",
       "   'm. turchi and i. flaounas and o. ali and t. debie and t. snowsill and n. cristianini .',\n",
       "   'proceedings of the european conference on machine learning and knowledge discovery in databases , pages 746749 .',\n",
       "   'bled , slovenia .']]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ' '.join(lm_papers[0]['article_text'])\n",
    "lm_papers[0]['article_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for content in papers:\n",
    "    print('\\n\\n')\n",
    "    print(\"Using prompt as the prefix of decoder:\",)\n",
    "    print('\\t', summary_by_allennlp(\"state-of-the-art results => \", content, keyword_first=True))\n",
    "    print('\\t', summary_by_allennlp(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=True ))\n",
    "    print('\\n\\n')\n",
    "    print(\"Without prompt as the prefix of decoder:\", )\n",
    "    print('\\t', summary_by_hf(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=False ))\n",
    "    print('\\t', summary_by_allennlp(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=False ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why adding prompts as the prefixes of the decoder result in nonsense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for abstract\n",
    "for abstract in abstracts:\n",
    "    print('\\n\\n')\n",
    "    print(\"Using prompt as the prefix of decoder:\",)\n",
    "    print('\\t', summary_by_hf(\"the main contributions of this paper are: (1) => \", abstract, keyword_as_prompt=True ))\n",
    "    print('\\t', summary_by_allennlp(\"the main contributions of this paper are: (1) => \", abstract, keyword_as_prompt=True ))\n",
    "    print('\\n\\n')\n",
    "    print(\"Without prompt as the prefix of decoder:\", )\n",
    "    print('\\t', summary_by_hf(\"the main contributions of this paper are: (1) => \", abstract, keyword_as_prompt=False ))\n",
    "    print('\\t', summary_by_allennlp(\"the main contributions of this paper are: (1) => \", abstract, keyword_as_prompt=False ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Using prompt as the prefix of decoder:\n",
      "\t <s>the main contributions of this paper are: (1) => ia@xmath0</s>\n",
      "\t ivanov et al. @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @\n",
      "\n",
      "\n",
      "\n",
      "Without prompt as the prefix of decoder:\n",
      "\t </s> in this paper we study the behavior of the @xmath0-dependence of the</s>\n",
      "\t  in this paper we study the behavior of the @xmath0-dependence of the @xmath1-dependence of the @xmath2 and @xmath3-dependence of the @xmath4-dependence of the\n",
      "\n",
      "\n",
      "\n",
      "Using prompt as the prefix of decoder:\n",
      "\t <s>the main contributions of this paper are: (1) => ivanov et al.</s>\n",
      "\t ivanov et al. @xcite ; ( 2 ) ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @\n",
      "\n",
      "\n",
      "\n",
      "Without prompt as the prefix of decoder:\n",
      "\t </s> in this paper we study the behavior of the @xmath0-dependence of the</s>\n",
      "\t  in this paper we study the behavior of the @xmath0-dependence of the @xmath1-dependence of the @xmath2-dependence of the @xmath3-dependence of the @xmath4-\n",
      "\n",
      "\n",
      "\n",
      "Using prompt as the prefix of decoder:\n",
      "\t <s>the main contributions of this paper are: (1) => ivanov - arn</s>\n",
      "\t ivanov et al. @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @xcite, @\n",
      "\n",
      "\n",
      "\n",
      "Without prompt as the prefix of decoder:\n",
      "\t </s> in this paper, we study the effect of an external magnetic field @xmath0 on</s>\n",
      "\t  in this paper, we study the effect of an external magnetic field @xmath0 on the magnetization of a two - dimensional electron gas. we show that the magnetization of a two - dimensional electron gas depends on the magnetic field @xmath\n"
     ]
    }
   ],
   "source": [
    "# for abstract+intro\n",
    "for  content in abs_intro:\n",
    "    print('\\n\\n')\n",
    "    print(\"Using prompt as the prefix of decoder:\",)\n",
    "    print('\\t', summary_by_hf(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=True ))\n",
    "    print('\\t', summary_by_allennlp(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=True ))\n",
    "    print('\\n\\n')\n",
    "    print(\"Without prompt as the prefix of decoder:\", )\n",
    "    print('\\t', summary_by_hf(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=False ))\n",
    "    print('\\t', summary_by_allennlp(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=False ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Using prompt as the prefix of decoder:\n",
      "\t <s>the main contributions of this paper are: (1) => ------------ this paper presents a</s>\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "\t iau symposium on the design and optimization of touchscreen keyboards ; ( 2 ) ; ( 3 ) ; ( 4 ) ; ( 5 ) ; ( 6 ).\n",
      "\n",
      "\n",
      "\n",
      "Without prompt as the prefix of decoder:\n",
      "\t </s> in this paper we present a new method for evaluating and comparing the performance of various keyboard layouts</s>\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "\t  in this paper we present a new method for evaluating and comparing the performance of various keyboard layouts that have been optimized for gesture typing. the method is based on the use of composition tasks to complement the standard transcription tasks used to evaluate the performance of the keyboards\n",
      "\n",
      "\n",
      "\n",
      "Using prompt as the prefix of decoder:\n",
      "\t <s>the main contributions of this paper are: (1) => entscheidungs</s>\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "\t entscheidung on the state - of - art of machine learning(ml ) in security ; ( 2 ) : survey of cutting - edge research on applied ml in security ; ( 3 ) : recommendation of a taxonomy of ml paradigms\n",
      "\n",
      "\n",
      "\n",
      "Without prompt as the prefix of decoder:\n",
      "\t </s> we survey the state - of - art of machine learning(ml ) in security, and</s>\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "\t  we survey the state - of - art of machine learning(ml ) in security, and provide a high - level overview taxonomy of ml paradigms and security domains. we also point to research challenges that will improve, enhance, and expand our\n",
      "\n",
      "\n",
      "\n",
      "Using prompt as the prefix of decoder:\n",
      "\t <s>the main contributions of this paper are: (1) => ------------ abstractive summarization</s>\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "\t ive developed a novel integer linear programming ( ilp ) based approach that jointly maximizes information content and readability for abstractive summarization ; ( 2 ) the proposed ilp based approach prevents redundant information from being included in the summary ; ( 3 )\n",
      "\n",
      "\n",
      "\n",
      "Without prompt as the prefix of decoder:\n",
      "\t </s> abstractive summarization has gained popularity due to its ability of generating concise abstractive summaries</s>\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "\t  in this work, we propose a novel approach to abstractive summarization that jointly maximizes information content and readability.\n",
      "\n",
      "\n",
      "\n",
      "Using prompt as the prefix of decoder:\n",
      "\t <s>the main contributions of this paper are: (1) => iau symposium on</s>\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "\t    we describe a mapreduce implementation of a sequential search system for experimental information retrieval. ( 2 ) : we show that sequential scanning is a viable experimental tool, even if only a few machines are available.\n",
      "\n",
      "\n",
      "\n",
      "Without prompt as the prefix of decoder:\n",
      "\t </s> we describe a mapreduce implementation of a sequential search system for experimental information retrieval. we</s>\n",
      "Keyword/Prompt \n",
      " \t  the main contributions of this paper are: (1) => \n",
      "\t  we describe a mapreduce implementation of a sequential search system for experimental information retrieval.\n"
     ]
    }
   ],
   "source": [
    "papers = [' '.join(lm_papers[i]['article_text']) for i in range(1, 5)]\n",
    "for content in papers:\n",
    "    print('\\n\\n')\n",
    "    print(\"Using prompt as the prefix of decoder:\",)\n",
    "    print('\\t', summary_by_hf(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=True ))\n",
    "    print('\\t', summary_by_allennlp(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=True ))\n",
    "    print('\\n\\n')\n",
    "    print(\"Without prompt as the prefix of decoder:\", )\n",
    "    print('\\t', summary_by_hf(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=False ))\n",
    "    print('\\t', summary_by_allennlp(\"the main contributions of this paper are: (1) => \", content, keyword_as_prompt=False ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa722aa51baa7ca1a14ae10c51947100c8742c9283df2adfc9442d206b591bbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
