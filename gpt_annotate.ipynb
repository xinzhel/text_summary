{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_facet_results(facet_results, file_name):\n",
    "    # check whether file exists\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"File already exists. Overwrite? (y/n)\")\n",
    "        if input() != 'y':\n",
    "            return\n",
    "    with open(file_name, 'w') as output_file:\n",
    "        for d in facet_results:\n",
    "            output_file.write(json.dumps(d)+'\\n')\n",
    "\n",
    "def read_facet_results(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    facet_results = []\n",
    "    for line in lines:\n",
    "        facet_results.append(json.loads(line))\n",
    "    return facet_results\n",
    "\n",
    "def annotate_abstracts_by_ssc(abstracts, max_examples=30):\n",
    "    print(\"Annotating abstracts by single sentence classification...\")\n",
    "    print('Total number of examples: ', len(abstracts))\n",
    "\n",
    "    prefix_prompt = \"\"\"\n",
    "    You are a classifier to label the text between the tag <S> and </S> into one of the four classes: background, methods, results, values. If you are uncertain, output \"others\".  You can only output one word from labels or \"others\".\n",
    "    \"\"\"\n",
    "\n",
    "    facet_results = []\n",
    "\n",
    "    for id, sentences  in list(abstracts.items())[:max_examples]: \n",
    "        \n",
    "        for sent in sentences:\n",
    "            if \"* keywords\" in sent: \n",
    "                continue    # ignore keywords\n",
    "            # message = prefix_prompt + sent\n",
    "            output = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prefix_prompt},\n",
    "                    {\"role\": \"user\", \"content\": sent}\n",
    "                ]\n",
    "            )\n",
    "            facet_label = output['choices'][0]['message']['content']\n",
    "            facet_results.append({\n",
    "                \"article_id\": id, \n",
    "                \"sent\": sent, \n",
    "                'gpt-annotation': facet_label, \n",
    "                \"prompt_tokens\": output.get(\"usage\").get('prompt_tokens'),\n",
    "                \"completion_tokens\": output.get(\"usage\").get('completion_tokens'),\n",
    "                \"total_tokens\": output.get(\"usage\").get('total_tokens')}\n",
    "            )\n",
    "    return facet_results\n",
    "\n",
    "\n",
    "def annotate_abstracts_by_tagging(abstracts):\n",
    "    prompt = \"\"\"\n",
    "        Task: You are a tagger to label each subtext between the tags <NUM> and </NUM> with one of four labels, where NUM is a placeholder for the numeric index of a sentence:\n",
    "\n",
    "        The labels are:\n",
    "        1. Background: This sentence provides context and motivation for the research by discussing previous work and highlighting the need for further investigation or a statement of the research problem.\n",
    "        2. Method: The setence outlines the approach used to carry out the research, including the experimental design, data collection and analysis methods, and any algorithms or models used.\n",
    "        3. Result: The sentence presents the findings of the research, quantitative or qualitative analysis or the main outcomes.\n",
    "        4. Value: The sentence discusses the broader impact and societal implications of the paper.\n",
    "        5. Others: If you are uncertain which class the sentence belongs to, label it as \"others\". \n",
    "\n",
    "        Output includes NUM-label pairs separated by a semicolon and give only one label to each NUM, e.g., \"1 Others; 2 Method\".\n",
    "\n",
    "        Here is the input for tagging:\n",
    "\n",
    "    \"\"\"\n",
    "    # Example Input: \"<0> matrix data sets are common nowadays like in biomedical imaging </0> <1> we design a fast and easy - to - implement iterative algorithm to approximate arbitrarily finely these extremal matrices</1>\"\n",
    "    # Example Output: \"1 Others; 2 Method\"\n",
    "\n",
    "    facet_results = []\n",
    "    total_tokens = 0\n",
    "    for id, sentences  in list(abstracts.items())[:14]: \n",
    "        # remove <S> and </S> tags\n",
    "        plain_sentences = [sent.replace('<S>', '').replace('</S>', '') for sent in sentences if \"* keywords\" not in sent]\n",
    "\n",
    "        # add <NUM> and </NUM> tags for each sentence where NUM is 1, 2, 3, ...\n",
    "        tagged_sentences = ' '.join([f\" <{i}> {sent.strip()} </{i}> \" for i, sent in enumerate(plain_sentences) ] )\n",
    "        \n",
    "        \n",
    "        specific_prompt = prompt + tagged_sentences\n",
    "    \n",
    "        output = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": specific_prompt}\n",
    "            ]\n",
    "        )\n",
    "        gpt_annotation = output['choices'][0]['message']['content']\n",
    "\n",
    "        facet_results.append({\n",
    "            \"article_id\": id, \n",
    "            \"sent\": plain_sentences, \n",
    "            'gpt_annotation': gpt_annotation, \n",
    "            \"prompt_tokens\": output.get(\"usage\").get('prompt_tokens'),\n",
    "            \"completion_tokens\": output.get(\"usage\").get('completion_tokens'),\n",
    "            \"total_tokens\": output.get(\"usage\").get('total_tokens')}\n",
    "        )\n",
    "        total_tokens += output.get(\"usage\").get('total_tokens')\n",
    "    print(\"Total tokens used: \", total_tokens)\n",
    "    return facet_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_abstracts_from_arxiv(category='cs'):\n",
    "    # load all domains of papers\n",
    "    i = 5\n",
    "    inp_path = '../arxiv-dataset/train' + str(i) +  '.txt'\n",
    "    only_cat = []\n",
    "    with open(inp_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    papers = []\n",
    "    for line in lines:\n",
    "        papers.append(json.loads(line))\n",
    "\n",
    "    # aggregate abstracts for papers in the category\n",
    "    with open('ids_for_cats.pickle', 'rb') as handle:\n",
    "        ids_for_cats2 = pickle.load(handle)\n",
    "\n",
    "    abstracts = {}\n",
    "    for i in ids_for_cats2[category]: \n",
    "        abstracts[papers[i]['article_id']] = papers[i]['abstract_text']\n",
    "    return abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Background', 'Method', 'Result']\n",
      "['Others', 'Background', 'Background', 'Result', 'Method', 'Result', 'Others']\n",
      "['Background', 'Background', 'Background', 'Method', 'Method', 'Result', 'Result']\n",
      "['Background', 'Method', 'Method', 'Result', 'Result', 'Result', 'Value']\n",
      "['Background', 'Background', 'Result', 'Method', 'Method', 'Method', 'Result']\n",
      "['Background', 'Method', 'Method', 'Method', 'Background', 'Result', 'Result', 'Result']\n",
      "['Others', 'Result']\n",
      "['Background', 'Background', 'Method']\n",
      "['Background', 'Result', 'Result', 'Method']\n",
      "['Background', 'Background', 'Method', 'Result', 'Result', 'Result', 'Result', 'Value']\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# load abstract\n",
    "abstracts = load_abstracts_from_arxiv(category='cs')\n",
    "print(f\"The number of abstracts in the category: {len(abstracts)}\")\n",
    "\n",
    "# annotate abstracts by tagging\n",
    "facet_results = annotate_abstracts_by_tagging(abstracts)\n",
    "\n",
    "\n",
    "# post processing: extract the facets from the GPT textual outputs\n",
    "for result in facet_results:\n",
    "    # remove </0>, </1>, ...\n",
    "    result['gpt_annotation'] = re.sub(r'</\\d>', '', result['gpt_annotation']).replace('<', '').replace('>', '').replace(' ', '').replace('.', '')\n",
    "\n",
    "    # split facets by ;\n",
    "    facets = result['gpt_annotation'].split(';')\n",
    "\n",
    "    # remove empty string in facets\n",
    "    facets = [facet for facet in facets if facet != '']\n",
    "\n",
    "    new_facets = []\n",
    "    # print(facets)\n",
    "    for i, facet in enumerate(facets):\n",
    "        assert str(i) in facet\n",
    "        new_facets.append(facet.replace(str(i), '').strip())\n",
    "    try:\n",
    "        assert len(new_facets) == len(result['sent'])\n",
    "    except:\n",
    "        print(len(new_facets), len(result['sent']))\n",
    "        print(result['sent'])\n",
    "        print(new_facets)\n",
    "    print(new_facets)\n",
    "        \n",
    "    result['facets'] = new_facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faceted_results = read_facet_results(\"cs5_abstract-tag.json\")\n",
    "\n",
    "read_facet_results\n",
    "\n",
    "# this is used temporarily for old versions of the facet results\n",
    "for result in faceted_results:\n",
    "    result['sent'] = [sent for sent in result['sent'] if \"* keywords\" not in sent]\n",
    "    facets = result['gpt_annotation'].replace('.', '').split(';')\n",
    "    new_facets = []\n",
    "    for i, facet in enumerate(facets):\n",
    "        assert str(i) in facet\n",
    "        new_facets.append(facet.replace(str(i), '').strip())\n",
    "    try:\n",
    "        assert len(new_facets) == len(result['sent'])\n",
    "    except:\n",
    "        print(len(new_facets), len(result['sent']))\n",
    "        print(result['sent'])\n",
    "        print(new_facets)\n",
    "        \n",
    "    result['facets'] = new_facets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load abstracts\n",
    "file_dir = \"annotation/human_annotation\"\n",
    "with open(f'{file_dir}test_abstracts.json', 'r') as f:\n",
    "    abstracts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_abstracts_and_human_annotation(FILE_NAME):\n",
    "    # read the merged dataframe\n",
    "    df = pd.read_excel(FILE_NAME)\n",
    "    # print the number of rows in the merged dataframe\n",
    "    print(len(df))\n",
    "    print(len(df.columns))\n",
    "    \n",
    "    orig_columns = df.columns\n",
    "    if len(df.columns) == 9:\n",
    "        df.columns = ['article_id', 'abstract', 'Background', 'Purpose', 'Method', 'Result', 'Conclusions', 'Limitation', 'Others']\n",
    "    if len(df.columns) == 14:\n",
    "        df.columns = ['dataset_type', 'row_id', 'article_id', 'category', 'abstract', 'Background', 'Purpose', 'Method', 'Result', 'Conclusions', 'Limitation', 'Others',  'Errors', 'TooLong']\n",
    "        assert len(df.columns) == 14\n",
    "    for orig_column, column in zip(orig_columns, df.columns):\n",
    "        print(orig_column, '->', column)\n",
    "\n",
    "    abstracts = {}\n",
    "    orig_abstracts = {}\n",
    "    facets_for_abstracts = {}\n",
    "    for row in df.iterrows():\n",
    "        # split the abstract into sentences according to the tages <S0>, <S1>, <S2>, ...\n",
    "        sentences =  row[1]['abstract'].split(\"<S\")[1:]        \n",
    "        sentences = [sentence[sentence.find(\">\")+1:] for sentence in sentences] # get the content of each sentence\n",
    "        sentences = [sentence[:sentence.find(\"</S\")] for sentence in sentences] # remove the tags </S0>, </S1>, </S2>, ...\n",
    "\n",
    "        # extract facet for each sentence\n",
    "        facets = [None] * len(sentences)\n",
    "        for i in range(len(sentences)):\n",
    "            identifier = 'S'+str(i)\n",
    "            for tag in [ 'Background', 'Purpose', 'Method', 'Result', 'Conclusions', 'Limitation', 'Others', ]:\n",
    "                sent_ids = row[1][tag]\n",
    "                if type(sent_ids) == float:\n",
    "                    continue\n",
    "                if identifier in sent_ids:\n",
    "                    if facets[i] == None:\n",
    "                        facets[i] = [tag]\n",
    "                    else:\n",
    "                        facets[i].append(tag)\n",
    "            if facets[i] == None:\n",
    "                facets[i] = [\"Others\"]\n",
    "                    \n",
    "\n",
    "        abstracts[str(row[1]['article_id'])] = sentences\n",
    "        orig_abstracts[str(row[1]['article_id'])] = row[1]['abstract']\n",
    "        facets_for_abstracts[str(row[1]['article_id'])] = facets\n",
    "    return abstracts, facets_for_abstracts, orig_abstracts\n",
    "\n",
    "# file_name = 'annotation/gpt_vs_human_annotation/test'\n",
    "# abstracts, facets_for_abstracts, orig_abstracts = read_abstracts_and_human_annotation(file_name+\".xlsx\")\n",
    "# with open(f'{file_name}_abstracts.json', 'w') as fp:\n",
    "#     json.dump(abstracts, fp)\n",
    "\n",
    "# with open(f'{file_name}_facets.json', 'w') as fp:\n",
    "#     json.dump(facets_for_abstracts, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate files for Error Analysis\n",
    "\n",
    "# generating a random number from 0 to 499\n",
    "# import random\n",
    "# random.seed(42)\n",
    "\n",
    "# file_name = \"error_analysis\"\n",
    "# with open(file_name+\".txt\", \"w\") as f:\n",
    "#     for i in range(10):\n",
    "#         random_number = random.randint(0, 499)\n",
    "#         f.write('Example '+str(random_number))\n",
    "#         f.write(\"\\n\")\n",
    "#         f.write(list(orig_abstracts.values())[random_number])\n",
    "#         f.write(\"\\n\")\n",
    "#         f.write(\"  \".join([str(i) +' ' +' '.join(sublst) for i, sublst in enumerate(list(facets_for_abstracts.values())[random_number])]))\n",
    "#         f.write(\"\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "facets_for_abstracts = \n",
    "file_name = \"error_analysis_test\"\n",
    "with open(file_name+\".txt\", \"w\") as f:\n",
    "    for random_number in range(10):\n",
    "  \n",
    "        f.write('Example '+str(random_number))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(list(orig_abstracts.values())[random_number])\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"  \".join([str(i) +' ' +' '.join(sublst) for i, sublst in enumerate(list(facets_for_abstracts.values())[random_number])]))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 56 0.5535714285714286\n"
     ]
    }
   ],
   "source": [
    "with open('abstract_tagging/test_facets.json', 'r') as f:\n",
    "    ground_truth = json.load(f)\n",
    "    \n",
    "ground_truth = [tags for tags in ground_truth.values()]\n",
    "\n",
    "# compare the facets\n",
    "total = 0\n",
    "correct = 0\n",
    "error_analysis = []\n",
    "for i, abstract_facets in enumerate([result['facets'] for result in facet_results]):\n",
    "    \n",
    "    for j, facet in enumerate(abstract_facets):\n",
    "        # remove Purpose, Conclusions, Limitation\n",
    "        ground_truth[i][j] = [tag for tag in ground_truth[i][j] if tag not in ['Purpose', 'Conclusions', 'Limitation']]\n",
    "        ground_truth[i][j] = [\"Others\"] if ground_truth[i][j] == [] else ground_truth[i][j]\n",
    "\n",
    "        if facet in ground_truth[i][j]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            error_analysis.append((list(abstracts.values())[i][j], facet, ground_truth[i][j]))\n",
    "\n",
    "        total += 1\n",
    "print(correct, total, correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('For the pedestrian observer, financial markets look completely random with erratic and uncontrollable behavior.',\n",
       "  'Others',\n",
       "  ['Background']),\n",
       " ('At first approximation the difference between real price changes and the random walk model is too small to be detected using traditional time series analysis.',\n",
       "  'Background',\n",
       "  ['Others']),\n",
       " (' However, we show in the following that this difference between real financial time series and random walks, as small as it is, is detectable using modern statistical multivariate analysis, with several triggers encoded in trading systems.',\n",
       "  'Result',\n",
       "  ['Method']),\n",
       " (' We develop Demand Response Management (DRM) plans that clearly spell out the maximum duration as well as maximum severity of inconvenience.',\n",
       "  'Method',\n",
       "  ['Others']),\n",
       " (' However, the peak load that can be reduced is diminishing with the increas e in number of states.',\n",
       "  'Result',\n",
       "  ['Others']),\n",
       " (' These observations can serve as useful guidelines for developing appropriate DRM plans.',\n",
       "  'Value',\n",
       "  ['Result']),\n",
       " ('Edge detection is a classic problem in the field of image processing, which lays foundations for other tasks such as image segmentation.',\n",
       "  'Background',\n",
       "  ['Others']),\n",
       " (' Conventionally, this operation is performed using gradient operators such as the Roberts or Sobel operator, which can discover local changes in intensity levels.',\n",
       "  'Background',\n",
       "  ['Others']),\n",
       " (' These operators, however, perform poorly on low contrast images.',\n",
       "  'Result',\n",
       "  ['Others']),\n",
       " (' In this paper, we propose an edge detector architecture for color images based on fuzzy theory and the Sobel operator.',\n",
       "  'Method',\n",
       "  ['Others']),\n",
       " (' First, the R, G and B channels are extracted from an image and enhanced using fuzzy methods, in order to suppress noise and improve the contrast between the background and the objects.',\n",
       "  'Method',\n",
       "  ['Others']),\n",
       " (' The Sobel operator is then applied to each of the channels, which are finally combined into an edge map of the origin image.',\n",
       "  'Method',\n",
       "  ['Others']),\n",
       " (' Experimental results obtained through an FPGA-based implementation have proved the proposed method effective.',\n",
       "  'Result',\n",
       "  ['Others']),\n",
       " (' The various rate allocation schemes are evaluated in simulations of multiple high-definition (HD) video streams sharing multiple access networks.',\n",
       "  'Background',\n",
       "  ['Method']),\n",
       " (' Media-aware allocation further exploits its knowledge of the video DR characteristics to achieve a more balanced video quality among all streams.',\n",
       "  'Result',\n",
       "  ['Others']),\n",
       " (' We study a biased random walk on the interlacement set of Z d for d ≥ 3.',\n",
       "  'Others',\n",
       "  ['Background']),\n",
       " (' However, it has proved difficult to perform this summation when massive fields are present.',\n",
       "  'Background',\n",
       "  ['Others']),\n",
       " (' We show that in the classical case, the network entropy decreases at the consensus limit if the node initial values are i.i.d. Bernoulli random variables, and the network differential entropy is monotonically non-increasing if the node initial values are i.i.d. Gaussian.',\n",
       "  'Result',\n",
       "  ['Others']),\n",
       " (' While for quantum consensus dynamics, the network’s von Neumann entropy is in contrast non-decreasing.',\n",
       "  'Result',\n",
       "  ['Others']),\n",
       " (' However, in conventional atomic Bose-Einstein condensates (BECs) VAs with S > 1 are unstable.',\n",
       "  'Background',\n",
       "  ['Others']),\n",
       " (' The fundamental solitons and VAs remain stable in the presence of an arbitrarily strong repulsive contact interaction (in that case, the solitons are constructed analytically by means of the Thomas-Fermi approximation).',\n",
       "  'Result',\n",
       "  ['Others']),\n",
       " (' Under the action of the attractive contact interaction with strength β, which, by itself, would lead to collapse, the fundamental solitons and VAs exist and are stable, respectively, at β < βmax(S) and β < βst(S), with βst(S = 0) = βmax(S = 0) and βst(S ≥ 1) < βmax(S ≥ 1).',\n",
       "  'Result',\n",
       "  ['Others']),\n",
       " (' Accurate analytical approximations are found for both βst and βmax, with βst(S) growing linearly with S.',\n",
       "  'Result',\n",
       "  ['Others']),\n",
       " (' Thus, higher-order VAs are more robust than their lower-order couterparts, on the contrary to what is known in other systems that may support stable self-trapped vortices.',\n",
       "  'Result',\n",
       "  ['Others']),\n",
       " (' Conditions for the experimental realizations of the VAs are discussed.',\n",
       "  'Value',\n",
       "  ['Others'])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_facet_results(facet_results, \"abstract_tagging/test_facets_from_gpt.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used:  7429\n",
      "Number of papers: 14\n",
      "Number of papers with background: 0.9285714285714286\n",
      "Number of papers with method: 0.9285714285714286\n",
      "Number of papers with value: 0.35714285714285715\n",
      "Number of papers with result: 0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "facet_results = read_facet_results('cs5_abstract-tag.json')\n",
    "num_papers = len(facet_results)\n",
    "with_background = 0\n",
    "with_method = 0\n",
    "with_value = 0\n",
    "with_result = 0\n",
    "total_tokens = 0\n",
    "for result in facet_results:\n",
    "    total_tokens += result['total_tokens']\n",
    "    all_facets = result['gpt_annotation'].lower()\n",
    "    # print(result['sent'])\n",
    "    # print(result['gpt_annotation'].split(';'))\n",
    "    # print(.split(';'))\n",
    "    if 'background' in all_facets:\n",
    "        with_background += 1\n",
    "    if 'method' in all_facets:\n",
    "        with_method += 1\n",
    "    if 'value' in all_facets:\n",
    "        with_value += 1\n",
    "    if 'result' in all_facets:\n",
    "        with_result += 1\n",
    "print(\"Total tokens used: \", total_tokens)\n",
    "print(f\"Number of papers: {num_papers}\")\n",
    "print(f\"Number of papers with background: {with_background/num_papers}\")\n",
    "print(f\"Number of papers with method: {with_method/num_papers}\")\n",
    "print(f\"Number of papers with value: {with_value/num_papers}\")\n",
    "print(f\"Number of papers with result: {with_result/num_papers}\")\n",
    "# {result[0]: result[1:].strip() for result in results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "others\n",
      "['<S> signals sparse in a transformation domain can be recovered from a reduced set of randomly positioned samples by using compressive sensing algorithms . </S>', '<S> simple reconstruction algorithms are presented in the first part of the paper . </S>', '<S> the missing samples manifest themselves as a noise in this reconstruction . </S>', '<S> once the reconstruction conditions for a sparse signal are met and the reconstruction is achieved , the noise due to missing samples does not influence the results in a direct way . </S>', '<S> it influences the possibility to recover a signal only . </S>', '<S> additive input noise will remain in the resulting reconstructed signal . </S>', '<S> the accuracy of the recovery results is related to the additive input noise . </S>', '<S> simple derivation of this relation is presented . </S>', '<S> if a reconstruction algorithm for a sparse signal is used in the reconstruction of a nonsparse signal then the noise due to missing samples will remain and behave as an additive input noise . </S>', '<S> an exact relation for the mean square error of this error is derived for the partial dft matrix case in this paper and presented in form of a theorem . </S>', '<S> it takes into account very important fact that if all samples are available then the error will be zero , for both sparse and nonsparse recovered signals . </S>', '<S> theory is illustrated and checked on statistical examples . </S>']\n"
     ]
    }
   ],
   "source": [
    "i = 19\n",
    "id = facet_results[i]['article_id']\n",
    "print(facet_results[i]['gpt-annotation'])\n",
    "print(abstracts[id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa722aa51baa7ca1a14ae10c51947100c8742c9283df2adfc9442d206b591bbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
