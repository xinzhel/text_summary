{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "def write_facet_results(facet_results, file_name):\n",
    "    # check whether file exists\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"File already exists. Overwrite? (y/n)\")\n",
    "        if input() != 'y':\n",
    "            return\n",
    "    with open(file_name, 'w') as output_file:\n",
    "        for d in facet_results:\n",
    "            output_file.write(json.dumps(d)+'\\n')\n",
    "\n",
    "\n",
    "def read_facet_results(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    facet_results = []\n",
    "    for line in lines:\n",
    "        facet_results.append(json.loads(line))\n",
    "    return facet_results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Categories for All Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3037"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "i = 5\n",
    "inp_path = '../arxiv-dataset/train' + str(i) +  '.txt'\n",
    "with open(inp_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "len(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1035"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def query_domain(arxiv_id):\n",
    "    # arxiv_id = \"1810.04805\"\n",
    "    url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'\n",
    "    response = requests.get(url)\n",
    "    root = ET.fromstring(response.text)\n",
    "    categories = [cat.attrib['term'] for cat in root.iter('{http://www.w3.org/2005/Atom}category')]\n",
    "    return categories\n",
    "only_cat = []\n",
    "for line in lines[2002:]:\n",
    "    paper = json.loads(line)\n",
    "    time.sleep(0.01)\n",
    "    paper['categories'] = query_domain(paper[\"article_id\"])\n",
    "    only_cat.append(paper['categories'])\n",
    "ids_for_cats = { 'cs': [], 'eco': [], 'eess': [], 'math': [], 'physics': [], 'q-bio': [], 'q-fin': [], 'stat': []}\n",
    "for i, cat in enumerate(only_cat):\n",
    "    if ' '.join(cat).startswith('cs'):\n",
    "        ids_for_cats['cs'].append(i)\n",
    "    if ' '.join(cat).startswith('eco'):\n",
    "        ids_for_cats['eco'].append(i)\n",
    "    if ' '.join(cat).startswith('eess'):\n",
    "        ids_for_cats['eess'].append(i)\n",
    "    if ' '.join(cat).startswith('math'):\n",
    "        ids_for_cats['math'].append(i)\n",
    "    if ' '.join(cat).startswith('physics'):\n",
    "        ids_for_cats['physics'].append(i)\n",
    "    if ' '.join(cat).startswith('q-bio'):\n",
    "        ids_for_cats['q-bio'].append(i)\n",
    "    if ' '.join(cat).startswith('q-fin'):\n",
    "        ids_for_cats['q-fin'].append(i)\n",
    "    if ' '.join(cat).startswith('stat'):\n",
    "        ids_for_cats['stat'].append(i)\n",
    "\n",
    "with open('ids_for_cats.pickle', 'rb') as handle:\n",
    "    ids_for_cats2 = pickle.load(handle)\n",
    "length1 = len([ele for lst in ids_for_cats2.values() for ele in lst ])\n",
    "length2 = len([ele for lst in ids_for_cats.values() for ele in lst ])\n",
    "for k in  ids_for_cats2:\n",
    "    ids_for_cats2[k] = ids_for_cats2[k] + ids_for_cats[k]\n",
    "length3 = len([ele for lst in ids_for_cats2.values() for ele in lst ])\n",
    "assert length3 == length1 + length2\n",
    "\n",
    "with open('ids_for_cats.pickle', 'wb') as handle:\n",
    "    pickle.dump(ids_for_cats2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "len(only_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for domain, idx in ids_for_cats2.items():\n",
    "    total += len(idx)\n",
    "    print(domain, len(idx))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs 88\n",
      "eco 0\n",
      "eess 0\n",
      "math 472\n",
      "physics 124\n",
      "q-bio 19\n",
      "q-fin 7\n",
      "stat 21\n",
      "731\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate: query gpt3.5-turbo to categorize each sentence in the abstract into facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "inp_path = '../arxiv-dataset/train' + str(i) +  '.txt'\n",
    "only_cat = []\n",
    "with open(inp_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "papers = []\n",
    "for line in lines:\n",
    "    papers.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate abstracts for CS papers\n",
    "with open('ids_for_cats.pickle', 'rb') as handle:\n",
    "    ids_for_cats2 = pickle.load(handle)\n",
    "\n",
    "abstracts = {}\n",
    "for i in ids_for_cats2['cs']: \n",
    "    abstracts[papers[i]['article_id']] = papers[i]['abstract_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts for Single-sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prefix_prompt = \"\"\"\n",
    "You are a classifier to label the text between the tag <S> and </S> into one of the four classes: background, methods, results, values. If you are uncertain, output \"others\".  You can only output one word from labels or \"others\".\n",
    "\"\"\"\n",
    "\n",
    "facet_results = []\n",
    "\n",
    "for id, sentences  in list(abstracts.items())[:30]: \n",
    "    \n",
    "    for sent in sentences:\n",
    "        if \"* keywords\" in sent: \n",
    "            continue    # ignore keywords\n",
    "        # message = prefix_prompt + sent\n",
    "        output = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prefix_prompt},\n",
    "                {\"role\": \"user\", \"content\": sent}\n",
    "            ]\n",
    "        )\n",
    "        facet_label = output['choices'][0]['message']['content']\n",
    "        facet_results.append({\n",
    "            \"article_id\": id, \n",
    "            \"sent\": sent, \n",
    "            'gpt-annotation': facet_label, \n",
    "            \"prompt_tokens\": output.get(\"usage\").get('prompt_tokens'),\n",
    "            \"completion_tokens\": output.get(\"usage\").get('completion_tokens'),\n",
    "            \"total_tokens\": output.get(\"usage\").get('total_tokens')}\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts for Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "print(len(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Task: You are a tagger to label each subtext between the tags <NUM> and </NUM> with one of four labels, where NUM is a placeholder for the numeric index of a sentence:\n",
    "\n",
    "The labels are:\n",
    "1. Background: This sentence provides context and motivation for the research by discussing previous work and highlighting the need for further investigation or a statement of the research problem.\n",
    "2. Method: The setence outlines the approach used to carry out the research, including the experimental design, data collection and analysis methods, and any algorithms or models used.\n",
    "3. Result: The sentence presents the findings of the research, quantitative or qualitative analysis or the main outcomes.\n",
    "4. Value: The sentence discusses the broader impact and societal implications of the paper.\n",
    "5. Others: If you are uncertain which class the sentence belongs to, label it as \"others\". \n",
    "\n",
    "Output includes NUM-label pairs separated by a semicolon and give only one label to each NUM, e.g., \"1 Others; 2 Method\".\n",
    "\n",
    "Here is the input for tagging:\n",
    "\n",
    "\"\"\"\n",
    "# Example Input: \"<0> matrix data sets are common nowadays like in biomedical imaging </0> <1> we design a fast and easy - to - implement iterative algorithm to approximate arbitrarily finely these extremal matrices</1>\"\n",
    "# Example Output: \"1 Others; 2 Method\"\n",
    "\n",
    "facet_results = []\n",
    "total_tokens = 0\n",
    "for id, sentences  in list(abstracts.items())[:14]: \n",
    "    # remove <S> and </S> tags\n",
    "    tagged_sentences = [sent.replace('<S>', '').replace('</S>', '') for sent in sentences]\n",
    "\n",
    "    # add <NUM> and </NUM> tags for each sentence where NUM is 1, 2, 3, ...\n",
    "    tagged_sentences = ' '.join([f\" <{i}> {sent.strip()} </{i}> \" for i, sent in enumerate(tagged_sentences) if \"* keywords\" not in sent] )\n",
    "    \n",
    "    \n",
    "    specific_prompt = prompt + tagged_sentences\n",
    "    \n",
    "\n",
    "   \n",
    "    output = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": specific_prompt}\n",
    "        ]\n",
    "    )\n",
    "    facet_label = output['choices'][0]['message']['content']\n",
    "\n",
    "    facet_results.append({\n",
    "        \"article_id\": id, \n",
    "        \"sent\": tagged_sentences, \n",
    "        'gpt_annotation': facet_label, \n",
    "        \"prompt_tokens\": output.get(\"usage\").get('prompt_tokens'),\n",
    "        \"completion_tokens\": output.get(\"usage\").get('completion_tokens'),\n",
    "        \"total_tokens\": output.get(\"usage\").get('total_tokens')}\n",
    "    )\n",
    "    total_tokens += output.get(\"usage\").get('total_tokens')\n",
    "print(\"Total tokens used: \", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 Background', ' 1 Method', ' 2 Value']\n",
      "['0 Background', ' 1 Background', ' 2 Method', ' 3 Others', ' 4 Result', ' 5 Others', ' 6 Others', ' 7 Method', ' 8 Background', ' 9 Method', ' 10 Background', ' 11 Method', ' 12 Method', ' 13 Result']\n",
      "['0 Background', ' 1 Method', ' 2 Result', ' 3 Result', ' 4 Result', ' 5 Result', ' 6 Result', ' 7 Result', ' 8 Result', ' 9 Result', ' 10 Result', ' 11 Result.']\n",
      "['0 Others', ' 1 Background', ' 2 Method', ' 3 Result', ' 4 Method', ' 5 Background', ' 6 Result', ' 7 Value.']\n",
      "['0 Background', ' 1 Method', ' 2 Result', ' 3 Method', ' 4 Result']\n",
      "['0 Background', ' 1 Background', ' 2 Background', ' 3 Method', ' 4 Method', ' 5 Value']\n",
      "['0 Background', ' 1 Method', ' 2 Result', ' 3 Others']\n",
      "['0 Method', ' 1 Result', ' 2 Background', ' 3 Result', ' 4 Result', ' 5 Result', ' 6 Result']\n",
      "['0 Others', ' 1 Background', ' 2 Method', ' 3 Result', ' 4 Result', ' 5 Value', ' 6 Result.']\n",
      "['0 Others', ' 1 Background', ' 2 Method', ' 3 Result', ' 4 Others']\n",
      "['0 Others', ' 1 Background', ' 2 Method']\n",
      "['0 Method', ' 1 Result', ' 2 Result', ' 3 Method', ' 4 Result', ' 5 Value', ' 6 Method.']\n",
      "['0 Others', ' 1 Background', ' 2 Result', ' 3 Result']\n",
      "['0 Background', ' 1 Method', ' 2 Result', ' 3 Result', ' 4 Result.']\n",
      "Total tokens used:  7429\n",
      "Number of papers: 14\n",
      "Number of papers with background: 13\n",
      "Number of papers with method: 13\n",
      "Number of papers with value: 5\n",
      "Number of papers with result: 11\n"
     ]
    }
   ],
   "source": [
    "facet_results = read_facet_results('cs5_abstract-tag.json')\n",
    "num_papers = len(facet_results)\n",
    "with_background = 0\n",
    "with_method = 0\n",
    "with_value = 0\n",
    "with_result = 0\n",
    "total_tokens = 0\n",
    "for result in facet_results:\n",
    "    total_tokens += result['total_tokens']\n",
    "    all_facets = result['gpt_annotation'].lower()\n",
    "    print(result['gpt_annotation'].split(';'))\n",
    "    # print(.split(';'))\n",
    "    if 'background' in all_facets:\n",
    "        with_background += 1\n",
    "    if 'method' in all_facets:\n",
    "        with_method += 1\n",
    "    if 'value' in all_facets:\n",
    "        with_value += 1\n",
    "    if 'result' in all_facets:\n",
    "        with_result += 1\n",
    "print(\"Total tokens used: \", total_tokens)\n",
    "print(f\"Number of papers: {num_papers}\")\n",
    "print(f\"Number of papers with background: {with_background}\")\n",
    "print(f\"Number of papers with method: {with_method}\")\n",
    "print(f\"Number of papers with value: {with_value}\")\n",
    "print(f\"Number of papers with result: {with_result}\")\n",
    "# {result[0]: result[1:].strip() for result in results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "others\n",
      "['<S> signals sparse in a transformation domain can be recovered from a reduced set of randomly positioned samples by using compressive sensing algorithms . </S>', '<S> simple reconstruction algorithms are presented in the first part of the paper . </S>', '<S> the missing samples manifest themselves as a noise in this reconstruction . </S>', '<S> once the reconstruction conditions for a sparse signal are met and the reconstruction is achieved , the noise due to missing samples does not influence the results in a direct way . </S>', '<S> it influences the possibility to recover a signal only . </S>', '<S> additive input noise will remain in the resulting reconstructed signal . </S>', '<S> the accuracy of the recovery results is related to the additive input noise . </S>', '<S> simple derivation of this relation is presented . </S>', '<S> if a reconstruction algorithm for a sparse signal is used in the reconstruction of a nonsparse signal then the noise due to missing samples will remain and behave as an additive input noise . </S>', '<S> an exact relation for the mean square error of this error is derived for the partial dft matrix case in this paper and presented in form of a theorem . </S>', '<S> it takes into account very important fact that if all samples are available then the error will be zero , for both sparse and nonsparse recovered signals . </S>', '<S> theory is illustrated and checked on statistical examples . </S>']\n"
     ]
    }
   ],
   "source": [
    "i = 19\n",
    "id = facet_results[i]['article_id']\n",
    "print(facet_results[i]['gpt-annotation'])\n",
    "print(abstracts[id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa722aa51baa7ca1a14ae10c51947100c8742c9283df2adfc9442d206b591bbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
